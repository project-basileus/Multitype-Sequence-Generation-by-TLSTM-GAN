{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/zhuo/notebooks', '/home/zhuo/miniconda3/lib/python37.zip', '/home/zhuo/miniconda3/lib/python3.7', '/home/zhuo/miniconda3/lib/python3.7/lib-dynload', '', '/home/zhuo/miniconda3/lib/python3.7/site-packages', '/home/zhuo/mz-ds-deep-learning/ds-kubeflow', '/home/zhuo/miniconda3/lib/python3.7/site-packages/IPython/extensions', '/home/zhuo/.ipython', '/home/zhuo/time_lstm/time_lstm_modules']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import json\n",
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from tensorflow.keras.models import Model\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import seaborn as sns\n",
    "\n",
    "module_path = '/home/zhuo/time_lstm/time_lstm_modules'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "if 'TimeLSTM' in sys.modules:\n",
    "    importlib.reload(sys.modules['TimeLSTM'])\n",
    "    \n",
    "from TimeLSTM import TimeLSTM0\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './event_sequence_data_20200607_hour_08.json'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0]\n",
    "raw_data_small = raw_data[:500000]\n",
    "neg_data = []\n",
    "pos_data = []\n",
    "\n",
    "for r in raw_data_small:\n",
    "    if r['fraud_prob'] > 0:\n",
    "        pos_data.append(r)\n",
    "    else:\n",
    "        neg_data.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "event_types = []\n",
    "labels = []\n",
    "\n",
    "vocab = ['na', 'start', 'view', 'click', 'install']\n",
    "str2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2str= np.array(vocab)\n",
    "\n",
    "for row in raw_data_small:\n",
    "    _event_type_list, _time_list = list(zip(*row['sequence']))\n",
    "    _event_type_list = tuple(str2idx[c] for c in _event_type_list)\n",
    "    # drop missing times for data points 5096332 and 8959733\n",
    "    # otherwise cause problems in creating embeddings\n",
    "    if not any(_time_list): \n",
    "        continue\n",
    "        \n",
    "    labels.append(row['fraud_prob'])    \n",
    "    event_types.append(_event_type_list)\n",
    "\n",
    "    times.append(_time_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_len = 20 \n",
    "padding_type = 'post'\n",
    "truncating_type = 'pre'\n",
    "\n",
    "# padding event types\n",
    "padded_event_types = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    event_types, \n",
    "    padding=padding_type, \n",
    "    truncating = truncating_type,\n",
    "    maxlen=padding_len,\n",
    "    value=0,\n",
    "    dtype=object,\n",
    ")\n",
    "padded_event_types = padded_event_types.astype(int)\n",
    "\n",
    "#padding time stamps\n",
    "padded_ts = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    times, \n",
    "    padding=padding_type, \n",
    "    truncating = truncating_type,\n",
    "    maxlen=padding_len,\n",
    "    value=0,\n",
    "    dtype=object,\n",
    ")\n",
    "padded_ts = padded_ts.astype(int)\n",
    "padded_ts = np.diff(padded_ts, axis=1, prepend=0)\n",
    "padded_ts = padded_ts.astype(np.float64)\n",
    "padded_ts = padded_ts.clip(0)\n",
    "padded_ts = padded_ts.reshape((DATA_SIZE,20,1))\n",
    "\n",
    "padded_event_types = padded_event_types.reshape((DATA_SIZE,20,1))\n",
    "\n",
    "\n",
    "labels = np.array(labels)\n",
    "train_data_discriminator = padded_event_types,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change to difference of times\n",
    "# # print(padded_ts)\n",
    "# padded_ts = np.diff(padded_ts, axis=1, prepend=0)\n",
    "# print(padded_ts.shape)\n",
    "# # padded_ts[:,0] = 0\n",
    "# print(padded_event_types)\n",
    "# print(padded_event_types.shape)\n",
    "# non_zero_ts = padded_ts[padded_ts!=0]\n",
    "# non_zero_ts[0:100]\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(non_zero_ts, columns=['ts'])\n",
    "# df.quantile([.1, .2, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 20\n",
    "# temp = padded_event_types[0:10, :] # batch of 5 rows\n",
    "# temp_ts = padded_ts[0:10, :].astype(float)\n",
    "# temp = temp.reshape((10,20,1))\n",
    "# temp_ts = temp_ts.reshape((10,20,1))\n",
    "# embed0 = Embedding(input_dim=len(vocab), output_dim=16, input_length=20)(temp)\n",
    "# embed0 = Reshape((T, 16))(embed0)\n",
    "# print(embed0.shape)\n",
    "# print(temp_ts.shape)\n",
    "# tf.keras.layers.concatenate([embed0, temp_ts], axis=2).shape\n",
    "# z = tf.zeros([10,1])\n",
    "# print(z.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM for fraud/non-fraud\n",
    "\n",
    "TODO:\n",
    "1. masking loss - gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from TimeLSTM import TimeLSTM0,TimeLSTM1\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "DATA_SIZE = 500000\n",
    "T = 20\n",
    "batch_size = 100\n",
    "emb_dim = 2\n",
    "\n",
    "# Time-LSTM:\n",
    "i_et = Input(shape=(T,1), name='event_type')  # input of discrete feature event type\n",
    "i_ts = Input(shape=(T,1), name='time_delta')  # input of continuous feature timestamp\n",
    "\n",
    "embed0 = Embedding(input_dim=len(vocab), output_dim=emb_dim, input_length=20, \n",
    "                   batch_input_shape=[batch_size, None])(i_et) \n",
    "embed0 = Reshape((T, emb_dim))(embed0) # shape=[Batch_size, 20, 16]\n",
    "merged0 = tf.keras.layers.concatenate([embed0, i_ts], axis=2) # # shape=[Batch_size, 20, 17]\n",
    " \n",
    "hm, tm = TimeLSTM1(3, activation='selu',name='time_lstm')(merged0)\n",
    "\n",
    "# gaussian mixture for time delta\n",
    "k_mixt = 5\n",
    "alpha = Dense(k_mixt, activation=tf.nn.softmax)(tm)\n",
    "mu = Dense(k_mixt, activation=None)(tm)\n",
    "sigma = Dense(k_mixt, activation=tf.nn.softplus,name='sigma')(tm)\n",
    "\n",
    "gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(\n",
    "        probs=alpha),\n",
    "        components_distribution=tfd.Normal(\n",
    "            loc=mu, \n",
    "            scale=sigma))\n",
    "\n",
    "gaussian_log = gm.log_prob(i_ts)# apply gaussian mixture to time stamp input\n",
    "\n",
    "# mask out zeros in time stamps\n",
    "mask = tf.not_equal(i_ts, 0)\n",
    "# gaussian_log = tf.boolean_mask(gaussian_log, mask)\n",
    "# time_delta = tf.boolean_mask(i_ts, mask)\n",
    "\n",
    "# predicted fraud prob\n",
    "fraud_prob = Dense(1, activation='sigmoid',name='fraud_prob')(hm)\n",
    "\n",
    "model_a = Model(\n",
    "    inputs=[i_et, i_ts], \n",
    "    outputs=[fraud_prob, gaussian_log])\n",
    "\n",
    "dot_img_file = '/tmp/mode_a.png'\n",
    "tf.keras.utils.plot_model(model_a, to_file=dot_img_file, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(5, 1), dtype=float64, numpy=\n",
      "array([[0.49902732],\n",
      "       [0.4985811 ],\n",
      "       [0.49835408],\n",
      "       [0.4984813 ],\n",
      "       [0.49975879]])>, <tf.Tensor: shape=(21,), dtype=float64, numpy=\n",
      "array([-4.22968079e+05, -4.22968079e+05, -4.22968079e+05, -4.22968079e+05,\n",
      "       -4.63462280e+05, -4.63462280e+05, -4.63462280e+05, -1.41939639e+00,\n",
      "       -1.41939639e+00, -1.41939639e+00, -1.41939639e+00, -1.41939639e+00,\n",
      "       -1.41939639e+00, -1.41939639e+00, -1.41939639e+00, -1.41939639e+00,\n",
      "       -1.41939639e+00, -1.41939639e+00, -1.41939639e+00, -1.41939639e+00,\n",
      "       -1.41939639e+00])>, <tf.Tensor: shape=(21,), dtype=float64, numpy=\n",
      "array([ 1370.,     6.,    57., -1433.,    34.,   139.,  -173.,   721.,\n",
      "        -721.,    30.,    18.,   -48.,    30.,    18.,     3.,     5.,\n",
      "           5.,    30.,    12.,   285.,  -388.])>]\n"
     ]
    }
   ],
   "source": [
    "gm = tfd.MixtureSameFamily(\n",
    "    mixture_distribution=tfd.Categorical(\n",
    "        probs=[0.3, 0.7]),\n",
    "    components_distribution=tfd.Normal(\n",
    "      loc=[-1., 1],       # One for each component.\n",
    "      scale=[0.1, 0.5]))  # And same here.\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# x = np.array([1., 2., 3.])\n",
    "# gm.log_prob(x)\n",
    "padded_ts = padded_ts.astype(np.float64)\n",
    "padded_ts = padded_ts.reshape((DATA_SIZE,20,1))\n",
    "padded_event_types = padded_event_types.reshape((DATA_SIZE,20,1))\n",
    "\n",
    "features = (padded_event_types, padded_ts)\n",
    "labels = tf.reshape(labels, (labels.shape[0],1))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.shuffle(1000).batch(5, drop_remainder=True)\n",
    "\n",
    "for features, labels_batch in dataset.take(1):\n",
    "    print(model_a(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-98.582466 ,  -2.5824664], dtype=float32)>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.log_prob([ 8., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-LSTM, Gaussian Mixture Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 20, 30)\n",
      "gaussian loss:4595710319008.129\n",
      "ce loss:0.6769465585839624\n",
      "loss:4595710319008.806\n",
      "(30, 20, 30)\n",
      "gaussian loss:17375126590.75474\n",
      "ce loss:0.6760417571892174\n",
      "loss:17375126591.430782\n",
      "(30, 20, 30)\n",
      "gaussian loss:4047423010.691985\n",
      "ce loss:0.6745067056774048\n",
      "loss:4047423011.366492\n",
      "(30, 20, 30)\n",
      "gaussian loss:2195752109.834849\n",
      "ce loss:0.6742117106422704\n",
      "loss:2195752110.5090604\n",
      "(30, 20, 30)\n",
      "gaussian loss:14570188758.361095\n",
      "ce loss:0.6718601887018989\n",
      "loss:14570188759.032955\n"
     ]
    }
   ],
   "source": [
    "padded_ts = padded_ts.astype(float)\n",
    "padded_ts = padded_ts.reshape((DATA_SIZE,20,1))\n",
    "padded_event_types = padded_event_types.reshape((DATA_SIZE,20,1))\n",
    "\n",
    "features = (padded_event_types, padded_ts)\n",
    "labels = tf.reshape(labels, (labels.shape[0],1))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.shuffle(1000).batch(30, drop_remainder=True)\n",
    "loss_history = []\n",
    "optimizer=Adam(lr=0.001)\n",
    "for features, labels_batch in dataset.take(5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        fraud_probs, gaussian_log_prob = model_a(features)\n",
    "        \n",
    "        # gaussian-mixture loss\n",
    "        print(gaussian_log_prob.shape)\n",
    "        gaussian_loss = -tf.reduce_sum(gaussian_log_prob)\n",
    "        print('gaussian loss:{}'.format(gaussian_loss))\n",
    "        \n",
    "        # cross-entropy loss\n",
    "        ce_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(labels_batch, fraud_probs, from_logits=False))\n",
    "        print('ce loss:{}'.format(ce_loss))\n",
    "        loss = gaussian_loss + ce_loss\n",
    "\n",
    "    loss_history.append(loss.numpy())\n",
    "    print('loss:{}'.format(loss.numpy()))\n",
    "    grads = tape.gradient(loss, model_a.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_a.trainable_variables))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator with CNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer gru is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 20, 1, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-556dba2a8708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     recurrent_initializer='glorot_uniform')(outputs)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# embed0 = Reshape((T, 16))(embed0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 737\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    738\u001b[0m         if (any(isinstance(x, ragged_tensor.RaggedTensor) for x in input_list)\n\u001b[1;32m    739\u001b[0m             and self._supports_ragged_inputs is False):  # pylint: disable=g-bool-id-comparison\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer gru is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 20, 1, 16]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "T = 20\n",
    "batch_size = 1\n",
    "# time LSTM usage example:\n",
    "i_et = Input(shape=(T, 1))  # input of discrete feature event type\n",
    "i_ts = Input(shape=(T, 1))  # input of continuous feature timestamp\n",
    "\n",
    "outputs = Embedding(input_dim=len(vocab), output_dim=16, input_length=20, batch_input_shape=[batch_size, None])(i_et)\n",
    "outputs = tf.keras.layers.GRU(32,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform')(outputs)\n",
    "\n",
    "# embed0 = Reshape((T, 16))(embed0)\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.GRU(rnn_units,\n",
    "#                             return_sequences=True,\n",
    "#                             stateful=True,\n",
    "#                             recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "outputs = Dense(1)(outputs)\n",
    "\n",
    "m = Model(i_et, outputs)\n",
    "m(padded_event_types[1:100, :])\n",
    "# merged0 = tf.keras.layers.concatenate([embed0, dense0], axis=2)\n",
    "# x_b, _ = TimeLSTM0(15, activation='selu')(merged0)\n",
    "# x_b = Dense(1, activation='sigmoid')(x_b)\n",
    "\n",
    "# model_b = Model([i_et, i_ts], x_b)\n",
    "# model_b([padded_event_types[1:5, :], padded_ts[1:5, :]])\n",
    "\n",
    "# model_b.compile(loss='binary_crossentropy', \n",
    "#               optimizer=Adam(lr=0.001),\n",
    "#              )\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss', patience=5\n",
    "# )\n",
    "\n",
    "# r_b = model_b.fit(\n",
    "#     [padded_event_types, padded_ts], labels,\n",
    "#     epochs=1,\n",
    "# )\n",
    "\n",
    "# # x_b, _ = TimeLSTM0(15, activation='selu')(merged0)\n",
    "# # x_b = Dense(1, activation='sigmoid')(x_b)\n",
    "\n",
    "# # model_b = Model([i_et, i_ts], x_b)\n",
    "\n",
    "# i_et = Input(shape=(20, 1))  # input of discrete feature event type\n",
    "# i_ts = Input(shape=(20, 1))  # input of continuous feature timestamp\n",
    "\n",
    "# # dense0 = Dense(1)(i_ts)\n",
    "\n",
    "# discriminator = Embedding(len(vocab), 16, \\\n",
    "#                             input_length=padding_len)(i_et)\n",
    "# discriminator = tf.keras.layers.concatenate([discriminator, dense0], axis=2)\n",
    "# discriminator = (Conv1D(32, 3, activation='relu'))(discriminator)\n",
    "# discriminator.add(MaxPooling1D(5))\n",
    "# discriminator.add(Conv1D(32, 3, activation='relu'))\n",
    "# discriminator.add(GlobalMaxPooling1D())\n",
    "# discriminator.add(Dense(1))\n",
    "# discriminator.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "# history = discriminator.fit(inp, labels,\n",
    "# epochs=2, batch_size=500, validation_split=0.2)\n",
    "\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(train_data_discriminator)\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(500, drop_remainder=True)\n",
    "# discriminator.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "# discriminator.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "features = (padded_event_types, padded_ts)\n",
    "labels = tf.reshape(labels, (labels.shape[0],1))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.shuffle(1000).batch(3, drop_remainder=True)\n",
    "loss_history = []\n",
    "optimizer=Adam(lr=0.001)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:, :-1, :]\n",
    "    target_text = chunk[:, 1:, :]\n",
    "    return input_text, target_text\n",
    "\n",
    "for features, labels_batch in dataset.take(1):\n",
    "    et, ts = features\n",
    "    input_ts, target_ts = split_input_target(ts)\n",
    "    input_et, target_et = split_input_target(et)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(pos_padded_event_types)\n",
    "# dataset = dataset.map(split_input_target)\n",
    "\n",
    "# BATCH_SIZE = 12\n",
    "# BUFFER_SIZE = 1000\n",
    "\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# for input_example, target_example in  dataset.take(1):\n",
    "#     print('input_example:{}'.format(input_example))\n",
    "#     print('target_example:{}'.format(target_example))\n",
    "# input_example.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_example:[[[1]\n",
      "  [2]\n",
      "  [1]\n",
      "  [2]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]]\n",
      "target_example:[[[2]\n",
      "  [1]\n",
      "  [2]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 19, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_et)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('input_example:{}'.format(input_example))\n",
    "    print('target_example:{}'.format(target_example))\n",
    "    \n",
    "et = train_et[0,:]\n",
    "ts = train_ts[0,:]\n",
    "    \n",
    "i_et = Input(shape=(T,1))  # input of discrete feature event type\n",
    "i_ts = Input(shape=(T,1))  # input of continuous feature timestamp\n",
    "\n",
    "embed0 = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,)(i_et)\n",
    "embed0 = Reshape((T, 16))(embed0)\n",
    "merged0 = tf.keras.layers.concatenate([embed0, i_ts], axis=2)\n",
    "\n",
    "x_b, _ = TimeLSTM0(15, activation='selu')(merged0)\n",
    "x_b = Dense(vocab_size, activation='sigmoid')(x_b)\n",
    "model_b = Model([i_et, i_ts], x_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Generator based on Time-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19, 1)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Could not compute output Tensor(\"dense_63/Identity:0\", shape=(None, 5), dtype=float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-27bacdf47a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_example\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0moutput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Could not compute output '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m       \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m       \u001b[0moutput_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Could not compute output Tensor(\"dense_63/Identity:0\", shape=(None, 5), dtype=float64)"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 16\n",
    "rnn_units = 32\n",
    "T = 19\n",
    "def build_model_time_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1):\n",
    "    \n",
    "    i_et = Input(shape=(T,1))  # input of discrete feature event type\n",
    "    i_ts = Input(shape=(T,1))  # input of continuous feature timestamp\n",
    "\n",
    "    embed0 = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,)(i_et)\n",
    "    embed0 = Reshape((T, 16))(embed0)\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, i_ts], axis=2)\n",
    "    \n",
    "    x_b, _ = TimeLSTM0(15, activation='selu')(merged0)\n",
    "    x_b = Dense(vocab_size, activation='sigmoid')(x_b)\n",
    "    model_b = Model([i_et, i_ts], x_b)\n",
    "    model_b.compile(loss='binary_crossentropy', \n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                 )\n",
    "    return model_b\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = build_model_time_lstm(vocab_size = len(vocab),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=1)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(input_example.shape)\n",
    "    print(model(input_example).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 19)\n",
      "(10, 19, 5)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 16\n",
    "rnn_units = 32\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size=1):\n",
    "    \n",
    "    i_et = Input(shape=(T,1))  # input of discrete feature event type\n",
    "    i_ts = Input(shape=(T,1))  # input of continuous feature timestamp\n",
    "\n",
    "    embed0 = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,),\n",
    "    embed0 = Reshape((T, 16))(embed0)\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, i_ts], axis=2)\n",
    "\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = build_model(vocab_size = len(vocab),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(input_example.shape)\n",
    "    print(model(input_example).shape)\n",
    "\n",
    "# model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# checkpoint_dir = './training_checkpoints'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "# checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_prefix,\n",
    "#     save_weights_only=True)\n",
    "\n",
    "# EPOCHS = 2\n",
    "# history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-ebd4e0106a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mrnn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     batch_size=1)\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minput_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_example\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd7645662d0>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "generator = build_model(vocab_size, embedding_dim, rnn_units, batch_size=50)\n",
    "\n",
    "generator.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll-out policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event(model, batch_size=50, length=20):\n",
    "    states = [1] * batch_size\n",
    "    states = tf.expand_dims(states, -1)\n",
    "    all_actions_one_hot = tf.one_hot(states, depth=5)\n",
    "    all_predicted_actions = None\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(length):\n",
    "        predictions = model(states)\n",
    "        if all_predicted_actions is None:\n",
    "            all_predicted_actions = predictions\n",
    "        else:\n",
    "            all_predicted_actions =tf.keras.layers.concatenate([all_predicted_actions, predictions], axis=1)\n",
    "        \n",
    "        # sample an action - token\n",
    "        predictions = tf.squeeze(predictions)\n",
    "        predicted_idx = tf.random.categorical(predictions, num_samples=1)\n",
    "        \n",
    "        #maintain 16 gaussians, use predicted_idx to index which one to use to sample\n",
    "        current_gaussian = list_gaussian[predicted_idx, prev_predicted_idx]\n",
    "        current_delta_t = current_gaussian(sample\n",
    "                                          )\n",
    "        \n",
    "        # sample a timestamp - Gaussian?\n",
    "        predicted_idx_one_hot = tf.one_hot(predicted_idx, depth=5)\n",
    "        all_actions_one_hot = tf.concat([all_actions_one_hot, predicted_idx_one_hot], axis=1)\n",
    "        \n",
    "        # pass the predicted action as the next input to the model\n",
    "        states = predicted_idx\n",
    "\n",
    "    return  all_actions_one_hot, all_predicted_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:-0.028991742059588432\n",
      "loss:-0.029290182515978813\n",
      "loss:-0.029063435271382332\n",
      "loss:-0.03051750175654888\n",
      "loss:-0.03157886117696762\n",
      "loss:-0.03368803858757019\n",
      "loss:-0.030589960515499115\n",
      "loss:-0.03016679361462593\n",
      "loss:-0.03109685517847538\n",
      "loss:-0.031214972957968712\n",
      "loss:[230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 10000 values, but the requested shape has 100 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-7134b185eeff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_ReshapeGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         message=\"Converting sparse IndexedSlices to a dense Tensor.*\")\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7438\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7440\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7441\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7442\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 10000 values, but the requested shape has 100 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_history = []\n",
    "g_steps = 10\n",
    "d_steps = 10\n",
    "# Generator training\n",
    "\n",
    "for i in range(g_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_actions_one_hot, all_predicted_actions = generate_event(generator, length=20)\n",
    "        all_actions_one_hot = all_actions_one_hot[:, 1:,]\n",
    "\n",
    "        # policy gradient loss\n",
    "        negative_likelihoods = tf.keras.losses.categorical_crossentropy\\\n",
    "            (all_actions_one_hot, all_predicted_actions,from_logits=True)\n",
    "        in_dis = tf.argmax(all_actions_one_hot, axis = -1)\n",
    "        q_values = tf.broadcast_to(discriminator(in_dis), negative_likelihoods.shape) # this needs N-time Monte Carlo search\n",
    "        weighted_negative_likelihoods = tf.multiply(negative_likelihoods, q_values)\n",
    "        loss = tf.reduce_mean(weighted_negative_likelihoods)\n",
    "\n",
    "    loss_history.append(loss.numpy())\n",
    "    print('loss:{}'.format(loss.numpy()))\n",
    "    grads = tape.gradient(loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "# Discriminator training\n",
    "for i in range(d_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_actions_one_hot, all_predicted_actions = generate_event(generator, length=20)\n",
    "        all_actions_one_hot = all_actions_one_hot[:, 1:,]\n",
    "\n",
    "        fake_train = tf.argmax(all_actions_one_hot, axis = -1)\n",
    "        fake_labels = tf.zeros(in_dis.shape[0])\n",
    "\n",
    "        real_train = np.random.permutation(pos_padded_event_types)[:50]\n",
    "        real_labels = tf.ones(real_train.shape[0])\n",
    "        train_all = np.concatenate((real_train, fake_train), axis=0)\n",
    "        labels_all = np.concatenate((real_labels, fake_labels, ), axis=0)\n",
    "\n",
    "        pred = discriminator(train_all)\n",
    "        dis_loss = tf.keras.losses.categorical_crossentropy\\\n",
    "                (labels_all, pred, from_logits=True)\n",
    "\n",
    "    print('loss:{}'.format(dis_loss.numpy()))\n",
    "    grads = tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:[230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853 230.25853\n",
      " 230.25853 230.25853]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 10000 values, but the requested shape has 100 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-296-0cf24aac7403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_ReshapeGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         message=\"Converting sparse IndexedSlices to a dense Tensor.*\")\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7438\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7440\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7441\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7442\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 10000 values, but the requested shape has 100 [Op:Reshape]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n",
      "(50, 20)\n",
      "(50, 20)\n",
      "(50, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 20), dtype=float32, numpy=\n",
       "array([[-0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375,\n",
       "        -0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375,\n",
       "        -0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375,\n",
       "        -0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375],\n",
       "       [-0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174,\n",
       "        -0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174,\n",
       "        -0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174,\n",
       "        -0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174],\n",
       "       [-0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667,\n",
       "        -0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667,\n",
       "        -0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667,\n",
       "        -0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667],\n",
       "       [-0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002,\n",
       "        -0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002,\n",
       "        -0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002,\n",
       "        -0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002],\n",
       "       [-0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753,\n",
       "        -0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753,\n",
       "        -0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753,\n",
       "        -0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753],\n",
       "       [-0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566,\n",
       "        -0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566,\n",
       "        -0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566,\n",
       "        -0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566],\n",
       "       [-0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364,\n",
       "        -0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364,\n",
       "        -0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364,\n",
       "        -0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364],\n",
       "       [-0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629,\n",
       "        -0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629,\n",
       "        -0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629,\n",
       "        -0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629],\n",
       "       [-0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229,\n",
       "        -0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229,\n",
       "        -0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229,\n",
       "        -0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229],\n",
       "       [-0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214,\n",
       "        -0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214,\n",
       "        -0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214,\n",
       "        -0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214],\n",
       "       [-0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897,\n",
       "        -0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897,\n",
       "        -0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897,\n",
       "        -0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897],\n",
       "       [-0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325,\n",
       "        -0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325,\n",
       "        -0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325,\n",
       "        -0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325],\n",
       "       [-0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983,\n",
       "        -0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983,\n",
       "        -0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983,\n",
       "        -0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983],\n",
       "       [-0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973,\n",
       "        -0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973,\n",
       "        -0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973,\n",
       "        -0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973],\n",
       "       [-0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286,\n",
       "        -0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286,\n",
       "        -0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286,\n",
       "        -0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286],\n",
       "       [-0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224,\n",
       "        -0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224,\n",
       "        -0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224,\n",
       "        -0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224],\n",
       "       [-0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032,\n",
       "        -0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032,\n",
       "        -0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032,\n",
       "        -0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032],\n",
       "       [-0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336,\n",
       "        -0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336,\n",
       "        -0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336,\n",
       "        -0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336],\n",
       "       [-0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082,\n",
       "        -0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082,\n",
       "        -0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082,\n",
       "        -0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082],\n",
       "       [-0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196,\n",
       "        -0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196,\n",
       "        -0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196,\n",
       "        -0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196],\n",
       "       [-0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632,\n",
       "        -0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632,\n",
       "        -0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632,\n",
       "        -0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632],\n",
       "       [-0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ,\n",
       "        -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ,\n",
       "        -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ,\n",
       "        -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ],\n",
       "       [-0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372,\n",
       "        -0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372,\n",
       "        -0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372,\n",
       "        -0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372],\n",
       "       [-0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308,\n",
       "        -0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308,\n",
       "        -0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308,\n",
       "        -0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308],\n",
       "       [-0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529,\n",
       "        -0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529,\n",
       "        -0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529,\n",
       "        -0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529],\n",
       "       [-0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649,\n",
       "        -0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649,\n",
       "        -0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649,\n",
       "        -0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649],\n",
       "       [-0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ,\n",
       "        -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ,\n",
       "        -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ,\n",
       "        -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ],\n",
       "       [-0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ,\n",
       "        -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ,\n",
       "        -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ,\n",
       "        -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ],\n",
       "       [-0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028,\n",
       "        -0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028,\n",
       "        -0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028,\n",
       "        -0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028],\n",
       "       [-0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295,\n",
       "        -0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295,\n",
       "        -0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295,\n",
       "        -0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295],\n",
       "       [-0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052,\n",
       "        -0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052,\n",
       "        -0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052,\n",
       "        -0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052],\n",
       "       [-0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641,\n",
       "        -0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641,\n",
       "        -0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641,\n",
       "        -0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641],\n",
       "       [-0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994,\n",
       "        -0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994,\n",
       "        -0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994,\n",
       "        -0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994],\n",
       "       [-0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836,\n",
       "        -0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836,\n",
       "        -0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836,\n",
       "        -0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836],\n",
       "       [-0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719,\n",
       "        -0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719,\n",
       "        -0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719,\n",
       "        -0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719],\n",
       "       [-0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402,\n",
       "        -0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402,\n",
       "        -0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402,\n",
       "        -0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402],\n",
       "       [-0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411,\n",
       "        -0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411,\n",
       "        -0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411,\n",
       "        -0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411],\n",
       "       [-0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806,\n",
       "        -0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806,\n",
       "        -0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806,\n",
       "        -0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806],\n",
       "       [-0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087,\n",
       "        -0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087,\n",
       "        -0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087,\n",
       "        -0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087],\n",
       "       [-0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899,\n",
       "        -0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899,\n",
       "        -0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899,\n",
       "        -0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899],\n",
       "       [-0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358,\n",
       "        -0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358,\n",
       "        -0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358,\n",
       "        -0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358],\n",
       "       [-0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469,\n",
       "        -0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469,\n",
       "        -0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469,\n",
       "        -0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469],\n",
       "       [-0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763,\n",
       "        -0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763,\n",
       "        -0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763,\n",
       "        -0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763],\n",
       "       [-0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ,\n",
       "        -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ,\n",
       "        -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ,\n",
       "        -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ],\n",
       "       [-0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329,\n",
       "        -0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329,\n",
       "        -0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329,\n",
       "        -0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329],\n",
       "       [-0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662,\n",
       "        -0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662,\n",
       "        -0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662,\n",
       "        -0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662],\n",
       "       [-0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218,\n",
       "        -0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218,\n",
       "        -0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218,\n",
       "        -0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218],\n",
       "       [-0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326,\n",
       "        -0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326,\n",
       "        -0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326,\n",
       "        -0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326],\n",
       "       [-0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121,\n",
       "        -0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121,\n",
       "        -0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121,\n",
       "        -0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121],\n",
       "       [-0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747,\n",
       "        -0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747,\n",
       "        -0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747,\n",
       "        -0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_actions_one_hot, all_predicted_actions = generate_event(generator, length=20)\n",
    "all_actions_one_hot = all_actions_one_hot[:, 1:,]\n",
    "all_actions_one_hot\n",
    "# policy gradient loss\n",
    "negative_likelihoods = tf.keras.losses.categorical_crossentropy\\\n",
    "    (all_actions_one_hot, all_predicted_actions,from_logits=True)\n",
    "in_dis = tf.argmax(all_actions_one_hot, axis = -1)\n",
    "print(in_dis.shape)\n",
    "discriminator(in_dis)\n",
    "q_values = tf.broadcast_to(discriminator(in_dis), negative_likelihoods.shape) # this needs N-time Monte Carlo search\n",
    "weighted_negative_likelihoods = tf.multiply(negative_likelihoods, q_values)\n",
    "loss = tf.reduce_mean(weighted_negative_likelihoods)\n",
    "    \n",
    "print(negative_likelihoods.shape)\n",
    "print(in_dis.shape)\n",
    "print(q_values.shape)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mnist_model(images, training=True)\n",
    "    \n",
    "        # Add asserts to check the shape of the output.\n",
    "        tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "        loss_value = loss_object(labels, logits)\n",
    "\n",
    "    loss_history.append(loss_value.numpy().mean())\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    print('grads:{}'.format(grads))\n",
    "    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start view view view view click view view start view install install view click view start install start install click view \n",
      "start start click view install start view click view view view install click start view view install install view view na \n",
      "start click install install click install na click na na na na na na na na na na na na na \n",
      "start click click view view view view install start na install click view view start start start install start click view \n",
      "start view start view start click click click install click view install start view install start click start start view install \n",
      "start start start click view view view na na na na na na na na na na na na na na \n",
      "start view view start start view view view install click start start start click view install click view click click view \n",
      "start view view click install start view view click start install click view install start click click view start view install \n",
      "start view install view click view install start view click install click view start view install install click view view start \n",
      "start start click click install view install click install start view view view view view start start view start click install \n",
      "start view view click click click view view click view view view view install view click click install start view install \n",
      "start view start install start install view install na na na na na na na na na na na na na \n",
      "start view start view click view install click view install install install start start install install click start view install click \n",
      "start view view click start view start start start install start view start start install view click view start install install \n",
      "start na na na na na na na na na na na na na na na na na na na na \n",
      "start view start click install click view install click view click start view view install view click install view click view \n",
      "start view click start view view click start start start view install view start click view view install install view click \n",
      "start na na na na na na na na na na na na na na na na na na na na \n",
      "start start view start view click start view install start start click start view view start view view view view click \n",
      "start view start click view view view view view start click view view view start view view view click install install \n"
     ]
    }
   ],
   "source": [
    "# for i in range(20):\n",
    "seq_str, seq_int = generate_event(model)\n",
    "#     print(seq_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 4, 4]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 20, 16)            80        \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 18, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 4, 32)             3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,785\n",
      "Trainable params: 4,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02621368]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discriminator - CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(len(vocab), 16, input_length=20))\n",
    "cnn_model.add(Conv1D(32, 3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(3))\n",
    "cnn_model.add(Conv1D(32, 3, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(1))\n",
    "cnn_model.summary()\n",
    "cnn_model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "# history = model.fit(x_train, y_train,\n",
    "# epochs=10, batch_size=128, validation_split=0.2)\n",
    "cnn_model.predict(np.array([seq_int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, discriminator, rollout_policy, path):\n",
    "        self.path = path\n",
    "        self.discriminator = discriminator\n",
    "        self.mdp = rollout_policy # an pretrained RNN model\n",
    "        self.base_state = [1]\n",
    "        self.state = self.base_state\n",
    "        self.reset()\n",
    "        self.T = 20\n",
    "        self.n_sample = 100\n",
    "    \n",
    "    def load_model():\n",
    "        tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        model.build(tf.TensorShape([1, None]))\n",
    "        model.summary()\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = t\n",
    "        self.mdp.reset_states()\n",
    "        self.state = self.base_state\n",
    "        return True\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.t = self.t + 1\n",
    "\n",
    "        reward = self.get_reward(action, self.n_sample)\n",
    "        is_episode_end = self.t > self.T\n",
    "\n",
    "        self.state.append(action)\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        return [next_state, reward, is_episode_end]\n",
    "    \n",
    "    def render(self):\n",
    "        for c in self.state:\n",
    "            \n",
    "        full_seq_str.append(idx2str[predicted_idx] + \" \")\n",
    "        full_seq_int.append(predicted_idx)\n",
    "\n",
    "    return 'start ' + ''.join(full_seq_str), full_seq_int\n",
    "\n",
    "\n",
    "    def get_reward(self, action, n_sample):\n",
    "        reward = 0\n",
    "        for idx_sample in range(n_sample):\n",
    "            full_seq = self.generate_event(state, self.T)\n",
    "            reward += self.discriminator.predict(full_seq) / n_sample\n",
    "        return reward\n",
    "        \n",
    "    def generate_event(state, length):\n",
    "        input_eval = state\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        full_seq = input_eval\n",
    "        # Empty string to store our results\n",
    "        text_generated = []\n",
    "\n",
    "        self.rollout_policy.reset_states()\n",
    "        for i in range(length):\n",
    "            predictions = mdp(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = predictions\n",
    "            predicted_idx = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            \n",
    "            # pass the predicted action as the next input to the model\n",
    "            input_eval = tf.expand_dims([predicted_idx], 0)\n",
    "            text_generated.append(idx2str[predicted_idx])\n",
    "            full_seq.append(predicted_idx)\n",
    "        return full_seq\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
