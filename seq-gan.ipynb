{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import json\n",
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './event_sequence_data_20200607_hour_08.json'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0]\n",
    "raw_data_small = raw_data[:50000]\n",
    "neg_data = []\n",
    "pos_data = []\n",
    "\n",
    "for r in raw_data_small:\n",
    "    if r['fraud_prob'] > 0:\n",
    "        pos_data.append(r)\n",
    "    else:\n",
    "        neg_data.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "event_types = []\n",
    "labels = []\n",
    "\n",
    "vocab = ['na', 'start', 'view', 'click', 'install']\n",
    "str2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2str= np.array(vocab)\n",
    "\n",
    "for row in raw_data_small:\n",
    "    _event_type_list, _time_list = list(zip(*row['sequence']))\n",
    "    _event_type_list = tuple(str2idx[c] for c in _event_type_list)\n",
    "    # drop missing times for data points 5096332 and 8959733\n",
    "    # otherwise cause problems in creating embeddings\n",
    "    if not any(_time_list): \n",
    "        continue\n",
    "        \n",
    "    labels.append(row['fraud_prob'])    \n",
    "    event_types.append(_event_type_list)\n",
    "\n",
    "    times.append(_time_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, ..., 0, 0, 0],\n",
       "       [1, 2, 1, ..., 0, 0, 0],\n",
       "       [1, 2, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 2, 0, ..., 0, 0, 0],\n",
       "       [1, 2, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_len = 20 \n",
    "padding_type = 'post'\n",
    "truncating_type = 'pre'\n",
    "\n",
    "padded_event_types = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    event_types, \n",
    "    padding=padding_type, \n",
    "    truncating = truncating_type,\n",
    "    maxlen=padding_len,\n",
    "    value=0,\n",
    "    dtype=object,\n",
    ")\n",
    "padded_event_types = padded_event_types.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_example:[[1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 3 2 2 1 2 3 0 0 0 0 0 0 0 0 0 0]]\n",
      "target_example:[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 3 2 2 1 2 3 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(padded_event_types)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('input_example:{}'.format(input_example))\n",
    "    print('target_example:{}'.format(target_example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.3405\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2035\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2001\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1956\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1912\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 16\n",
    "rnn_units = 32\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = build_model(vocab_size = len(vocab),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS = 5\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (1, None, 16)             80        \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, None, 32)             4800      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, None, 5)              165       \n",
      "=================================================================\n",
      "Total params: 5,045\n",
      "Trainable params: 5,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event(model):\n",
    "    length = 20\n",
    "    input_eval = [1]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    full_seq_str = []\n",
    "    full_seq_int = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(length):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # sample an action\n",
    "        predictions = predictions \n",
    "        predicted_idx = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # pass the predicted action as the next input to the model\n",
    "        input_eval = tf.expand_dims([predicted_idx], 0)\n",
    "\n",
    "        full_seq_str.append(idx2str[predicted_idx] + \" \")\n",
    "        full_seq_int.append(predicted_idx)\n",
    "\n",
    "    return 'start ' + ''.join(full_seq_str), full_seq_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start view view view view click view view start view install install view click view start install start install click view \n",
      "start start click view install start view click view view view install click start view view install install view view na \n",
      "start click install install click install na click na na na na na na na na na na na na na \n",
      "start click click view view view view install start na install click view view start start start install start click view \n",
      "start view start view start click click click install click view install start view install start click start start view install \n",
      "start start start click view view view na na na na na na na na na na na na na na \n",
      "start view view start start view view view install click start start start click view install click view click click view \n",
      "start view view click install start view view click start install click view install start click click view start view install \n",
      "start view install view click view install start view click install click view start view install install click view view start \n",
      "start start click click install view install click install start view view view view view start start view start click install \n",
      "start view view click click click view view click view view view view install view click click install start view install \n",
      "start view start install start install view install na na na na na na na na na na na na na \n",
      "start view start view click view install click view install install install start start install install click start view install click \n",
      "start view view click start view start start start install start view start start install view click view start install install \n",
      "start na na na na na na na na na na na na na na na na na na na na \n",
      "start view start click install click view install click view click start view view install view click install view click view \n",
      "start view click start view view click start start start view install view start click view view install install view click \n",
      "start na na na na na na na na na na na na na na na na na na na na \n",
      "start start view start view click start view install start start click start view view start view view view view click \n",
      "start view start click view view view view view start click view view view start view view view click install install \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    seq_str, seq_int = generate_event(model)\n",
    "    print(seq_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 4, 4]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 20, 16)            80        \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 18, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 4, 32)             3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,785\n",
      "Trainable params: 4,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02621368]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discriminator - CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(len(vocab), 16, input_length=20))\n",
    "cnn_model.add(Conv1D(32, 3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(3))\n",
    "cnn_model.add(Conv1D(32, 3, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(1))\n",
    "cnn_model.summary()\n",
    "cnn_model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "# history = model.fit(x_train, y_train,\n",
    "# epochs=10, batch_size=128, validation_split=0.2)\n",
    "cnn_model.predict(np.array([seq_int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, discriminator, rollout_policy, path):\n",
    "        self.path = path\n",
    "        self.discriminator = discriminator\n",
    "        self.rollout_policy = rollout_policy # an pretrained RNN model\n",
    "        self.base_state = [1]\n",
    "        self.state = self.base_state\n",
    "        self.reset()\n",
    "        self.T = 20\n",
    "        self.n_sample = 100\n",
    "    \n",
    "    def load_model():\n",
    "        tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        model.build(tf.TensorShape([1, None]))\n",
    "        model.summary()\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = t\n",
    "        self.rollout_policy.reset_states()\n",
    "        self.state = self.base_state\n",
    "        return True\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.t = self.t + 1\n",
    "\n",
    "        reward = self.get_reward(action, self.n_sample)\n",
    "        is_episode_end = self.t > self.T\n",
    "\n",
    "        self.state.append(action)\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        return [next_state, reward, is_episode_end]\n",
    "\n",
    "\n",
    "    def get_reward(self, action, n_sample):\n",
    "        reward = 0\n",
    "        for idx_sample in range(n_sample):\n",
    "            full_seq = self.generate_event(state, self.T)\n",
    "            reward += self.discriminator.predict(full_seq) / n_sample\n",
    "        return reward\n",
    "        \n",
    "    def generate_event(state, length):\n",
    "        input_eval = state\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        full_seq = input_eval\n",
    "        # Empty string to store our results\n",
    "        text_generated = []\n",
    "\n",
    "        self.rollout_policy.reset_states()\n",
    "        for i in range(length):\n",
    "            predictions = rollout_policy(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = predictions\n",
    "            predicted_idx = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            \n",
    "            # pass the predicted action as the next input to the model\n",
    "            input_eval = tf.expand_dims([predicted_idx], 0)\n",
    "            text_generated.append(idx2str[predicted_idx])\n",
    "            full_seq.append(predicted_idx)\n",
    "        return full_seq\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plicy gradient update\n",
    "log_prob = tf.log(tf.reduce_mean(prob * action, axis=-1))\n",
    "loss = - log_prob * reward\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "minimize = optimizer.minimize(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
