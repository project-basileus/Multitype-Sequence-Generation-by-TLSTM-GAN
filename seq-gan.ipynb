{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import json\n",
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './event_sequence_data_20200607_hour_08.json'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0]\n",
    "raw_data_small = raw_data[:500000]\n",
    "neg_data = []\n",
    "pos_data = []\n",
    "\n",
    "for r in raw_data_small:\n",
    "    if r['fraud_prob'] > 0:\n",
    "        pos_data.append(r)\n",
    "    else:\n",
    "        neg_data.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "event_types = []\n",
    "labels = []\n",
    "\n",
    "vocab = ['na', 'start', 'view', 'click', 'install']\n",
    "str2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2str= np.array(vocab)\n",
    "\n",
    "for row in raw_data_small:\n",
    "    _event_type_list, _time_list = list(zip(*row['sequence']))\n",
    "    _event_type_list = tuple(str2idx[c] for c in _event_type_list)\n",
    "    # drop missing times for data points 5096332 and 8959733\n",
    "    # otherwise cause problems in creating embeddings\n",
    "    if not any(_time_list): \n",
    "        continue\n",
    "        \n",
    "    labels.append(row['fraud_prob'])    \n",
    "    event_types.append(_event_type_list)\n",
    "\n",
    "    times.append(_time_list)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_len = 20 \n",
    "padding_type = 'post'\n",
    "truncating_type = 'pre'\n",
    "\n",
    "padded_event_types = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    event_types, \n",
    "    padding=padding_type, \n",
    "    truncating = truncating_type,\n",
    "    maxlen=padding_len,\n",
    "    value=0,\n",
    "    dtype=object,\n",
    ")\n",
    "padded_event_types = padded_event_types.astype(int)\n",
    "labels = np.array(labels)\n",
    "train_data_discriminator = padded_event_types,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_event_types[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "400000/400000 [==============================] - 6s 15us/sample - loss: 0.1397 - acc: 0.9906 - val_loss: 0.1406 - val_acc: 0.9905\n",
      "Epoch 2/2\n",
      "400000/400000 [==============================] - 5s 11us/sample - loss: 0.1397 - acc: 0.9906 - val_loss: 0.1406 - val_acc: 0.9905\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Embedding(vocab_size, 16, \\\n",
    "                            input_length=padding_len))\n",
    "discriminator.add(Conv1D(32, 3, activation='relu'))\n",
    "discriminator.add(MaxPooling1D(5))\n",
    "discriminator.add(Conv1D(32, 3, activation='relu'))\n",
    "discriminator.add(GlobalMaxPooling1D())\n",
    "discriminator.add(Dense(1))\n",
    "discriminator.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "history = discriminator.fit(padded_event_types, labels,\n",
    "epochs=2, batch_size=500, validation_split=0.2)\n",
    "\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(train_data_discriminator)\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(500, drop_remainder=True)\n",
    "# discriminator.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "# discriminator.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_padded_event_types:(4719, 20)\n",
      "input_example:[[1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "target_example:[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([12, 19])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_event_types = []\n",
    "for row in pos_data:\n",
    "    _event_type_list, _time_list = list(zip(*row['sequence']))\n",
    "    _event_type_list = tuple(str2idx[c] for c in _event_type_list)\n",
    "    # drop missing times for data points 5096332 and 8959733\n",
    "    # otherwise cause problems in creating embeddings\n",
    "    if not any(_time_list): \n",
    "        continue\n",
    "    pos_event_types.append(_event_type_list)\n",
    "\n",
    "    times.append(_time_list)    \n",
    "\n",
    "pos_padded_event_types = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    pos_event_types, \n",
    "    padding=padding_type, \n",
    "    truncating = truncating_type,\n",
    "    maxlen=padding_len,\n",
    "    value=0,\n",
    "    dtype=object,\n",
    ")\n",
    "pos_padded_event_types = pos_padded_event_types.astype(int)\n",
    "\n",
    "print('pos_padded_event_types:{}'.format(pos_padded_event_types.shape))\n",
    "# auto-regressive data set\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(pos_padded_event_types)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('input_example:{}'.format(input_example))\n",
    "    print('target_example:{}'.format(target_example))\n",
    "input_example.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 19)\n",
      "(10, 19, 5)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 16\n",
    "rnn_units = 32\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = build_model(vocab_size = len(vocab),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(input_example.shape)\n",
    "    print(model(input_example).shape)\n",
    "\n",
    "# model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# checkpoint_dir = './training_checkpoints'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "# checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_prefix,\n",
    "#     save_weights_only=True)\n",
    "\n",
    "# EPOCHS = 2\n",
    "# history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd7645662d0>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "generator = build_model(vocab_size, embedding_dim, rnn_units, batch_size=50)\n",
    "\n",
    "generator.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll-out policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event(model, batch_size=50, length=20):\n",
    "    states = [1] * batch_size\n",
    "    states = tf.expand_dims(states, -1)\n",
    "    all_actions_one_hot = tf.one_hot(states, depth=5)\n",
    "    all_predicted_actions = None\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(length):\n",
    "        predictions = model(states)\n",
    "        if all_predicted_actions is None:\n",
    "            all_predicted_actions = predictions\n",
    "        else:\n",
    "            all_predicted_actions =tf.keras.layers.concatenate([all_predicted_actions, predictions], axis=1)\n",
    "        \n",
    "        # sample an action\n",
    "        predictions = tf.squeeze(predictions)\n",
    "        predicted_idx = tf.random.categorical(predictions, num_samples=1)\n",
    "        predicted_idx_one_hot = tf.one_hot(predicted_idx, depth=5)\n",
    "        all_actions_one_hot = tf.concat([all_actions_one_hot, predicted_idx_one_hot], axis=1)\n",
    "        \n",
    "        # pass the predicted action as the next input to the model\n",
    "        states = predicted_idx\n",
    "\n",
    "    return  all_actions_one_hot, all_predicted_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train-process.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:-0.032061103731393814\n",
      "loss:-0.0318526066839695\n",
      "loss:-0.03146100416779518\n",
      "loss:-0.030350029468536377\n",
      "loss:-0.028879273682832718\n",
      "loss:-0.028719399124383926\n",
      "loss:-0.02949499525129795\n",
      "loss:-0.03095909208059311\n",
      "loss:-0.030988048762083054\n",
      "loss:-0.03166618570685387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_history = []\n",
    "g_steps = 10\n",
    "d_steps = 10\n",
    "# Generator training\n",
    "\n",
    "for i in range(g_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_actions_one_hot, all_predicted_actions = generate_event(generator, length=20)\n",
    "        all_actions_one_hot = all_actions_one_hot[:, 1:,]\n",
    "\n",
    "        # policy gradient loss\n",
    "        negative_likelihoods = tf.keras.losses.categorical_crossentropy\\\n",
    "            (all_actions_one_hot, all_predicted_actions,from_logits=True)\n",
    "        in_dis = tf.argmax(all_actions_one_hot, axis = -1)\n",
    "        q_values = tf.broadcast_to(discriminator(in_dis), negative_likelihoods.shape) # this needs N-time Monte Carlo search\n",
    "        weighted_negative_likelihoods = tf.multiply(negative_likelihoods, q_values)\n",
    "        loss = tf.reduce_mean(weighted_negative_likelihoods)\n",
    "\n",
    "    loss_history.append(loss.numpy())\n",
    "    print('loss:{}'.format(loss.numpy()))\n",
    "    grads = tape.gradient(loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "# Discriminator training\n",
    "for i in range(d_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "    all_actions_one_hot, all_predicted_actions = generate_event(generator, length=20)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n",
      "(50, 20)\n",
      "(50, 20)\n",
      "(50, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 20), dtype=float32, numpy=\n",
       "array([[-0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375,\n",
       "        -0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375,\n",
       "        -0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375,\n",
       "        -0.03283375, -0.03283375, -0.03283375, -0.03283375, -0.03283375],\n",
       "       [-0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174,\n",
       "        -0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174,\n",
       "        -0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174,\n",
       "        -0.02649174, -0.02649174, -0.02649174, -0.02649174, -0.02649174],\n",
       "       [-0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667,\n",
       "        -0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667,\n",
       "        -0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667,\n",
       "        -0.03558667, -0.03558667, -0.03558667, -0.03558667, -0.03558667],\n",
       "       [-0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002,\n",
       "        -0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002,\n",
       "        -0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002,\n",
       "        -0.03349002, -0.03349002, -0.03349002, -0.03349002, -0.03349002],\n",
       "       [-0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753,\n",
       "        -0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753,\n",
       "        -0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753,\n",
       "        -0.02874753, -0.02874753, -0.02874753, -0.02874753, -0.02874753],\n",
       "       [-0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566,\n",
       "        -0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566,\n",
       "        -0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566,\n",
       "        -0.02462566, -0.02462566, -0.02462566, -0.02462566, -0.02462566],\n",
       "       [-0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364,\n",
       "        -0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364,\n",
       "        -0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364,\n",
       "        -0.01087364, -0.01087364, -0.01087364, -0.01087364, -0.01087364],\n",
       "       [-0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629,\n",
       "        -0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629,\n",
       "        -0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629,\n",
       "        -0.03237629, -0.03237629, -0.03237629, -0.03237629, -0.03237629],\n",
       "       [-0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229,\n",
       "        -0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229,\n",
       "        -0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229,\n",
       "        -0.02675229, -0.02675229, -0.02675229, -0.02675229, -0.02675229],\n",
       "       [-0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214,\n",
       "        -0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214,\n",
       "        -0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214,\n",
       "        -0.02304214, -0.02304214, -0.02304214, -0.02304214, -0.02304214],\n",
       "       [-0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897,\n",
       "        -0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897,\n",
       "        -0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897,\n",
       "        -0.01541897, -0.01541897, -0.01541897, -0.01541897, -0.01541897],\n",
       "       [-0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325,\n",
       "        -0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325,\n",
       "        -0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325,\n",
       "        -0.00611325, -0.00611325, -0.00611325, -0.00611325, -0.00611325],\n",
       "       [-0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983,\n",
       "        -0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983,\n",
       "        -0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983,\n",
       "        -0.01516983, -0.01516983, -0.01516983, -0.01516983, -0.01516983],\n",
       "       [-0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973,\n",
       "        -0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973,\n",
       "        -0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973,\n",
       "        -0.02331973, -0.02331973, -0.02331973, -0.02331973, -0.02331973],\n",
       "       [-0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286,\n",
       "        -0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286,\n",
       "        -0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286,\n",
       "        -0.02433286, -0.02433286, -0.02433286, -0.02433286, -0.02433286],\n",
       "       [-0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224,\n",
       "        -0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224,\n",
       "        -0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224,\n",
       "        -0.03852224, -0.03852224, -0.03852224, -0.03852224, -0.03852224],\n",
       "       [-0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032,\n",
       "        -0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032,\n",
       "        -0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032,\n",
       "        -0.03565032, -0.03565032, -0.03565032, -0.03565032, -0.03565032],\n",
       "       [-0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336,\n",
       "        -0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336,\n",
       "        -0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336,\n",
       "        -0.02215336, -0.02215336, -0.02215336, -0.02215336, -0.02215336],\n",
       "       [-0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082,\n",
       "        -0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082,\n",
       "        -0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082,\n",
       "        -0.01518082, -0.01518082, -0.01518082, -0.01518082, -0.01518082],\n",
       "       [-0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196,\n",
       "        -0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196,\n",
       "        -0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196,\n",
       "        -0.03619196, -0.03619196, -0.03619196, -0.03619196, -0.03619196],\n",
       "       [-0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632,\n",
       "        -0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632,\n",
       "        -0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632,\n",
       "        -0.02114632, -0.02114632, -0.02114632, -0.02114632, -0.02114632],\n",
       "       [-0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ,\n",
       "        -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ,\n",
       "        -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ,\n",
       "        -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 , -0.0310711 ],\n",
       "       [-0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372,\n",
       "        -0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372,\n",
       "        -0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372,\n",
       "        -0.03465372, -0.03465372, -0.03465372, -0.03465372, -0.03465372],\n",
       "       [-0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308,\n",
       "        -0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308,\n",
       "        -0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308,\n",
       "        -0.03500308, -0.03500308, -0.03500308, -0.03500308, -0.03500308],\n",
       "       [-0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529,\n",
       "        -0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529,\n",
       "        -0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529,\n",
       "        -0.02933529, -0.02933529, -0.02933529, -0.02933529, -0.02933529],\n",
       "       [-0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649,\n",
       "        -0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649,\n",
       "        -0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649,\n",
       "        -0.02866649, -0.02866649, -0.02866649, -0.02866649, -0.02866649],\n",
       "       [-0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ,\n",
       "        -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ,\n",
       "        -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ,\n",
       "        -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 , -0.0253471 ],\n",
       "       [-0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ,\n",
       "        -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ,\n",
       "        -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ,\n",
       "        -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 , -0.0273164 ],\n",
       "       [-0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028,\n",
       "        -0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028,\n",
       "        -0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028,\n",
       "        -0.03264028, -0.03264028, -0.03264028, -0.03264028, -0.03264028],\n",
       "       [-0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295,\n",
       "        -0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295,\n",
       "        -0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295,\n",
       "        -0.03324295, -0.03324295, -0.03324295, -0.03324295, -0.03324295],\n",
       "       [-0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052,\n",
       "        -0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052,\n",
       "        -0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052,\n",
       "        -0.01925052, -0.01925052, -0.01925052, -0.01925052, -0.01925052],\n",
       "       [-0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641,\n",
       "        -0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641,\n",
       "        -0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641,\n",
       "        -0.03955641, -0.03955641, -0.03955641, -0.03955641, -0.03955641],\n",
       "       [-0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994,\n",
       "        -0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994,\n",
       "        -0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994,\n",
       "        -0.03812994, -0.03812994, -0.03812994, -0.03812994, -0.03812994],\n",
       "       [-0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836,\n",
       "        -0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836,\n",
       "        -0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836,\n",
       "        -0.02646836, -0.02646836, -0.02646836, -0.02646836, -0.02646836],\n",
       "       [-0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719,\n",
       "        -0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719,\n",
       "        -0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719,\n",
       "        -0.04436719, -0.04436719, -0.04436719, -0.04436719, -0.04436719],\n",
       "       [-0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402,\n",
       "        -0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402,\n",
       "        -0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402,\n",
       "        -0.02287402, -0.02287402, -0.02287402, -0.02287402, -0.02287402],\n",
       "       [-0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411,\n",
       "        -0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411,\n",
       "        -0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411,\n",
       "        -0.02254411, -0.02254411, -0.02254411, -0.02254411, -0.02254411],\n",
       "       [-0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806,\n",
       "        -0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806,\n",
       "        -0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806,\n",
       "        -0.04941806, -0.04941806, -0.04941806, -0.04941806, -0.04941806],\n",
       "       [-0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087,\n",
       "        -0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087,\n",
       "        -0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087,\n",
       "        -0.04024087, -0.04024087, -0.04024087, -0.04024087, -0.04024087],\n",
       "       [-0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899,\n",
       "        -0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899,\n",
       "        -0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899,\n",
       "        -0.02028899, -0.02028899, -0.02028899, -0.02028899, -0.02028899],\n",
       "       [-0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358,\n",
       "        -0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358,\n",
       "        -0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358,\n",
       "        -0.03215358, -0.03215358, -0.03215358, -0.03215358, -0.03215358],\n",
       "       [-0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469,\n",
       "        -0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469,\n",
       "        -0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469,\n",
       "        -0.03262469, -0.03262469, -0.03262469, -0.03262469, -0.03262469],\n",
       "       [-0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763,\n",
       "        -0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763,\n",
       "        -0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763,\n",
       "        -0.01614763, -0.01614763, -0.01614763, -0.01614763, -0.01614763],\n",
       "       [-0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ,\n",
       "        -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ,\n",
       "        -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ,\n",
       "        -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 , -0.0198884 ],\n",
       "       [-0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329,\n",
       "        -0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329,\n",
       "        -0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329,\n",
       "        -0.00293329, -0.00293329, -0.00293329, -0.00293329, -0.00293329],\n",
       "       [-0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662,\n",
       "        -0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662,\n",
       "        -0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662,\n",
       "        -0.03419662, -0.03419662, -0.03419662, -0.03419662, -0.03419662],\n",
       "       [-0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218,\n",
       "        -0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218,\n",
       "        -0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218,\n",
       "        -0.03093218, -0.03093218, -0.03093218, -0.03093218, -0.03093218],\n",
       "       [-0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326,\n",
       "        -0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326,\n",
       "        -0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326,\n",
       "        -0.03771326, -0.03771326, -0.03771326, -0.03771326, -0.03771326],\n",
       "       [-0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121,\n",
       "        -0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121,\n",
       "        -0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121,\n",
       "        -0.02647121, -0.02647121, -0.02647121, -0.02647121, -0.02647121],\n",
       "       [-0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747,\n",
       "        -0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747,\n",
       "        -0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747,\n",
       "        -0.02880747, -0.02880747, -0.02880747, -0.02880747, -0.02880747]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_actions_one_hot, all_predicted_actions = generate_event(generator, length=20)\n",
    "all_actions_one_hot = all_actions_one_hot[:, 1:,]\n",
    "all_actions_one_hot\n",
    "# policy gradient loss\n",
    "negative_likelihoods = tf.keras.losses.categorical_crossentropy\\\n",
    "    (all_actions_one_hot, all_predicted_actions,from_logits=True)\n",
    "in_dis = tf.argmax(all_actions_one_hot, axis = -1)\n",
    "print(in_dis.shape)\n",
    "discriminator(in_dis)\n",
    "q_values = tf.broadcast_to(discriminator(in_dis), negative_likelihoods.shape) # this needs N-time Monte Carlo search\n",
    "weighted_negative_likelihoods = tf.multiply(negative_likelihoods, q_values)\n",
    "loss = tf.reduce_mean(weighted_negative_likelihoods)\n",
    "    \n",
    "print(negative_likelihoods.shape)\n",
    "print(in_dis.shape)\n",
    "print(q_values.shape)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mnist_model(images, training=True)\n",
    "    \n",
    "        # Add asserts to check the shape of the output.\n",
    "        tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "        loss_value = loss_object(labels, logits)\n",
    "\n",
    "    loss_history.append(loss_value.numpy().mean())\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    print('grads:{}'.format(grads))\n",
    "    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start view view view view click view view start view install install view click view start install start install click view \n",
      "start start click view install start view click view view view install click start view view install install view view na \n",
      "start click install install click install na click na na na na na na na na na na na na na \n",
      "start click click view view view view install start na install click view view start start start install start click view \n",
      "start view start view start click click click install click view install start view install start click start start view install \n",
      "start start start click view view view na na na na na na na na na na na na na na \n",
      "start view view start start view view view install click start start start click view install click view click click view \n",
      "start view view click install start view view click start install click view install start click click view start view install \n",
      "start view install view click view install start view click install click view start view install install click view view start \n",
      "start start click click install view install click install start view view view view view start start view start click install \n",
      "start view view click click click view view click view view view view install view click click install start view install \n",
      "start view start install start install view install na na na na na na na na na na na na na \n",
      "start view start view click view install click view install install install start start install install click start view install click \n",
      "start view view click start view start start start install start view start start install view click view start install install \n",
      "start na na na na na na na na na na na na na na na na na na na na \n",
      "start view start click install click view install click view click start view view install view click install view click view \n",
      "start view click start view view click start start start view install view start click view view install install view click \n",
      "start na na na na na na na na na na na na na na na na na na na na \n",
      "start start view start view click start view install start start click start view view start view view view view click \n",
      "start view start click view view view view view start click view view view start view view view click install install \n"
     ]
    }
   ],
   "source": [
    "# for i in range(20):\n",
    "seq_str, seq_int = generate_event(model)\n",
    "#     print(seq_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 4, 4]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 20, 16)            80        \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 18, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 4, 32)             3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,785\n",
      "Trainable params: 4,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02621368]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discriminator - CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(len(vocab), 16, input_length=20))\n",
    "cnn_model.add(Conv1D(32, 3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(3))\n",
    "cnn_model.add(Conv1D(32, 3, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(1))\n",
    "cnn_model.summary()\n",
    "cnn_model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy',metrics=['acc'])\n",
    "# history = model.fit(x_train, y_train,\n",
    "# epochs=10, batch_size=128, validation_split=0.2)\n",
    "cnn_model.predict(np.array([seq_int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, discriminator, rollout_policy, path):\n",
    "        self.path = path\n",
    "        self.discriminator = discriminator\n",
    "        self.mdp = rollout_policy # an pretrained RNN model\n",
    "        self.base_state = [1]\n",
    "        self.state = self.base_state\n",
    "        self.reset()\n",
    "        self.T = 20\n",
    "        self.n_sample = 100\n",
    "    \n",
    "    def load_model():\n",
    "        tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        model.build(tf.TensorShape([1, None]))\n",
    "        model.summary()\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = t\n",
    "        self.mdp.reset_states()\n",
    "        self.state = self.base_state\n",
    "        return True\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.t = self.t + 1\n",
    "\n",
    "        reward = self.get_reward(action, self.n_sample)\n",
    "        is_episode_end = self.t > self.T\n",
    "\n",
    "        self.state.append(action)\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        return [next_state, reward, is_episode_end]\n",
    "    \n",
    "    def render(self):\n",
    "        for c in self.state:\n",
    "            \n",
    "        full_seq_str.append(idx2str[predicted_idx] + \" \")\n",
    "        full_seq_int.append(predicted_idx)\n",
    "\n",
    "    return 'start ' + ''.join(full_seq_str), full_seq_int\n",
    "\n",
    "\n",
    "    def get_reward(self, action, n_sample):\n",
    "        reward = 0\n",
    "        for idx_sample in range(n_sample):\n",
    "            full_seq = self.generate_event(state, self.T)\n",
    "            reward += self.discriminator.predict(full_seq) / n_sample\n",
    "        return reward\n",
    "        \n",
    "    def generate_event(state, length):\n",
    "        input_eval = state\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        full_seq = input_eval\n",
    "        # Empty string to store our results\n",
    "        text_generated = []\n",
    "\n",
    "        self.rollout_policy.reset_states()\n",
    "        for i in range(length):\n",
    "            predictions = mdp(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = predictions\n",
    "            predicted_idx = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            \n",
    "            # pass the predicted action as the next input to the model\n",
    "            input_eval = tf.expand_dims([predicted_idx], 0)\n",
    "            text_generated.append(idx2str[predicted_idx])\n",
    "            full_seq.append(predicted_idx)\n",
    "        return full_seq\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
