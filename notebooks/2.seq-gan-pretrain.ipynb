{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = #'.../path-to-module/'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sgtlstm' in sys.modules:\n",
    "    importlib.reload(sys.modules['sgtlstm'])\n",
    "    \n",
    "from sgtlstm.utils import create_dataset\n",
    "from sgtlstm.SeqGan import build_G, build_D\n",
    "from sgtlstm.oracle import get_G_metrics, get_hidden_metrics\n",
    "from sgtlstm.pretrain import pretrain_discriminator, pretrain_generator\n",
    "from sgtlstm.train import generate_sequences\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = # '...path-to-data/positive_long_sequences.pickle'\n",
    "neg_data_path = # '...path-to-data/negative_long_sequences.pickle'\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, time_delta)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, time_delta)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "#     print(seqs.shape)\n",
    "    \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    return et_seqs, ts_seqs\n",
    "    \n",
    "pos_event_type_seqs, pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path)\n",
    "neg_event_type_seqs, neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample negative data\n",
    "N_neg_sample = pos_event_type_seqs.shape[0]\n",
    "neg_sample_idx = np.random.choice(np.arange(0, neg_event_type_seqs.shape[0]), size=N_neg_sample, replace=False)\n",
    "\n",
    "neg_event_type_seqs = neg_event_type_seqs[neg_sample_idx,:,:]\n",
    "neg_timestamp_seqs = neg_timestamp_seqs[neg_sample_idx,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "T = 20 + 1\n",
    "\n",
    "# remove padding token, shift start token to 0\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 6\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_G_et = pos_event_type_seqs\n",
    "pretrain_G_ts = pos_timestamp_seqs\n",
    "pretrain_G_labels = np.ones((pos_event_type_seqs.shape[0], 1))\n",
    "\n",
    "pretrain_G_features = (pretrain_G_et, pretrain_G_ts)\n",
    "N_total_G = pretrain_G_et.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "_TOTAL_STEPS = int(EPOCHS * N_total_G / BATCH_SIZE)\n",
    "\n",
    "\n",
    "pretrain_G_dataset = create_dataset(pretrain_G_features,\n",
    "                                  np.ones((N_total_G, 1)),\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_total_G)\n",
    "\n",
    "\n",
    "pretrain_gen_ce_loss_history = []\n",
    "pretrain_gen_gaussian_loss_history = []\n",
    "pretrain_gen_metrics_history = []\n",
    "\n",
    "\n",
    "pretrained_generator = build_G(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "OPTIMIZER = Adam(lr=1e-3)\n",
    "WEIGHT_GAUSSIAN_LOSS = 1\n",
    "\n",
    "for feature_sample, _ in tqdm(pretrain_G_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    gen_ce_loss, gen_gaussian_loss =  pretrain_generator(feature_sample, \n",
    "                                                         pretrained_generator,\n",
    "                                                         verbose=True, \n",
    "                                                         weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS, \n",
    "                                                         optimizer=OPTIMIZER)\n",
    "    # calculate G metrics \n",
    "    batch_gen_seqs = generate_sequences(BATCH_SIZE, pretrained_generator, batch_size=BATCH_SIZE, T=T, recover_to_timestamp=False)\n",
    "    batch_gen_seqs = np.array(batch_gen_seqs)\n",
    "    pos_sample = np.concatenate([feature_sample[0].numpy(),feature_sample[1].numpy()], axis=2)\n",
    "    # batch_metrics = [rbq, fid, mad, mmd, mmd_et, mmd_ts]\n",
    "    batch_metrics = get_G_metrics(pos_sample, batch_gen_seqs)\n",
    "    print('batch metrics:', batch_metrics)\n",
    "            \n",
    "    pretrain_gen_ce_loss_history.append(gen_ce_loss.numpy())\n",
    "    pretrain_gen_gaussian_loss_history.append(gen_gaussian_loss.numpy())\n",
    "    pretrain_gen_metrics_history.append(batch_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(pretrain_gen_ce_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_gen_ce_loss_history)\n",
    "plt.title('Pre-training Generator Categorical Cross-Entropy Loss History')\n",
    "plt.xlabel('Pre-training steps')\n",
    "\n",
    "x = range(len(pretrain_gen_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_gen_gaussian_loss_history)\n",
    "plt.title('Pre-training Generator Gaussian Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = #'.../path-to-experiment-results/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'pretrain_gen_ce_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_ce_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'pretrain_gen_gaussian_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_gaussian_loss_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_save_dir = #'.../path-to-experiment-results/metrics'\n",
    "if not os.path.exists(metrics_save_dir):\n",
    "    os.makedirs(metrics_save_dir)\n",
    "    \n",
    "with open(os.path.join(metrics_save_dir, 'pretrain_gen_metrics_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_metrics_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pretrained G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = #'.../path-to-experiment-results/models'\n",
    "if not os.path.exists(model_save_dir + '/pretrained_gen_weights'):\n",
    "    os.makedirs(model_save_dir + '/pretrained_gen_weights'')\n",
    "\n",
    "G_save_path = model_save_dir + '/pretrained_gen_weights/model.tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_generator.save_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_gen = build_G(\n",
    "    batch_size = BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_gen.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_gen.load_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_D_et = np.concatenate([pos_event_type_seqs, neg_event_type_seqs], axis=0)\n",
    "pretrain_D_ts = np.concatenate([pos_timestamp_seqs, neg_timestamp_seqs], axis=0)\n",
    "\n",
    "pretrain_D_labels = np.concatenate([np.ones((pos_event_type_seqs.shape[0], 1)), \n",
    "                                  np.zeros((neg_event_type_seqs.shape[0], 1))\n",
    "                                 ], axis=0)\n",
    "pretrain_D_features = (pretrain_D_et, pretrain_D_ts)\n",
    "N_pretrain_D = pretrain_D_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "_TOTAL_STEPS = int(EPOCHS * N_pretrain_D / BATCH_SIZE)\n",
    "\n",
    "pretrain_disc_token_loss_history = []\n",
    "pretrain_disc_gaussian_loss_history = []\n",
    "\n",
    "\n",
    "pretrain_D_dataset = create_dataset(pretrain_D_features,\n",
    "                                  pretrain_D_labels,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_pretrain_D)\n",
    "\n",
    "pretrained_discriminator = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "OPTIMIZER = Adam(lr=1e-4)\n",
    "for features_batch, real_labels in tqdm(pretrain_D_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    disc_token_loss = pretrain_discriminator(features_batch, real_labels, pretrained_discriminator, verbose=True, optimizer=OPTIMIZER)\n",
    "    pretrain_disc_token_loss_history.append(disc_token_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain D: Loss over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(pretrain_disc_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_disc_token_loss_history)\n",
    "plt.title('Pre-training Discriminator CE Loss History')\n",
    "plt.xlabel('Pre-training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = #'.../path-to-experiment-results/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'pretrain_disc_token_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_disc_token_loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pretrained D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = #'.../path-to-experiment-results/models'\n",
    "if not os.path.exists(model_save_dir + '/pretrained_disc_weights'):\n",
    "    os.makedirs(model_save_dir + '/pretrained_disc_weights')\n",
    "    \n",
    "D_save_path = model_save_dir + '/pretrained_disc_weights/pretrained_disc_weights/model.tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_discriminator.save_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_disc.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_disc.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and predict seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_gen = 1000\n",
    "generator = reload_pretrained_gen\n",
    "\n",
    "generated_seqs = generate_sequences(N_gen, generator, batch_size=BATCH_SIZE, T=T, recover_to_timestamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs = np.array(generated_seqs)\n",
    "pred_generated = reload_pretrained_disc((generated_seqs[:,:,[0]], generated_seqs[:,:,[1]]))\n",
    "(pred_generated > 0.5).numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_generated.numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_generated = reload_pretrained_disc((pos_event_type_seqs[:1000,:, :], pos_timestamp_seqs[:1000,:, :]))\n",
    "(pred_generated > 0.5).numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_generated = reload_pretrained_disc((neg_event_type_seqs[:1000,:, :], neg_timestamp_seqs[:1000,:, :]))\n",
    "(pred_generated > 0.5).numpy().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
