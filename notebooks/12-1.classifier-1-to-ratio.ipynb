{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "module_path = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/\"\n",
    "data_path_train = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/long_seqs_v11/\"\n",
    "data_path_val = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/long_seqs_v11_val/\"\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blessed-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_DATA = 400\n",
    "T = 20\n",
    "VOCAB = ['start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 16\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-object",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflected-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence_from_pickle_to_numpy(pickle_file_path, use_init_token=False):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, time_delta)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, time_delta)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "        \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    \n",
    "    return et_seqs, ts_seqs\n",
    "\n",
    "def zcore_norm(data):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    _mean = tf.reduce_mean(data, axis=0)\n",
    "    _mean = tf.expand_dims(_mean, axis=0)\n",
    "    _mean = tf.repeat(_mean, N, axis=0)\n",
    "\n",
    "    _std = tf.math.reduce_std(data, axis=0)\n",
    "    _std = tf.expand_dims(_std, axis=0)\n",
    "    _std = tf.repeat(_std, N, axis=0)\n",
    "\n",
    "    return (data - _mean) / _std, _mean[0,:,:], _std[0,:,:]\n",
    "\n",
    "def zcore_norm_2(data, _mean, _std):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    _mean = tf.expand_dims(_mean, axis=0)\n",
    "    _mean = tf.repeat(_mean, N, axis=0)\n",
    "\n",
    "    _std = tf.expand_dims(_std, axis=0)\n",
    "    _std = tf.repeat(_std, N, axis=0)\n",
    "\n",
    "    return (data - _mean) / _std\n",
    "\n",
    "def get_mean_std(data):\n",
    "    _mean = tf.reduce_mean(data, axis=0)\n",
    "    _mean = tf.expand_dims(_mean, axis=0)\n",
    "\n",
    "    _std = tf.math.reduce_std(data, axis=0)\n",
    "    _std = tf.expand_dims(_std, axis=0)\n",
    "\n",
    "    return _mean, _std\n",
    "\n",
    "def apply_mean_std(data, _mean, _std):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    _mean = tf.reshape(_mean, (1, T, 1))\n",
    "    _mean = tf.repeat(_mean, N, axis=0)\n",
    "    \n",
    "    _std = tf.reshape(_std, (1, T, 1))\n",
    "    _std = tf.repeat(_std, N, axis=0)\n",
    "    \n",
    "    return data * _std + _mean\n",
    "\n",
    "# pos_timestamp_seqs, GLOBAL_MEAN_POS, GLOBAL_STD_POS = zcore_norm(raw_pos_timestamp_seqs)\n",
    "# neg_timestamp_seqs, GLOBAL_MEAN_NEG, GLOBAL_STD_NEG = zcore_norm(raw_neg_timestamp_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "atlantic-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_process(pos_data_path, neg_data_path, ratio=500):\n",
    "    raw_pos_event_type_seqs, raw_pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path, use_init_token=False)\n",
    "    raw_neg_event_type_seqs, raw_neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path, use_init_token=False)\n",
    "    \n",
    "    # cast indicator data into one-hot\n",
    "    pos_event_type_seqs = tf.cast(raw_pos_event_type_seqs, tf.int32)\n",
    "    pos_event_type_seqs = tf.one_hot(pos_event_type_seqs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    pos_event_type_seqs = tf.squeeze(pos_event_type_seqs, axis=3)\n",
    "\n",
    "    # cast indicator data into one-hot\n",
    "    neg_event_type_seqs = tf.cast(raw_neg_event_type_seqs, tf.int32)\n",
    "    neg_event_type_seqs = tf.one_hot(neg_event_type_seqs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    neg_event_type_seqs = tf.squeeze(neg_event_type_seqs, axis=3)\n",
    "    \n",
    "    pos_timestamp_seqs, _mean_pos, _std_pos = zcore_norm(raw_pos_timestamp_seqs)\n",
    "    neg_timestamp_seqs, _mean_neg, _std_neg = zcore_norm(raw_neg_timestamp_seqs)\n",
    "    \n",
    "    pos_event_type_seqs, pos_timestamp_seqs = pos_event_type_seqs[:N_DATA, :, :], pos_timestamp_seqs[:N_DATA, :, :]\n",
    "    neg_event_type_seqs, neg_timestamp_seqs = neg_event_type_seqs[:int(N_DATA*ratio), :, :], neg_timestamp_seqs[:int(N_DATA*ratio), :, :]\n",
    "    \n",
    "    return pos_event_type_seqs, pos_timestamp_seqs, neg_event_type_seqs, neg_timestamp_seqs, _mean_pos, _std_pos, _mean_neg, _std_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedicated-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path_train = os.path.join(data_path_train, 'positive_long_sequences.pickle')\n",
    "neg_data_path_train = os.path.join(data_path_train, 'negative_long_sequences.pickle')\n",
    "\n",
    "pos_data_path_val = os.path.join(data_path_val, 'positive_long_sequences.pickle')\n",
    "neg_data_path_val = os.path.join(data_path_val, 'negative_long_sequences.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "official-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_event_type_seqs_train, pos_timestamp_seqs_train, neg_event_type_seqs_train, neg_timestamp_seqs_train, GLOBAL_MEAN_POS, GLOBAL_STD_POS, GLOBAL_MEAN_NEG, GLOBAL_STD_NEG = load_data_and_process(pos_data_path_train, neg_data_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stylish-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_event_type_seqs_val, pos_timestamp_seqs_val, neg_event_type_seqs_val, neg_timestamp_seqs_val, _, _, _, _ = load_data_and_process(pos_data_path_val, neg_data_path_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-organic",
   "metadata": {},
   "source": [
    "## Create multitype SeqGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense, Dropout, Activation, Multiply, Add, Lambda\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy, Precision, Recall\n",
    "\n",
    "\n",
    "def build_classifier(batch_size, T, event_vocab_dim, emb_dim, hidden_dim, dropout_rate=0.25):\n",
    "    # normal LSTM\n",
    "    i_et = Input(batch_shape=(batch_size, None, event_vocab_dim), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(batch_shape=(batch_size, None, 1), name='time_delta_in')  # input of continuous feature timestamp\n",
    "    \n",
    "    embed0 = Dense(emb_dim, name='dense_emb')(i_et) # dense matrix size: 6*16\n",
    "    merged0 = tf.concat([embed0, i_ts], axis=2)\n",
    "    \n",
    "    hm = LSTM(hidden_dim,\n",
    "          name='lstm_token',\n",
    "          stateful=False,\n",
    "          return_sequences=False, \n",
    "          kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          recurrent_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))(merged0)\n",
    "\n",
    "    tm = LSTM(hidden_dim,\n",
    "          name='lstm_time',\n",
    "          stateful=False,\n",
    "          return_sequences=False,\n",
    "          kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          recurrent_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))(merged0)\n",
    "    \n",
    "    token_time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "    \n",
    "    dropped = Dropout(rate=dropout_rate)(token_time_comb)\n",
    "    \n",
    "    prob = Dense(1, \n",
    "             activation='sigmoid',\n",
    "             name='final',\n",
    "             kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "             bias_initializer=tf.keras.initializers.Constant(value=0.1))(dropped)\n",
    "        \n",
    "    classifier = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=prob)\n",
    "        \n",
    "    metrics = [\n",
    "        BinaryAccuracy(name='accuracy'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        AUC(num_thresholds=500, curve='PR', name='auc_pr'),\n",
    "        AUC(num_thresholds=500, curve='ROC', name='auc_roc')\n",
    "    ]\n",
    "    \n",
    "    classifier.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "        metrics=metrics)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attractive-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "provincial-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "event_type (InputLayer)         [(256, None, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_emb (Dense)               (256, None, 16)      80          event_type[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_delta_in (InputLayer)      [(256, None, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (256, None, 17)      0           dense_emb[0][0]                  \n",
      "                                                                 time_delta_in[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_token (LSTM)               (256, 128)           74752       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_time (LSTM)                (256, 128)           74752       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (256, 256)           0           lstm_token[0][0]                 \n",
      "                                                                 lstm_time[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (256, 256)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "final (Dense)                   (256, 1)             257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 149,841\n",
      "Trainable params: 149,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-internship",
   "metadata": {},
   "source": [
    "## Load Train and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "supreme-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pos = pos_event_type_seqs_train.shape[0]\n",
    "N_neg = neg_event_type_seqs_train.shape[0]\n",
    "\n",
    "# def split(data):\n",
    "#     N = data.shape[0]\n",
    "#     train_data, val_data, test_data = data[:int(0.6*N),:,:], data[int(0.6*N):int(1*N),:,:], data[int(1*N):,:,:]\n",
    "#     return train_data, val_data, test_data\n",
    "\n",
    "train_pos_et, val_pos_et = pos_event_type_seqs_train, pos_event_type_seqs_val\n",
    "train_pos_ts, val_pos_ts = pos_timestamp_seqs_train, pos_timestamp_seqs_val\n",
    "\n",
    "train_neg_et, val_neg_et = neg_event_type_seqs_train, neg_event_type_seqs_val\n",
    "train_neg_ts, val_neg_ts = neg_timestamp_seqs_train, neg_timestamp_seqs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pressed-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(features: np.array, labels: np.array, batch_size=2, epochs=10, buffer_size=10000):\n",
    "    \"\"\"\n",
    "    Create TF dataset from\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def combine_seqs_to_dataset(pos_et, pos_ts, neg_et, neg_ts, batch_size, epochs):\n",
    "    _N_pos = pos_et.shape[0]\n",
    "    _N_neg = neg_et.shape[0]\n",
    "\n",
    "    _ets = tf.concat([pos_et, neg_et], axis=0)\n",
    "    _tss = tf.concat([pos_ts, neg_ts], axis=0)\n",
    "    _labels = tf.concat([np.ones((_N_pos, 1)), np.zeros((_N_neg, 1))], axis=0)\n",
    "\n",
    "    _dataset = create_tf_dataset((_ets, _tss), \n",
    "                                 _labels,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 buffer_size=_N_pos + _N_neg) # shuffle the entire Dataset\n",
    "    \n",
    "    return _dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "organic-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = combine_seqs_to_dataset(val_pos_et, val_pos_ts, val_neg_et, val_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "passive-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import TensorSpec\n",
    "element_spec = ((TensorSpec(shape=(BATCH_SIZE, T, 4), dtype=tf.float64, name=None), \n",
    "                 TensorSpec(shape=(BATCH_SIZE, T, 1), dtype=tf.float64, name=None)),\n",
    "                TensorSpec(shape=(BATCH_SIZE, 1), dtype=tf.float64, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-standalone-val.tf.data')\n",
    "tf.data.experimental.save(val_dataset, dataset_save_path)\n",
    "val_dataset = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-glory",
   "metadata": {},
   "source": [
    "## Train with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining imbalanced data, no augmentations\n",
    "train_dataset_im = combine_seqs_to_dataset(train_pos_et, train_pos_ts, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-im.tf.data')\n",
    "tf.data.experimental.save(train_dataset_im, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_im = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_im = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_gs_im = classifier_im.fit(train_dataset_im, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-beauty",
   "metadata": {},
   "source": [
    "## Train with upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train_pos = train_pos_et.shape[0]\n",
    "N_train_neg = train_neg_et.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_indices = np.random.choice(N_train_pos, size=N_train_neg, replace=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_train_pos_et = tf.gather(train_pos_et, upsampled_indices)\n",
    "upsampled_train_pos_ts = tf.gather(train_pos_ts, upsampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_up = combine_seqs_to_dataset(upsampled_train_pos_et, upsampled_train_pos_ts, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-up.tf.data')\n",
    "tf.data.experimental.save(train_dataset_up, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_up = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_up = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_up = classifier_up.fit(train_dataset_up, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-plate",
   "metadata": {},
   "source": [
    "## Train Using Gumbel-Softmax generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "macro-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G2_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G2/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G2_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G2/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "artificial-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G2_gs_path, 'rb') as f:\n",
    "    syn_pos_et_G2_gs = pickle.load(f)\n",
    "    syn_pos_et_G2_gs = tf.one_hot(syn_pos_et_G2_gs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G2_gs = tf.squeeze(syn_pos_et_G2_gs, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G2_gs_path, 'rb') as f:\n",
    "    syn_pos_ts_G2_gs = pickle.load(f)\n",
    "    syn_pos_ts_G2_gs = zcore_norm_2(syn_pos_ts_G2_gs, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "indie-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_syn_data(syn_pos_et, syn_pos_ts, train_pos_et, train_pos_ts):\n",
    "    N_train_pos = train_pos_et.shape[0]\n",
    "    N_train_neg = train_neg_et.shape[0]\n",
    "    N_syn_sample = N_train_neg - N_train_pos\n",
    "    \n",
    "    syn_sample_indices = np.random.choice(syn_pos_et.shape[0], size=N_syn_sample, replace=False).tolist()    \n",
    "    \n",
    "    sampled_syn_pos_et = tf.gather(syn_pos_et, syn_sample_indices)\n",
    "    sampled_syn_pos_ts = tf.gather(syn_pos_ts, syn_sample_indices)\n",
    "    \n",
    "    aug_pos_et = tf.concat([train_pos_et, sampled_syn_pos_et], axis=0)\n",
    "    aug_pos_ts = tf.concat([train_pos_ts, sampled_syn_pos_ts], axis=0)\n",
    "    \n",
    "    assert(aug_pos_et.shape[0] == aug_pos_ts.shape[0] ==N_train_neg)\n",
    "    \n",
    "    return aug_pos_et, aug_pos_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bound-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_gs_G2, aug_pos_ts_gs_G2 = sample_from_syn_data(syn_pos_et_G2_gs, syn_pos_ts_G2_gs, train_pos_et, train_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "handled-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_gs_G2 = combine_seqs_to_dataset(aug_pos_et_gs_G2, aug_pos_ts_gs_G2, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-gs-G2.tf.data')\n",
    "tf.data.experimental.save(train_dataset_gs_G2, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_gs_G2 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "renewable-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs_G2 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "toxic-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 620/1562 [==========>...................] - ETA: 1:35 - loss: 0.0865 - accuracy: 0.9685 - precision: 0.9663 - recall: 0.9703 - auc_pr: 0.9912 - auc_roc: 0.9913"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-91b62f67f553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                        )\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/envs/basileus/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_gs_g2 = classifier_gs_G2.fit(train_dataset_gs_G2, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-track",
   "metadata": {},
   "source": [
    "## Train Using GS MLE generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G1_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G1/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G1_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G1/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G1_gs_path, 'rb') as f:\n",
    "    syn_pos_et_G1_gs = pickle.load(f)\n",
    "    syn_pos_et_G1_gs = tf.one_hot(syn_pos_et_G1_gs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G1_gs = tf.squeeze(syn_pos_et_G1_gs, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G1_gs_path, 'rb') as f:\n",
    "    syn_pos_ts_G1_gs = pickle.load(f)\n",
    "    syn_pos_ts_G1_gs = zcore_norm_2(syn_pos_ts_G1_gs, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_gs_G1, aug_pos_ts_gs_G1 = sample_from_syn_data(syn_pos_et_G1_gs, syn_pos_ts_G1_gs, train_pos_et, train_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_gs_G1 = combine_seqs_to_dataset(aug_pos_et_gs_G1, aug_pos_ts_gs_G1, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-gs-G1.tf.data')\n",
    "tf.data.experimental.save(train_dataset_gs_G1, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_gs_G1 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs_G1 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_g1 = classifier_gs_G1.fit(train_dataset_gs_G1, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-berlin",
   "metadata": {},
   "source": [
    "## Train Using GS Random G0 generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G0_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G0/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G0_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G0/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G0_gs_path, 'rb') as f:\n",
    "    syn_pos_et_G0_gs = pickle.load(f)\n",
    "    syn_pos_et_G0_gs = tf.one_hot(syn_pos_et_G0_gs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G0_gs = tf.squeeze(syn_pos_et_G0_gs, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G0_gs_path, 'rb') as f:\n",
    "    syn_pos_ts_G0_gs = pickle.load(f)\n",
    "    syn_pos_ts_G0_gs = zcore_norm_2(syn_pos_ts_G0_gs, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_gs_G0, aug_pos_ts_gs_G0 = sample_from_syn_data(syn_pos_et_G0_gs, syn_pos_ts_G0_gs, train_pos_et, train_pos_ts)\n",
    "train_dataset_gs_G0 = combine_seqs_to_dataset(aug_pos_et_gs_G0, aug_pos_ts_gs_G0, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-gs-G0.tf.data')\n",
    "tf.data.experimental.save(train_dataset_gs_G0, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_gs_G0 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs_G0 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_g0 = classifier_gs_G0.fit(train_dataset_gs_G0,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-least",
   "metadata": {},
   "source": [
    "## Train Using RL MCC generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G2_mcc_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/monte_carlo_critic_2021-04-02-09-25-47/G2/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G2_mcc_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/monte_carlo_critic_2021-04-02-09-25-47/G2/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G2_mcc_path, 'rb') as f:\n",
    "    syn_pos_et_G2_mcc = pickle.load(f)\n",
    "    syn_pos_et_G2_mcc = tf.one_hot(syn_pos_et_G2_mcc, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G2_mcc = tf.squeeze(syn_pos_et_G2_mcc, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G2_mcc_path, 'rb') as f:\n",
    "    syn_pos_ts_G2_mcc = pickle.load(f)\n",
    "    syn_pos_ts_G2_mcc = zcore_norm_2(syn_pos_ts_G2_mcc, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_syn_data(syn_pos_et, syn_pos_ts, train_pos_et, train_pos_ts):\n",
    "    N_train_pos = train_pos_et.shape[0]\n",
    "    N_train_neg = train_neg_et.shape[0]\n",
    "    N_syn_sample = N_train_neg - N_train_pos\n",
    "    \n",
    "    syn_sample_indices = np.random.choice(syn_pos_et.shape[0], size=N_syn_sample, replace=False).tolist()    \n",
    "    \n",
    "    sampled_syn_pos_et = tf.gather(syn_pos_et, syn_sample_indices)\n",
    "    sampled_syn_pos_ts = tf.gather(syn_pos_ts, syn_sample_indices)\n",
    "    \n",
    "    aug_pos_et = tf.concat([train_pos_et, sampled_syn_pos_et], axis=0)\n",
    "    aug_pos_ts = tf.concat([train_pos_ts, sampled_syn_pos_ts], axis=0)\n",
    "    \n",
    "    assert(aug_pos_et.shape[0] == aug_pos_ts.shape[0] ==N_train_neg)\n",
    "    \n",
    "    return aug_pos_et, aug_pos_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_mcc_G2, aug_pos_ts_mcc_G2 = sample_from_syn_data(syn_pos_et_G2_mcc, syn_pos_ts_G2_mcc, train_pos_et, train_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mcc_G2 = combine_seqs_to_dataset(aug_pos_et_mcc_G2, aug_pos_ts_mcc_G2, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-mcc-G2.tf.data')\n",
    "tf.data.experimental.save(train_dataset_mcc_G2, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_mcc_G2 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mcc_G2 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_mcc_g2 = classifier_mcc_G2.fit(train_dataset_mcc_G2, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-prefix",
   "metadata": {},
   "source": [
    "## Train Using MCC MLE generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G1_mcc_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/monte_carlo_critic_2021-04-02-09-25-47/G1/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G1_mcc_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/monte_carlo_critic_2021-04-02-09-25-47/G1/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G1_mcc_path, 'rb') as f:\n",
    "    syn_pos_et_G1_mcc = pickle.load(f)\n",
    "    syn_pos_et_G1_mcc = tf.one_hot(syn_pos_et_G1_mcc, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G1_mcc = tf.squeeze(syn_pos_et_G1_mcc, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G1_mcc_path, 'rb') as f:\n",
    "    syn_pos_ts_G1_mcc = pickle.load(f)\n",
    "    syn_pos_ts_G1_mcc = zcore_norm_2(syn_pos_ts_G1_mcc, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_mcc_G1, aug_pos_ts_mcc_G1 = sample_from_syn_data(syn_pos_et_G1_mcc, syn_pos_ts_G1_mcc, train_pos_et, train_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mcc_G1 = combine_seqs_to_dataset(aug_pos_et_mcc_G1, aug_pos_ts_mcc_G1, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-500-mcc-G1.tf.data')\n",
    "tf.data.experimental.save(train_dataset_mcc_G1, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_mcc_G1 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mcc_G1 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3\n",
    ")\n",
    "history_mcc_g1 = classifier_mcc_G1.fit(train_dataset_mcc_G1, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-reliance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
