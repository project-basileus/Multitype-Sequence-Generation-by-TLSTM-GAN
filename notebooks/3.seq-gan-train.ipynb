{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = #'.../path-to-module/'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sgtlstm' in sys.modules:\n",
    "    importlib.reload(sys.modules['sgtlstm'])\n",
    "\n",
    "from sgtlstm.utils import create_dataset, recover_timedelta_to_timestamp\n",
    "from sgtlstm.SeqGan import build_G, build_D, build_critic\n",
    "from sgtlstm.oracle import get_G_metrics, get_hidden_metrics\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "from sgtlstm.train import train_discriminator, train_generator, generate_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = # '...path-to-data/positive_long_sequences.pickle'\n",
    "neg_data_path = # '...path-to-data/negative_long_sequences.pickle'\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, time_delta)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, time_delta)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "#     print(seqs.shape)\n",
    "    \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    return et_seqs, ts_seqs\n",
    "    \n",
    "pos_event_type_seqs, pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path)\n",
    "neg_event_type_seqs, neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "T = 20 + 1\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 6\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "discriminator.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "D_save_path = #'.../path-to-experiment-results/models/pretrained_disc_weights/model.tf'\n",
    "discriminator.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_G(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)\n",
    "\n",
    "generator.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "G_save_path = #'.../path-to-experiment-results/models/pretrained_gen_weights/model.tf'\n",
    "generator.load_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = build_critic(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_et = pos_event_type_seqs\n",
    "train_ts = pos_timestamp_seqs\n",
    "\n",
    "train_labels = np.ones((pos_event_type_seqs.shape[0], 1))\n",
    "\n",
    "train_features = (train_et, train_ts)\n",
    "N_train = train_et.shape[0]\n",
    "N_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "dataset = create_dataset(train_features,\n",
    "                         train_labels,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=EPOCHS,\n",
    "                         buffer_size=N_train)\n",
    "\n",
    "gen_token_loss_history = []\n",
    "gen_gaussian_loss_history = []\n",
    "disc_ce_loss_history = []\n",
    "critic_network_loss_history = []\n",
    "average_true_return_history = []\n",
    "gen_metrics_history = []\n",
    "hidden_metrics_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS =  int(EPOCHS * N_train / BATCH_SIZE)\n",
    "step = 0\n",
    "_TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G_optimizer = SGD(learning_rate=1e-4)\n",
    "D_optimizer = SGD(learning_rate=1e-4)\n",
    "\n",
    "WEIGHT_GAUSSIAN_LOSS = 1\n",
    "_G_STEPS = 1\n",
    "_D_STEPS = 1\n",
    "\n",
    "for features_batch, _ in tqdm(dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "    # train the generator\n",
    "    for _ in range(_G_STEPS):\n",
    "        gen_token_loss, gen_gaussian_loss, critic_network_loss, average_true_return = train_generator(generator, discriminator, critic, \n",
    "                                                                                 batch_size=BATCH_SIZE, T=T, verbose=True, \n",
    "                                                                                 weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS,\n",
    "                                                                                 optimizer=G_optimizer\n",
    "                                                                                )\n",
    "        gen_token_loss_history.append(gen_token_loss.numpy())\n",
    "        gen_gaussian_loss_history.append(gen_gaussian_loss.numpy())    \n",
    "        critic_network_loss_history.append(critic_network_loss.numpy())\n",
    "        average_true_return_history.append(average_true_return.numpy())\n",
    "    \n",
    "    # train the discriminator\n",
    "    for _ in range(_D_STEPS):\n",
    "        disc_ce_loss = train_discriminator(features_batch, generator, discriminator, \n",
    "                                           batch_size=BATCH_SIZE, T=T, verbose=True, \n",
    "                                           optimizer=D_optimizer)\n",
    "        disc_ce_loss_history.append(disc_ce_loss.numpy())\n",
    "        \n",
    "    # calculate G  metrics \n",
    "    batch_gen_seqs = generate_sequences(BATCH_SIZE, generator, batch_size=BATCH_SIZE, T=T, recover_to_timestamp=False)\n",
    "    batch_gen_seqs = np.array(batch_gen_seqs)\n",
    "    pos_sample = np.concatenate([features_batch[0].numpy(),features_batch[1].numpy()], axis=2)\n",
    "    # batch_metrics : [rbq, fid, mad, mmd, mmd_et, mmd_ts]\n",
    "    batch_metrics = get_G_metrics(pos_sample, batch_gen_seqs)\n",
    "    print('batch metrics:', batch_metrics)\n",
    "    gen_metrics_history.append(batch_metrics)\n",
    "    \n",
    "    # calculate hidden metrics\n",
    "    pos_time_comb = discriminator(features_batch)[1]\n",
    "    batch_time_comb = discriminator([batch_gen_seqs[:,:,[0]], batch_gen_seqs[:,:,[1]]])[1]\n",
    "    # hidden_metrics : [fid, mmd]\n",
    "    hidden_metrics = get_hidden_metrics(pos_time_comb, batch_time_comb)\n",
    "    print('hidden metrics:', hidden_metrics)\n",
    "    hidden_metrics_history.append(hidden_metrics)\n",
    "    \n",
    "    # save weights every 200 steps\n",
    "    if step % 100 == 0:\n",
    "        print('Saving weights...')\n",
    "        save_path_prefix = f'/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v10/oracle_train_{step}'\n",
    "        save_model_weights(save_path_prefix, generator, discriminator, critic)\n",
    "        print('All Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(gen_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, gen_token_loss_history)\n",
    "plt.title('Generator Toke Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "x = range(len(gen_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, gen_gaussian_loss_history)\n",
    "plt.title('Generator Gaussian Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "x = range(len(disc_ce_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, disc_ce_loss_history)\n",
    "plt.title('Discriminator CE Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(critic_network_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, critic_network_loss_history)\n",
    "plt.title('Critic Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "\n",
    "x = range(len(average_true_return_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, average_true_return_history)\n",
    "plt.title('Average True Return History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_gen = 100\n",
    "generated_seqs = generate_sequences(N_gen, generator, batch_size=BATCH_SIZE, T=T, recover_to_timestamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the purpose of a performance test\n",
    "# we can save this sequence using np.save to '.../path-to-gan-generated/performance_test/'\n",
    "generated_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict use GAN trained D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs = np.array(generated_seqs)\n",
    "pred_1, _ = discriminator((generated_seqs[:,:,[0]], generated_seqs[:,:,[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1.numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict use pre-trained D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_D = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_D.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "D_save_path = #'.../path-to-experiment-results/models/pretrained_disc_weights/model.tf'\n",
    "reload_pretrained_D.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs = np.array(generated_seqs)\n",
    "pred_2, _ = reload_pretrained_D((generated_seqs[:,:,[0]], generated_seqs[:,:,[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2.numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save G and D models and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = #'.../path-to-experiment-results/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'gen_token_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(gen_token_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'gen_gaussian_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(gen_gaussian_loss_history, f)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'critic_network_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(critic_network_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'disc_ce_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(disc_ce_loss_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_save_dir = #'.../path-to-experiment-results/metrics'\n",
    "if not os.path.exists(metrics_save_dir):\n",
    "    os.makedirs(metrics_save_dir)\n",
    "    \n",
    "with open(os.path.join(metrics_save_dir, 'gen_metrics_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(gen_metrics_history, f)\n",
    "\n",
    "with open(os.path.join(metrics_save_dir, 'hidden_metrics_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(hidden_metrics_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_save_dir = #'.../path-to-experiment-results/models/disc_weights'\n",
    "if not os.path.exists(D_save_dir):\n",
    "    os.makedirs(D_save_dir)\n",
    "    \n",
    "D_save_path = os.path.join(D_save_dir, 'disc_model.tf')\n",
    "discriminator.save_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_save_dir = #'.../path-to-experiment-results/models/gen_weights'\n",
    "if not os.path.exists(G_save_dir):\n",
    "    os.makedirs(G_save_dir)\n",
    "    \n",
    "G_save_path = os.path.join(G_save_dir, 'gen_model.tf')\n",
    "generator.save_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_save_dir = #'.../path-to-experiment-results/models/critic_weights'\n",
    "if not os.path.exists(critic_save_dir):\n",
    "    os.makedirs(critic_save_dir)\n",
    "    \n",
    "critic_save_path = os.path.join(critic_save_dir, 'critic_model.tf')\n",
    "critic.save_weights(critic_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(save_path_prefix, G, D, critic):\n",
    "    G_save_path = os.path.join(save_path_prefix, 'gen_weights',  'gen_model.tf')\n",
    "    G.save_weights(G_save_path)\n",
    "    print('G saved to:', G_save_path)\n",
    "    \n",
    "    D_save_path = os.path.join(save_path_prefix, 'disc_weights', 'disc_model.tf')\n",
    "    D.save_weights(D_save_path)\n",
    "    print('D saved to:', D_save_path)\n",
    "    \n",
    "    critic_save_path = os.path.join(save_path_prefix, 'critic_weights', 'critic_model.tf')\n",
    "    critic.save_weights(critic_save_path)                \n",
    "    print('Critic saved to:', critic_save_path)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
