{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = '/home/lun/project-basileus/seq-gan/'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from tensorflow_probability import distributions as tfd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sgtlstm' in sys.modules:\n",
    "    importlib.reload(sys.modules['sgtlstm'])\n",
    "    \n",
    "from sgtlstm.utils import create_dataset\n",
    "from sgtlstm.SeqGan import build_G, build_D\n",
    "from sgtlstm.pretrain import pretrain_generator, pretrain_discriminator, create_self_regression_data_batch\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# path-to-v7: gs://unity-ads-ml-fraud-data-prd/pickcle/seq_gan/long_seqs_v7\n",
    "# path-to-v8: gs://unity-ads-ml-fraud-data-prd/pickcle/seq_gan/long_seqs_v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v8/positive_long_sequences.pickle'\n",
    "neg_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v8/negative_long_sequences.pickle'\n",
    "all_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v8/all_long_sequences.pickle'\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, time_delta)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, time_delta)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "#     print(seqs.shape)\n",
    "    \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    return et_seqs, ts_seqs\n",
    "    \n",
    "pos_event_type_seqs, pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path)\n",
    "neg_event_type_seqs, neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path)\n",
    "# all_event_type_seqs, all_timestamp_seqs = load_sequence_from_pickle_to_numpy(all_data_path)\n",
    "all_event_type_seqs = np.concatenate([pos_event_type_seqs, neg_event_type_seqs], axis=0)\n",
    "all_timestamp_seqs = np.concatenate([pos_timestamp_seqs, neg_timestamp_seqs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_event_type_seqs.shape[0] / neg_event_type_seqs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timestamp_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "T = 20 + 1\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 6\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense, Dropout\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "def build_D_2(T, event_vocab_dim, emb_dim, hidden_dim=11):\n",
    "    \"\"\"\n",
    "        Build a discriminator for event type sequence of shape (batch_size, T, input_dim)\n",
    "        and input event type sequence of shape (batch_size, T, 1)\n",
    "    :param T: length of the sequence\n",
    "    :param event_vocab_dim: size of event vocabulary ['na', 'init', 'start', 'view', 'click', 'install']\n",
    "    :param emb_dim: dimension of the embedding layer output for event type\n",
    "    :param hidden_dim: dimension hidden of the time lstm cell\n",
    "    :return: discriminator D\n",
    "    \"\"\"\n",
    "    # Time-LSTM:\n",
    "    i_et = Input(shape=(T, 1), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(shape=(T, 1), name='time_delta')  # input of continuous feature timestamp\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(T, 1))\n",
    "    masked_ts = mask_layer(i_ts)\n",
    "    masked_et = mask_layer(i_et)\n",
    "\n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, input_length=T, mask_zero=True)(masked_et)\n",
    "    embed0 = Reshape((T, emb_dim))(embed0)  # shape=[Batch_size, T, emb_dim]\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, masked_ts], axis=2)  # # shape=[Batch_size, T, emb_dim + time_dim]\n",
    "\n",
    "    hm, tm = TimeLSTM1(hidden_dim, activation='selu', name='time_lstm', return_sequences=False)(merged0)\n",
    "\n",
    "    time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "\n",
    "    # predicted real prob\n",
    "    real_prob = Dense(1, activation='sigmoid', name='fraud_prob', kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3))(\n",
    "        time_comb)\n",
    "\n",
    "    discriminator = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=[real_prob])\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def build_G_2(batch_size, event_vocab_dim, emb_dim, hidden_dim=11):\n",
    "    \"\"\"\n",
    "        Build a generator for event type sequence of shape (batch_size, T, input_dim)\n",
    "        and input event type sequence of shape (batch_size, T, 1)\n",
    "    :param batch_size: batch size must been specified at generator\n",
    "    :param event_vocab_dim: size of event vocabulary ['na', 'start', 'click', 'install']\n",
    "    :param emb_dim: dimension of the embedding layer output for event type\n",
    "    :param hidden_dim: dimension hidden of the time lstm cell\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Time-LSTM:\n",
    "    i_et = Input(batch_shape=(batch_size, None, 1), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(batch_shape=(batch_size, None, 1), name='time_delta')  # input of continuous feature timestamp\n",
    "\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(None, 1))\n",
    "    masked_et = mask_layer(i_et)\n",
    "    masked_ts = mask_layer(i_ts)\n",
    "\n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, mask_zero=True)(masked_et)\n",
    "    embed0 = Reshape([1, emb_dim])(embed0)\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, masked_ts], axis=2)\n",
    "\n",
    "    hm, tm = TimeLSTM1(hidden_dim, activation='selu', name='time_lstm',\n",
    "                       stateful=True, return_sequences=False)(merged0)\n",
    "    \n",
    "    time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "    \n",
    "    dense_time = Dense(hidden_dim // 2, activation='linear', name='dense_time')(time_comb)\n",
    "    dense_token = Dropout(rate=0.4)(dense_time)\n",
    "    time_out = Dense(1 + 1, activation='linear', name='output')(dense_token)\n",
    "    time_out = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[..., :1],\n",
    "                             scale=1 + tf.math.softplus(t[..., 1:])),\n",
    "        name='Normal')(time_out)\n",
    "\n",
    "    # predicted prob of next token\n",
    "    dense_token = Dense(hidden_dim // 2, activation='linear', name='dense_token')(time_comb)\n",
    "    dense_token = Dropout(rate=0.4)(dense_token)\n",
    "    token_prob = Dense(event_vocab_dim, activation='softmax', name='token_prob')(dense_token)\n",
    "    generator = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=[token_prob, time_out])\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = build_G(\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "#     emb_dim = EMB_DIM,\n",
    "#     hidden_dim= HIDDEN_DIM)\n",
    "\n",
    "# discriminator = build_D(\n",
    "#     T = T,\n",
    "#     event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "#     emb_dim = EMB_DIM,\n",
    "#     hidden_dim= HIDDEN_DIM,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_G_et = all_event_type_seqs\n",
    "pretrain_G_ts = all_timestamp_seqs\n",
    "pretrain_G_labels = np.ones((all_event_type_seqs.shape[0], 1))\n",
    "\n",
    "pretrain_G_features = (pretrain_G_et, pretrain_G_ts)\n",
    "N_pretrain_G = pretrain_G_et.shape[0]\n",
    "N_pretrain_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "_TOTAL_STEPS = int(EPOCHS * N_pretrain_G / BATCH_SIZE)\n",
    "\n",
    "\n",
    "pretrain_G_dataset = create_dataset(pretrain_G_features,\n",
    "                                  pretrain_G_labels,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_pretrain_G)\n",
    "\n",
    "pretrain_gen_ce_loss_history = []\n",
    "pretrain_gen_gaussian_loss_history = []\n",
    "\n",
    "pretrained_generator = build_G_2(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "OPTIMIZER = Adam(lr=1e-4)\n",
    "WEIGHT_GAUSSIAN_LOSS = 1\n",
    "\n",
    "for feature_sample, _ in tqdm(pretrain_G_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    gen_ce_loss, gen_gaussian_loss =  pretrain_generator(feature_sample, \n",
    "                                                         pretrained_generator,\n",
    "                                                         verbose=True, \n",
    "                                                         weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS, \n",
    "                                                         optimizer=OPTIMIZER)\n",
    "        \n",
    "    pretrain_gen_ce_loss_history.append(gen_ce_loss.numpy())\n",
    "    pretrain_gen_gaussian_loss_history.append(gen_gaussian_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(pretrain_gen_ce_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_gen_ce_loss_history)\n",
    "plt.title('Pre-training Generator Categorical Cross-Entropy Loss History')\n",
    "plt.xlabel('Pre-training steps')\n",
    "\n",
    "x = range(len(pretrain_gen_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_gen_gaussian_loss_history)\n",
    "plt.title('Pre-training Generator Gaussian Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'pretrain_gen_ce_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_ce_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'pretrain_gen_gaussian_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_gaussian_loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pretrained G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/pretrained_gen_weights'):\n",
    "    os.makedirs('./experiment_results/long_seqs_v7/init_pretrained/pretrained_gen_weights')\n",
    "    \n",
    "G_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/pretrained_gen_weights/model_dropout.tf'\n",
    "pretrained_generator.save_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/pretrained_gen_weights/model_dropout.tf'\n",
    "\n",
    "reload_pretrained_gen = build_G_2(\n",
    "    batch_size = BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_gen.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_gen.load_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_D_et = np.concatenate([pos_event_type_seqs, neg_event_type_seqs], axis=0)\n",
    "pretrain_D_ts = np.concatenate([pos_timestamp_seqs, neg_timestamp_seqs], axis=0)\n",
    "\n",
    "pretrain_D_labels = np.concatenate([np.ones((pos_event_type_seqs.shape[0], 1)), \n",
    "                                  np.zeros((neg_event_type_seqs.shape[0], 1))\n",
    "                                 ], axis=0)\n",
    "pretrain_D_features = (pretrain_D_et, pretrain_D_ts)\n",
    "N_pretrain_D = pretrain_D_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_GAUSSIAN_LOSS = 1e-2\n",
    "OPTIMIZER = Adam(lr=1e-3)\n",
    "\n",
    "EPOCHS = 1\n",
    "_TOTAL_STEPS = int(EPOCHS * N_pretrain_D / BATCH_SIZE)\n",
    "# _TOTAL_STEPS = 1000\n",
    "\n",
    "pretrain_disc_token_loss_history = []\n",
    "pretrain_disc_gaussian_loss_history = []\n",
    "\n",
    "\n",
    "pretrain_D_dataset = create_dataset(pretrain_D_features,\n",
    "                                  pretrain_D_labels,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_pretrain_D)\n",
    "\n",
    "pretrained_discriminator = build_D_2(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "for features_batch, real_labels in tqdm(pretrain_D_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    disc_token_loss = pretrain_discriminator(features_batch, real_labels, pretrained_discriminator, verbose=True, \n",
    "                                                                 weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS, optimizer=OPTIMIZER)\n",
    "    pretrain_disc_token_loss_history.append(disc_token_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain D: Loss over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(pretrain_disc_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_disc_token_loss_history)\n",
    "plt.title('Pre-training Discriminator CE Loss History')\n",
    "plt.xlabel('Pre-training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'pretrain_disc_token_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_disc_token_loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pretrained D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/pretrained_disc_weights'):\n",
    "    os.makedirs('/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/pretrained_disc_weights')\n",
    "    \n",
    "D_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v7/init_pretrained/pretrained_disc_weights/model.tf'\n",
    "pretrained_discriminator.save_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_disc.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_disc.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and predict seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sequence_by_rollout(\n",
    "        G, batch_size, T, end_token=0, init_token=1.0, max_time=1024, verbose=False):\n",
    "    # Begin from dummy init state (init_token=1, init_timestamp=0.0)\n",
    "    curr_state_et = tf.ones([batch_size, 1, 1], dtype=tf.float64)\n",
    "    curr_state_ts = tf.zeros([batch_size, 1, 1], dtype=tf.float64)\n",
    "\n",
    "    all_state_et = curr_state_et\n",
    "    all_state_ts = curr_state_ts\n",
    "\n",
    "    episode_token_probs = tf.constant(1., dtype=tf.float64, shape=(batch_size, 1))\n",
    "    gaussian_log = tf.constant(0., dtype=tf.float64, shape=(batch_size, 1))\n",
    "\n",
    "    G.reset_states()\n",
    "\n",
    "    for step in range(1, T):  # sequence length\n",
    "        token_prob, time_out = G([curr_state_et, curr_state_ts])\n",
    "\n",
    "        sampled_et = tf.random.categorical(tf.math.log(token_prob), num_samples=1, dtype=tf.int32)\n",
    "        sampled_et = tf.reshape(sampled_et, [batch_size, 1, 1])\n",
    "\n",
    "        # get the chosen token probability per batch for each step\n",
    "        batch_sample_et = tf.reshape(sampled_et, (batch_size, 1))\n",
    "        batch_ind = tf.reshape(tf.range(0, batch_size), (batch_size, 1))\n",
    "        batch_sample_et_2d = tf.concat([batch_ind, batch_sample_et], axis=1)\n",
    "\n",
    "        sampled_token_prob = tf.reshape(tf.gather_nd(token_prob, batch_sample_et_2d), (batch_size, 1))\n",
    "        episode_token_probs = tf.concat([episode_token_probs, sampled_token_prob], axis=1)\n",
    "\n",
    "        # cast sampled_et into float\n",
    "        sampled_et = tf.cast(sampled_et, dtype=tf.float64)\n",
    "\n",
    "        # stop genererating once hit end_token\n",
    "        cond_end_token = tf.equal(curr_state_et, end_token)\n",
    "        curr_state_et = tf.where(cond_end_token, curr_state_et, sampled_et)\n",
    "        all_state_et = tf.concat([all_state_et, curr_state_et], axis=1)\n",
    "\n",
    "        # generate one timstamp using time_out\n",
    "        sampled_ts_raw = time_out.sample()\n",
    "        sampled_ts = tf.clip_by_value(tf.reshape(sampled_ts_raw, (batch_size, 1, 1))\n",
    "                                      , clip_value_min=1, clip_value_max=max_time)\n",
    "\n",
    "        # get the gaussian log likelihood for the sampled timestamps\n",
    "        sampled_gaussian_log = time_out.log_prob(sampled_ts_raw)\n",
    "        gaussian_log = tf.concat([gaussian_log, sampled_gaussian_log], axis=1)\n",
    "\n",
    "        # stop generating once hit end_token\n",
    "        curr_state_ts = tf.where(cond_end_token, curr_state_et, sampled_ts)\n",
    "        all_state_ts = tf.concat([all_state_ts, curr_state_ts], axis=1)\n",
    "\n",
    "    return all_state_et, all_state_ts, episode_token_probs, gaussian_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(N_gen, generator, batch_size, T, recover_to_timestamp=True):\n",
    "    \"\"\"\n",
    "        Generate sequences batch per batch\n",
    "    :param N_gen: total number of seqs to be generated\n",
    "    :param generator:\n",
    "    :param batch_size:\n",
    "    :param T:\n",
    "    :param recover_to_timestamp: whether to recover time deltas to absolute timestamps\n",
    "    :return: a python list of shape [N_gen, T, 2]\n",
    "    \"\"\"\n",
    "    all_type_seq = None\n",
    "    all_time_seq = None\n",
    "    N = 0\n",
    "\n",
    "    while N < N_gen:\n",
    "        batch_state_et, batch_state_ts, _, _ = generate_batch_sequence_by_rollout(generator, batch_size, T,\n",
    "                                                                                  end_token=0, init_token=1.0,\n",
    "                                                                                  max_time=1024, verbose=False)\n",
    "\n",
    "        batch_type_seq = batch_state_et.numpy()\n",
    "        batch_time_seq = batch_state_ts.numpy()\n",
    "\n",
    "        # recover time delta to time stamps\n",
    "        if recover_to_timestamp:\n",
    "            batch_time_seq = np.cumsum(batch_time_seq, axis=1)\n",
    "\n",
    "        if all_type_seq is None:\n",
    "            all_type_seq = batch_type_seq\n",
    "        else:\n",
    "            all_type_seq = np.concatenate([all_type_seq, batch_type_seq], axis=0)\n",
    "\n",
    "        if all_time_seq is None:\n",
    "            all_time_seq = batch_time_seq\n",
    "        else:\n",
    "            all_time_seq = np.concatenate([all_time_seq, batch_time_seq], axis=0)\n",
    "\n",
    "        N += batch_size\n",
    "\n",
    "    # concat type and time in depth\n",
    "    concated_seq_list = np.concatenate([all_type_seq, all_time_seq], axis=2).tolist()\n",
    "\n",
    "    return concated_seq_list[:N_gen]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_gen = 100\n",
    "generator = pretrained_generator\n",
    "\n",
    "generated_seqs = generate_sequences(N_gen, generator, batch_size=BATCH_SIZE, T=T, recover_to_timestamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs = np.array(generated_seqs)\n",
    "generated_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs = np.array(generated_seqs)\n",
    "pred_generated = reload_pretrained_disc((generated_seqs[:,:,[0]], generated_seqs[:,:,[1]]))\n",
    "(pred_generated > 0.9).numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_event_type_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_event_type_seqs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_event_type_seqs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_event_type_seqs.mean(axis=0) - neg_event_type_seqs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
