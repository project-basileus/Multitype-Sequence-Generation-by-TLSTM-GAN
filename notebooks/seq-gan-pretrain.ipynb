{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = '/home/lun/project-basileus/seq-gan/'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from tensorflow_probability import distributions as tfd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sgtlstm' in sys.modules:\n",
    "    importlib.reload(sys.modules['sgtlstm'])\n",
    "    \n",
    "from sgtlstm.utils import create_dataset\n",
    "# from sgtlstm.SeqGan import build_G, build_D\n",
    "from sgtlstm.pretrain import pretrain_generator, pretrain_discriminator, create_self_regression_data_batch\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v6/positive_long_sequences.pickle'\n",
    "neg_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v6/negative_long_sequences.pickle'\n",
    "all_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v6/all_long_sequences.pickle'\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, timestamp)\n",
    "        [[(1, 11), (1, 24), (2, 37), (3, 47), (2, 63), (2, 80), (1, 88), (2, 95), (2, 104), (3, 111)], ...]\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, timestamp_seqs)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "#     print(seqs.shape)\n",
    "    \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    return et_seqs, ts_seqs\n",
    "    \n",
    "pos_event_type_seqs, pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path)\n",
    "neg_event_type_seqs, neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path)\n",
    "all_event_type_seqs, all_timestamp_seqs = load_sequence_from_pickle_to_numpy(all_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "T = 20 + 1\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 5\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "def build_D_2(T, event_vocab_dim, emb_dim, hidden_dim=11):\n",
    "    \"\"\"\n",
    "        Build a discriminator for event type sequence of shape (batch_size, T, input_dim)\n",
    "        and input event type sequence of shape (batch_size, T, 1)\n",
    "    :param T: length of the sequence\n",
    "    :param event_vocab_dim: size of event vocabulary ['na', 'init', 'start', 'view', 'click', 'install']\n",
    "    :param emb_dim: dimension of the embedding layer output for event type\n",
    "    :param hidden_dim: dimension hidden of the time lstm cell\n",
    "    :return: discriminator D\n",
    "    \"\"\"\n",
    "    # Time-LSTM:\n",
    "    i_et = Input(shape=(T, 1), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(shape=(T, 1), name='time_delta')  # input of continuous feature timestamp\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(T, 1))\n",
    "    masked_ts = mask_layer(i_ts)\n",
    "    masked_et = mask_layer(i_et)\n",
    "\n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, input_length=T, mask_zero=True)(masked_et)\n",
    "    embed0 = Reshape((T, emb_dim))(embed0)  # shape=[Batch_size, T, emb_dim]\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, masked_ts], axis=2)  # # shape=[Batch_size, T, emb_dim + time_dim]\n",
    "\n",
    "    hm, tm = TimeLSTM1(hidden_dim, activation='selu', name='time_lstm', return_sequences=False)(merged0)\n",
    "\n",
    "    time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "\n",
    "    # predicted real prob\n",
    "    real_prob = Dense(1, activation='sigmoid', name='fraud_prob', kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3))(\n",
    "        time_comb)\n",
    "\n",
    "    discriminator = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=[real_prob])\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def build_G(batch_size, event_vocab_dim, emb_dim, hidden_dim=11):\n",
    "    \"\"\"\n",
    "        Build a generator for event type sequence of shape (batch_size, T, input_dim)\n",
    "        and input event type sequence of shape (batch_size, T, 1)\n",
    "    :param batch_size: batch size must been specified at generator\n",
    "    :param event_vocab_dim: size of event vocabulary ['na', 'start', 'click', 'install']\n",
    "    :param emb_dim: dimension of the embedding layer output for event type\n",
    "    :param hidden_dim: dimension hidden of the time lstm cell\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Time-LSTM:\n",
    "    i_et = Input(batch_shape=(batch_size, None, 1), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(batch_shape=(batch_size, None, 1), name='time_delta')  # input of continuous feature timestamp\n",
    "\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(None, 1))\n",
    "    masked_et = mask_layer(i_et)\n",
    "    masked_ts = mask_layer(i_ts)\n",
    "\n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, mask_zero=True)(masked_et)\n",
    "    embed0 = Reshape([1, emb_dim])(embed0)\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, masked_ts], axis=2)\n",
    "\n",
    "    hm, tm = TimeLSTM1(hidden_dim, activation='selu', name='time_lstm',\n",
    "                       stateful=True, return_sequences=False)(merged0)\n",
    "    # time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "    time_out = Dense(1 + 1, activation='linear', name='output')(tm)\n",
    "    time_out = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[..., :1],\n",
    "                             scale=1 + tf.math.softplus(t[..., 1:])),\n",
    "        name='Normal')(time_out)\n",
    "\n",
    "    # predicted prob of next token\n",
    "    token_prob = Dense(event_vocab_dim, activation='softmax', name='token_prob')(hm)\n",
    "    generator = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=[token_prob, time_out])\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = build_G(\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "#     emb_dim = EMB_DIM,\n",
    "#     hidden_dim= HIDDEN_DIM)\n",
    "\n",
    "# discriminator = build_D(\n",
    "#     T = T,\n",
    "#     event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "#     emb_dim = EMB_DIM,\n",
    "#     hidden_dim= HIDDEN_DIM,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_G_et = all_event_type_seqs\n",
    "pretrain_G_ts = all_timestamp_seqs\n",
    "pretrain_G_labels = np.ones((all_event_type_seqs.shape[0], 1))\n",
    "\n",
    "pretrain_G_features = (pretrain_G_et, pretrain_G_ts)\n",
    "N_pretrain_G = pretrain_G_et.shape[0]\n",
    "N_pretrain_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_GAUSSIAN_LOSS = 1\n",
    "OPTIMIZER = Adam(lr=1e-3)\n",
    "\n",
    "EPOCHS = 2\n",
    "_TOTAL_STEPS = int(EPOCHS * N_pretrain_G / BATCH_SIZE)\n",
    "\n",
    "\n",
    "pretrain_G_dataset = create_dataset(pretrain_G_features,\n",
    "                                  pretrain_G_labels,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_pretrain_G)\n",
    "\n",
    "pretrain_gen_ce_loss_history = []\n",
    "pretrain_gen_gaussian_loss_history = []\n",
    "\n",
    "pretrained_generator = build_G(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "for feature_sample, _ in tqdm(pretrain_G_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    gen_ce_loss, gen_gaussian_loss =  pretrain_generator(feature_sample, \n",
    "                                                         pretrained_generator,\n",
    "                                                         verbose=True, \n",
    "                                                         weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS, \n",
    "                                                         optimizer=OPTIMIZER)\n",
    "        \n",
    "    pretrain_gen_ce_loss_history.append(gen_ce_loss.numpy())\n",
    "    pretrain_gen_gaussian_loss_history.append(gen_gaussian_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(pretrain_gen_ce_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_gen_ce_loss_history)\n",
    "plt.title('Pre-training Generator Categorical Cross-Entropy Loss History')\n",
    "plt.xlabel('Pre-training steps')\n",
    "\n",
    "x = range(len(pretrain_gen_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_gen_gaussian_loss_history)\n",
    "plt.title('Pre-training Generator Gaussian Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'pretrain_gen_ce_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_ce_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'pretrain_gen_gaussian_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_gen_gaussian_loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pretrained G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_gen_weights'):\n",
    "    os.makedirs('./experiment_results/long_seqs_v6/init_pretrained/pretrained_gen_weights')\n",
    "    \n",
    "G_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_gen_weights/model.tf'\n",
    "pretrained_generator.save_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_gen_weights/model.tf'\n",
    "\n",
    "reload_pretrained_gen = build_G(\n",
    "    batch_size = BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_gen.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_gen.load_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_D_et = np.concatenate([pos_event_type_seqs, neg_event_type_seqs], axis=0)\n",
    "pretrain_D_ts = np.concatenate([pos_timestamp_seqs, neg_timestamp_seqs], axis=0)\n",
    "\n",
    "pretrain_D_labels = np.concatenate([np.ones((pos_event_type_seqs.shape[0], 1)), \n",
    "                                  np.zeros((neg_event_type_seqs.shape[0], 1))\n",
    "                                 ], axis=0)\n",
    "pretrain_D_features = (pretrain_D_et, pretrain_D_ts)\n",
    "N_pretrain_D = pretrain_D_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_GAUSSIAN_LOSS = 1e-2\n",
    "OPTIMIZER = Adam(lr=1e-3)\n",
    "\n",
    "EPOCHS = 10\n",
    "_TOTAL_STEPS = int(EPOCHS * N_pretrain_D / BATCH_SIZE)\n",
    "# _TOTAL_STEPS = 1000\n",
    "\n",
    "pretrain_disc_token_loss_history = []\n",
    "pretrain_disc_gaussian_loss_history = []\n",
    "\n",
    "\n",
    "pretrain_D_dataset = create_dataset(pretrain_D_features,\n",
    "                                  pretrain_D_labels,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_pretrain_D)\n",
    "\n",
    "pretrained_discriminator = build_D_2(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "for features_batch, real_labels in tqdm(pretrain_D_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    disc_token_loss = pretrain_discriminator(features_batch, real_labels, pretrained_discriminator, verbose=True, \n",
    "                                                                 weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS, optimizer=OPTIMIZER)\n",
    "    pretrain_disc_token_loss_history.append(disc_token_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain D: Loss over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(pretrain_disc_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, pretrain_disc_token_loss_history)\n",
    "plt.title('Pre-training Discriminator CE Loss History')\n",
    "plt.xlabel('Pre-training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'pretrain_disc_token_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(pretrain_disc_token_loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pretrained D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_disc_weights'):\n",
    "    os.makedirs('/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_disc_weights')\n",
    "    \n",
    "D_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_disc_weights/model_2.tf'\n",
    "pretrained_discriminator.save_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_disc.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_disc.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
