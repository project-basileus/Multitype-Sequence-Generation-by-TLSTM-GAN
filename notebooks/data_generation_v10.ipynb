{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for synthesized Temporal Sequence Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of version 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence length T = 20;\n",
    "\n",
    "Token Types (Token-Encoding-meaning): \n",
    "\n",
    "P-0-padding, reserved token\n",
    "\n",
    "N-1-initial token\n",
    "\n",
    "A-2-start\n",
    "\n",
    "B-3-view\n",
    "\n",
    "C-4-click\n",
    "\n",
    "D-5-install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be positive, a temporal sequence MUST NOT violate any of the following rules / hidden patterns:\n",
    "\n",
    "0. __[Increasing time]__: Timestamp must be strictly increasing. A later event must have a greater timestamp then any previous event. Since delta_t is used for data generation, timestamp will always increasing, so __this rule is NOT used for oracle__\n",
    "\n",
    "1. __[Starting with A]__: A sequence must start with an A event.\n",
    "\n",
    "2. __[At least 3 types]__: There must be at least 3 distinct types of token after the init token, and at least 1 in the 3 types should be A.\n",
    "\n",
    "3. __[Pairing C & D]__: Each C event can either appear alone, or be paired with one and only one later D event. Each D event has to be paired with one and only one previous C event. Pairing can be non-unique. \n",
    "\n",
    "4. __[Number Decay]__: The total number of A's must be greater than B; The total number of B's must be >= the nums of C; The total number of C's must be >= the nums of D.\n",
    "\n",
    "5. __[Minimum Same Delay]__: The minimum time delay between two consecutive __same__ tokens is 10 secs\n",
    "\n",
    "6. __[Maximum Pair Delay]__: The time delay between a paired C and D cannot be > 50 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp distributions conditioned on the upcoming event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ts distribution is conditioned on the previous event\n",
    "# e.g. if the upcoming event is an A, it follows chi-square 8 distribution\n",
    "event_to_ts_dist = dict({\n",
    "    'A' : lambda: np.random.chisquare(df=8),\n",
    "    'B' : lambda: np.random.chisquare(df=16),\n",
    "    'C' : lambda: np.random.chisquare(df=24),\n",
    "    'D' : lambda: np.random.chisquare(df=32),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Context and Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENT_TYPES = {0:'':'A', 3:'B', 4:'C'} # 0 is reserved for padding 1 is for 'init token'\n",
    "EVENT_TYPES = ['P', 'N', 'A', 'B', 'C', 'D']\n",
    "EVENT_ENCODE = {'P':0, 'N':1, 'A':2, 'B':3, 'C':4, 'D':5}\n",
    "INIT_TOKEN = EVENT_ENCODE['N']\n",
    "END_TOKEN = EVENT_ENCODE['P']\n",
    "\n",
    "MIN_SAME_DELAY = 10\n",
    "MAX_PAIR_DELAY = 50\n",
    "\n",
    "def check_increasing_rule(seq):\n",
    "    for i in range(1, len(seq)):\n",
    "        if seq[i][1] <= seq[i-1][1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_rule_1(seq, use_init_token=True):\n",
    "    if use_init_token:\n",
    "        seq = seq[1:]\n",
    "    return seq[0][0] == EVENT_ENCODE['A']\n",
    "        \n",
    "    \n",
    "def check_rule_2(seq, use_init_token=True):\n",
    "    if use_init_token:\n",
    "        seq = seq[1:]\n",
    "    cnt = Counter()\n",
    "    for et, dt in seq:\n",
    "        cnt[et] += 1\n",
    "    # rule 2\n",
    "    if len(cnt.keys()) > 3 and EVENT_ENCODE['A'] in cnt.keys():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_rule_3(seq, use_init_token=True):\n",
    "    if use_init_token:\n",
    "        seq = seq[1:]    \n",
    "    # one-pass: add D to queue to be attributed to the first available C in a reversed linear scanning\n",
    "    queue = []\n",
    "    for i in range(len(seq)-1, -1, -1):\n",
    "        if seq[i][0] == EVENT_ENCODE['D']: # encounter a D event\n",
    "            queue.append(i)\n",
    "        elif seq[i][0] == EVENT_ENCODE['C'] and queue: # encounter a C event\n",
    "            queue.pop(0)\n",
    "    return len(queue) == 0\n",
    "\n",
    "\n",
    "def check_rule_4(seq, use_init_token=True):\n",
    "    if use_init_token:\n",
    "        seq = seq[1:]\n",
    "    cnt = Counter()\n",
    "    for et, dt in seq:\n",
    "        cnt[et] += 1\n",
    "    # rule 4\n",
    "    if cnt[EVENT_ENCODE['A']] < EVENT_ENCODE['B']:\n",
    "        return False\n",
    "    if cnt[EVENT_ENCODE['B']] < EVENT_ENCODE['C']:\n",
    "        return False\n",
    "    if cnt[EVENT_ENCODE['C']] < EVENT_ENCODE['D']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_rule_5(seq, use_init_token=True):\n",
    "    if use_init_token:\n",
    "        seq = seq[1:]\n",
    "    prev_et, _ = EVENT_ENCODE['N'], 0.0\n",
    "    for et, dt in seq:\n",
    "        if et == prev_et and dt < MIN_SAME_DELAY:\n",
    "            return False\n",
    "        prev_et = et\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_rule_6(seq, use_init_token=True):    \n",
    "    if use_init_token:\n",
    "        seq = seq[1:]    \n",
    "        \n",
    "    def recover_timedelta_to_timestamp(time_seq):\n",
    "        csum = []\n",
    "        curr = 0\n",
    "        for dt in time_seq:\n",
    "            if dt != 0:\n",
    "                curr += dt\n",
    "                csum.append(curr)\n",
    "            else:\n",
    "                csum.append(0)\n",
    "        return csum\n",
    "    \n",
    "    ets = [e[0] for e in seq]\n",
    "    tss = recover_timedelta_to_timestamp([e[1] for e in seq])\n",
    "        \n",
    "    # one-pass: add D to queue to be attributed to the first available C in a reversed linear scanning\n",
    "    queue = []\n",
    "    for i in range(len(seq)-1, -1, -1):\n",
    "        if ets[i] == EVENT_ENCODE['D']: # encounter a D event\n",
    "            queue.append(i)\n",
    "        elif ets[i] == EVENT_ENCODE['C'] and queue: # encounter a C event\n",
    "            if tss[queue[0]] - tss[i] <= MAX_PAIR_DELAY:\n",
    "                queue.pop(0)\n",
    "            else:\n",
    "                return False\n",
    "    # for rule 6, it's fine if there are unpaired D in queue\n",
    "    # b/c this rules is to ensure for each paired (C, D), the delay is bounded\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_rule_dist(seqs, use_init_token=True):\n",
    "    seq_to_rules = [0] * 7\n",
    "    N = len(seqs)\n",
    "    \n",
    "    for i in tqdm(range(N)):\n",
    "        seq = seqs[i]\n",
    "        # check rules one by one:\n",
    "        if check_rule_1(seq):\n",
    "            seq_to_rules[1] += 1\n",
    "        if check_rule_2(seq):\n",
    "            seq_to_rules[2] += 1\n",
    "        if check_rule_3(seq):\n",
    "            seq_to_rules[3] += 1\n",
    "        if check_rule_4(seq):\n",
    "            seq_to_rules[4] += 1\n",
    "        if check_rule_5(seq):\n",
    "            seq_to_rules[5] += 1\n",
    "        if check_rule_6(seq):\n",
    "            seq_to_rules[6] += 1\n",
    "            \n",
    "    print(seq_to_rules)\n",
    "    print(N)\n",
    "    return [freq / N for freq in seq_to_rules[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Uniform-length Dataset: generate valid and invalid sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:54<00:00, 8719.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# length of a temporal sequence\n",
    "L = 20\n",
    "\n",
    "# size of the dataset\n",
    "N = 1000000\n",
    "\n",
    "all_seqs = []\n",
    "seq_to_rules = defaultdict(list)\n",
    "# neg_seqs = []\n",
    "\n",
    "use_init_token = True\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "#     seq_len = np.random.binomial(n=L, p=0.6)\n",
    "    seq_len = L\n",
    "    \n",
    "    # Generate the time sequences only\n",
    "    type_seq = [INIT_TOKEN] + np.random.randint(low=EVENT_ENCODE['A'], high=EVENT_ENCODE['D']+1, size=seq_len).tolist()\n",
    "    \n",
    "    # Generate a seq of timestamps. Time delta conditions on the upcoming token\n",
    "    dts = []\n",
    "    for et in type_seq[1:]:\n",
    "        token = EVENT_TYPES[et]\n",
    "        dt_dist = event_to_ts_dist[token]\n",
    "        dt_sample = float(np.ceil(dt_dist()))\n",
    "        dts.append(dt_sample) \n",
    "    time_seq = [0.0] + dts\n",
    "        \n",
    "    seq = list(zip(type_seq, time_seq))\n",
    "    \n",
    "    # check rules one by one:\n",
    "    if check_rule_1(seq):\n",
    "        seq_to_rules[i].append(1)\n",
    "    if check_rule_2(seq):\n",
    "        seq_to_rules[i].append(2)\n",
    "    if check_rule_3(seq):\n",
    "        seq_to_rules[i].append(3)\n",
    "    if check_rule_4(seq):\n",
    "        seq_to_rules[i].append(4)\n",
    "    if check_rule_5(seq):\n",
    "        seq_to_rules[i].append(5)\n",
    "    if check_rule_6(seq):\n",
    "        seq_to_rules[i].append(6)            \n",
    "        \n",
    "    all_seqs.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.0),\n",
       " (5, 28.0),\n",
       " (2, 6.0),\n",
       " (2, 3.0),\n",
       " (4, 18.0),\n",
       " (4, 32.0),\n",
       " (2, 15.0),\n",
       " (4, 26.0),\n",
       " (4, 27.0),\n",
       " (2, 11.0),\n",
       " (2, 3.0),\n",
       " (2, 7.0),\n",
       " (2, 10.0),\n",
       " (5, 26.0),\n",
       " (3, 8.0),\n",
       " (4, 24.0),\n",
       " (5, 22.0),\n",
       " (2, 6.0),\n",
       " (5, 39.0),\n",
       " (5, 19.0),\n",
       " (4, 21.0)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seqs[724]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide pos and neg sequences by any 3 rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seqs = []\n",
    "neg_seqs = []\n",
    "\n",
    "for i in range(N):    \n",
    "    if len(seq_to_rules[i]) > 3:\n",
    "        pos_seqs.append(all_seqs[i])\n",
    "    else:\n",
    "        neg_seqs.append(all_seqs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154189\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845811\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check rule distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154189/154189 [00:04<00:00, 31323.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 79998, 153010, 110143, 136325, 114210, 57761]\n",
      "154189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_rule_dist = get_rule_dist(pos_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 845811/845811 [00:25<00:00, 32640.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 169752, 834373, 134221, 218426, 323497, 72187]\n",
      "845811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "neg_rule_dist = get_rule_dist(neg_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5188307855943031,\n",
       " 0.9923535401358073,\n",
       " 0.7143375986613831,\n",
       " 0.8841421891315204,\n",
       " 0.7407143181420205,\n",
       " 0.37461167787585364]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rule_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20069731890457798,\n",
       " 0.9864768843157632,\n",
       " 0.15868911612641595,\n",
       " 0.2582444541392817,\n",
       " 0.38246960609403285,\n",
       " 0.08534648993687716]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rule_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paddings(seq, T=21, inplace=False):\n",
    "    if inplace:\n",
    "        while len(seq) < T:\n",
    "            seq.append((0, 0.0))\n",
    "        return\n",
    "    else:\n",
    "        seq_copy = list(seq)\n",
    "        while len(seq_copy) < T:\n",
    "            seq_copy.append((0, 0.0))\n",
    "        return seq_copy\n",
    "    \n",
    "def trim_paddings(seq, T=21, inplace=False):\n",
    "    if inplace:\n",
    "        while seq and seq[-1] == (0, 0.0):\n",
    "            seq.pop()\n",
    "        return\n",
    "    else:\n",
    "        seq_copy = list(seq)\n",
    "        while seq_copy and seq_copy[-1] == (0, 0.0):\n",
    "            seq_copy.pop()\n",
    "        return seq_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_pos_seqs = [add_paddings(seq) for seq in pos_seqs]\n",
    "padded_neg_seqs = [add_paddings(seq) for seq in neg_seqs]\n",
    "padded_all_seqs = padded_pos_seqs + padded_neg_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_pos_seqs = [trim_paddings(seq) for seq in padded_pos_seqs]\n",
    "trimmed_neg_seqs = [trim_paddings(seq) for seq in padded_neg_seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset : Dump into binay files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downsample the negative seqs\n",
    "# random_idx = np.arange(len(neg_seqs))\n",
    "# np.random.shuffle(random_idx)\n",
    "\n",
    "# # random_idx\n",
    "\n",
    "# neg_seqs_downsample = neg_seqs[:len(pos_seqs)]\n",
    "# len(neg_seqs_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pos_seqs_filename = 'positive_long_sequences.pickle'\n",
    "neg_seqs_filename = 'negative_long_sequences.pickle'\n",
    "all_seqs_filename = 'all_long_sequences.pickle'\n",
    "\n",
    "repo_path = '/home/lun/project-basileus/seq-gan/'\n",
    "\n",
    "with open(os.path.join(repo_path, 'data', 'long_seqs_v10', pos_seqs_filename), 'wb') as f:\n",
    "    pickle.dump(pos_seqs, f)\n",
    "    \n",
    "with open(os.path.join(repo_path, 'data', 'long_seqs_v10', neg_seqs_filename), 'wb') as f:\n",
    "    pickle.dump(neg_seqs, f)\n",
    "    \n",
    "with open(os.path.join(repo_path, 'data', 'long_seqs_v10', all_seqs_filename), 'wb') as f:\n",
    "    pickle.dump(all_seqs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
