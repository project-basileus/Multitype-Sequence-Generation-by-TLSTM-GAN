{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "starting-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "module_path = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/\"\n",
    "data_path = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/long_seqs_v11/\"\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "blessed-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_DATA = 4000\n",
    "T = 20\n",
    "VOCAB = ['start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 16\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-object",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "reflected-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = os.path.join(data_path, 'positive_long_sequences.pickle')\n",
    "neg_data_path = os.path.join(data_path, 'negative_long_sequences.pickle')\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path, use_init_token=False):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, time_delta)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, time_delta)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "        \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    \n",
    "    return et_seqs, ts_seqs\n",
    "\n",
    "raw_pos_event_type_seqs, raw_pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path, use_init_token=False)\n",
    "raw_neg_event_type_seqs, raw_neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path, use_init_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "saving-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast indicator data into one-hot\n",
    "pos_event_type_seqs = tf.cast(raw_pos_event_type_seqs, tf.int32)\n",
    "pos_event_type_seqs = tf.one_hot(pos_event_type_seqs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "pos_event_type_seqs = tf.squeeze(pos_event_type_seqs, axis=3)\n",
    "\n",
    "# cast indicator data into one-hot\n",
    "neg_event_type_seqs = tf.cast(raw_neg_event_type_seqs, tf.int32)\n",
    "neg_event_type_seqs = tf.one_hot(neg_event_type_seqs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "neg_event_type_seqs = tf.squeeze(neg_event_type_seqs, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "stylish-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcore_norm(data):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    _mean = tf.reduce_mean(data, axis=0)\n",
    "    _mean = tf.expand_dims(_mean, axis=0)\n",
    "    _mean = tf.repeat(_mean, N, axis=0)\n",
    "\n",
    "    _std = tf.math.reduce_std(data, axis=0)\n",
    "    _std = tf.expand_dims(_std, axis=0)\n",
    "    _std = tf.repeat(_std, N, axis=0)\n",
    "\n",
    "    return (data - _mean) / _std, _mean[0,:,:], _std[0,:,:]\n",
    "\n",
    "def zcore_norm_2(data, _mean, _std):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    _mean = tf.expand_dims(_mean, axis=0)\n",
    "    _mean = tf.repeat(_mean, N, axis=0)\n",
    "\n",
    "    _std = tf.expand_dims(_std, axis=0)\n",
    "    _std = tf.repeat(_std, N, axis=0)\n",
    "\n",
    "    return (data - _mean) / _std\n",
    "\n",
    "def get_mean_std(data):\n",
    "    _mean = tf.reduce_mean(data, axis=0)\n",
    "    _mean = tf.expand_dims(_mean, axis=0)\n",
    "\n",
    "    _std = tf.math.reduce_std(data, axis=0)\n",
    "    _std = tf.expand_dims(_std, axis=0)\n",
    "\n",
    "    return _mean, _std\n",
    "\n",
    "def apply_mean_std(data, _mean, _std):\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    _mean = tf.reshape(_mean, (1, T, 1))\n",
    "    _mean = tf.repeat(_mean, N, axis=0)\n",
    "    \n",
    "    _std = tf.reshape(_std, (1, T, 1))\n",
    "    _std = tf.repeat(_std, N, axis=0)\n",
    "    \n",
    "    return data * _std + _mean\n",
    "\n",
    "pos_timestamp_seqs, GLOBAL_MEAN_POS, GLOBAL_STD_POS = zcore_norm(raw_pos_timestamp_seqs)\n",
    "neg_timestamp_seqs, GLOBAL_MEAN_NEG, GLOBAL_STD_NEG = zcore_norm(raw_neg_timestamp_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-organic",
   "metadata": {},
   "source": [
    "## Create multitype SeqGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "linear-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense, Dropout, Activation, Multiply, Add, Lambda\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy, Precision, Recall\n",
    "\n",
    "# def F1Score(y_true, y_pred):\n",
    "#     precision = Precision(y_true, y_pred)\n",
    "#     recall = Recall(y_true, y_pred)\n",
    "#     return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def build_classifier(batch_size, T, event_vocab_dim, emb_dim, hidden_dim, dropout_rate=0.25):\n",
    "    # normal LSTM\n",
    "    i_et = Input(batch_shape=(batch_size, None, event_vocab_dim), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(batch_shape=(batch_size, None, 1), name='time_delta_in')  # input of continuous feature timestamp\n",
    "    \n",
    "    embed0 = Dense(emb_dim, name='dense_emb')(i_et) # dense matrix size: 6*16\n",
    "    merged0 = tf.concat([embed0, i_ts], axis=2)\n",
    "    \n",
    "    hm = LSTM(hidden_dim,\n",
    "          name='lstm_token',\n",
    "          stateful=False,\n",
    "          return_sequences=False, \n",
    "          kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          recurrent_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))(merged0)\n",
    "\n",
    "    tm = LSTM(hidden_dim,\n",
    "          name='lstm_time',\n",
    "          stateful=False,\n",
    "          return_sequences=False,\n",
    "          kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          recurrent_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "          bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))(merged0)\n",
    "    \n",
    "    token_time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "    \n",
    "    dropped = Dropout(rate=dropout_rate)(token_time_comb)\n",
    "    \n",
    "    prob = Dense(1, \n",
    "             activation='sigmoid',\n",
    "             name='final',\n",
    "             kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "             bias_initializer=tf.keras.initializers.Constant(value=0.1))(dropped)\n",
    "        \n",
    "    classifier = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=prob)\n",
    "        \n",
    "    metrics = [\n",
    "        BinaryAccuracy(name='accuracy'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        AUC(num_thresholds=500, curve='PR', name='auc_pr'),\n",
    "        AUC(num_thresholds=500, curve='ROC', name='auc_roc')\n",
    "    ]\n",
    "    \n",
    "    classifier.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "        metrics=metrics)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "pediatric-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "norwegian-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "event_type (InputLayer)         [(256, None, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_emb (Dense)               (256, None, 16)      80          event_type[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_delta_in (InputLayer)      [(256, None, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (256, None, 17)      0           dense_emb[0][0]                  \n",
      "                                                                 time_delta_in[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_token (LSTM)               (256, 128)           74752       tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_time (LSTM)                (256, 128)           74752       tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (256, 256)           0           lstm_token[0][0]                 \n",
      "                                                                 lstm_time[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (256, 256)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final (Dense)                   (256, 1)             257         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,841\n",
      "Trainable params: 149,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-saturday",
   "metadata": {},
   "source": [
    "## Train, val, test split : 0.6/0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "still-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pos = pos_event_type_seqs.shape[0]\n",
    "N_neg = neg_event_type_seqs.shape[0]\n",
    "\n",
    "def split(data):\n",
    "    N = data.shape[0]\n",
    "    train_data, val_data, test_data = data[:int(0.6*N),:,:], data[int(0.6*N):int(1*N),:,:], data[int(1*N):,:,:]\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_pos_et, val_pos_et, test_pos_et = split(pos_event_type_seqs)\n",
    "train_pos_ts, val_pos_ts, test_pos_ts = split(pos_timestamp_seqs)\n",
    "\n",
    "train_neg_et, val_neg_et, test_neg_et = split(neg_event_type_seqs)\n",
    "train_neg_ts, val_neg_ts, test_neg_ts = split(neg_timestamp_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "coastal-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(features: np.array, labels: np.array, batch_size=2, epochs=10, buffer_size=10000):\n",
    "    \"\"\"\n",
    "    Create TF dataset from\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def combine_seqs_to_dataset(pos_et, pos_ts, neg_et, neg_ts, batch_size, epochs):\n",
    "    _N_pos = pos_et.shape[0]\n",
    "    _N_neg = neg_et.shape[0]\n",
    "\n",
    "    _ets = tf.concat([pos_et, neg_et], axis=0)\n",
    "    _tss = tf.concat([pos_ts, neg_ts], axis=0)\n",
    "    _labels = tf.concat([np.ones((_N_pos, 1)), np.zeros((_N_neg, 1))], axis=0)\n",
    "\n",
    "    _dataset = create_tf_dataset((_ets, _tss), \n",
    "                                 _labels,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 buffer_size=_N_pos + _N_neg) # shuffle the entire Dataset\n",
    "    \n",
    "    return _dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "quiet-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = combine_seqs_to_dataset(val_pos_et, val_pos_ts, val_neg_et, val_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "willing-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = combine_seqs_to_dataset(test_pos_et, test_pos_ts, test_neg_et, test_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "listed-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import TensorSpec\n",
    "element_spec = ((TensorSpec(shape=(BATCH_SIZE, T, 4), dtype=tf.float64, name=None), \n",
    "                 TensorSpec(shape=(BATCH_SIZE, T, 1), dtype=tf.float64, name=None)),\n",
    "                TensorSpec(shape=(BATCH_SIZE, 1), dtype=tf.float64, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "serious-stylus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(256, 20, 4), dtype=tf.float64, name=None),\n",
       "  TensorSpec(shape=(256, 20, 1), dtype=tf.float64, name=None)),\n",
       " TensorSpec(shape=(256, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "coupled-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-100-gs-val.tf.data')\n",
    "tf.data.experimental.save(val_dataset, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "val_dataset = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "located-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets/1-to-100-gs-val.tf.data\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "composed-publisher",
   "metadata": {},
   "source": [
    "## Train with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "behavioral-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining imbalanced data, no augmentations\n",
    "train_dataset_im = combine_seqs_to_dataset(train_pos_et, train_pos_ts, train_neg_et, train_neg_ts, BATCH_SIZE, 1)\n",
    "# element_spec = train_dataset_im.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "finnish-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-100-gs-im.tf.data')\n",
    "tf.data.experimental.save(train_dataset_im, dataset_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "rapid-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_im = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "derived-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_im = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "difficult-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5\n",
    ")\n",
    "history_im = classifier_im.fit(train_dataset_im, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-technical",
   "metadata": {},
   "source": [
    "## Train with upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "correct-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train_pos = train_pos_et.shape[0]\n",
    "N_train_neg = train_neg_et.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "renewable-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_indices = np.random.choice(N_train_pos, size=N_train_neg, replace=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "regular-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_train_pos_et = tf.gather(train_pos_et, upsampled_indices)\n",
    "upsampled_train_pos_ts = tf.gather(train_pos_ts, upsampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "distributed-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_up = combine_seqs_to_dataset(upsampled_train_pos_et, upsampled_train_pos_ts, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "modern-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets/1-to-100-gs-up.tf.data\n"
     ]
    }
   ],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-100-gs-up.tf.data')\n",
    "tf.data.experimental.save(train_dataset_up, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_up = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "united-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_up = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "alien-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5\n",
    ")\n",
    "history_up = classifier_up.fit(train_dataset_up, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-fields",
   "metadata": {},
   "source": [
    "## Train Using Gumbel-Softmax generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "important-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G2_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G2/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G2_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G2/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "valued-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G2_gs_path, 'rb') as f:\n",
    "    syn_pos_et_G2_gs = pickle.load(f)\n",
    "    syn_pos_et_G2_gs = tf.one_hot(syn_pos_et_G2_gs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G2_gs = tf.squeeze(syn_pos_et_G2_gs, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G2_gs_path, 'rb') as f:\n",
    "    syn_pos_ts_G2_gs = pickle.load(f)\n",
    "    syn_pos_ts_G2_gs = zcore_norm_2(syn_pos_ts_G2_gs, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "confirmed-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_syn_data(syn_pos_et, syn_pos_ts, train_pos_et, train_pos_ts):\n",
    "    N_train_pos = train_pos_et.shape[0]\n",
    "    N_train_neg = train_neg_et.shape[0]\n",
    "    N_syn_sample = N_train_neg - N_train_pos\n",
    "    \n",
    "    syn_sample_indices = np.random.choice(syn_pos_et.shape[0], size=N_syn_sample, replace=False).tolist()    \n",
    "    \n",
    "    sampled_syn_pos_et = tf.gather(syn_pos_et, syn_sample_indices)\n",
    "    sampled_syn_pos_ts = tf.gather(syn_pos_ts, syn_sample_indices)\n",
    "    \n",
    "    aug_pos_et = tf.concat([train_pos_et, sampled_syn_pos_et], axis=0)\n",
    "    aug_pos_ts = tf.concat([train_pos_ts, sampled_syn_pos_ts], axis=0)\n",
    "    \n",
    "    assert(aug_pos_et.shape[0] == aug_pos_ts.shape[0] ==N_train_neg)\n",
    "    \n",
    "    return aug_pos_et, aug_pos_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "equal-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_gs_G2, aug_pos_ts_gs_G2 = sample_from_syn_data(syn_pos_et_G2_gs, syn_pos_ts_G2_gs, train_pos_et, train_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "induced-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_gs_G2 = combine_seqs_to_dataset(aug_pos_et_gs_G2, aug_pos_ts_gs_G2, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "hispanic-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets/1-to-100-gs-G2.tf.data\n"
     ]
    }
   ],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-100-gs-G2.tf.data')\n",
    "tf.data.experimental.save(train_dataset_gs_G2, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_gs_G2 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "confident-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs_G2 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "signed-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 124s 64ms/step - loss: 0.0518 - accuracy: 0.9808 - precision: 0.9818 - recall: 0.9798 - auc_pr: 0.9956 - auc_roc: 0.9951 - val_loss: 0.0229 - val_accuracy: 0.9932 - val_precision: 0.6884 - val_recall: 0.5677 - val_auc_pr: 0.6456 - val_auc_roc: 0.9877\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0102 - accuracy: 0.9966 - precision: 0.9985 - recall: 0.9946 - auc_pr: 0.9998 - auc_roc: 0.9998 - val_loss: 0.0153 - val_accuracy: 0.9951 - val_precision: 0.9229 - val_recall: 0.5551 - val_auc_pr: 0.8294 - val_auc_roc: 0.9958\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0070 - accuracy: 0.9975 - precision: 0.9988 - recall: 0.9961 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0117 - val_accuracy: 0.9961 - val_precision: 0.8795 - val_recall: 0.7034 - val_auc_pr: 0.8579 - val_auc_roc: 0.9863\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0061 - accuracy: 0.9978 - precision: 0.9988 - recall: 0.9968 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0098 - val_accuracy: 0.9967 - val_precision: 0.9058 - val_recall: 0.7469 - val_auc_pr: 0.9021 - val_auc_roc: 0.9966\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0049 - accuracy: 0.9982 - precision: 0.9989 - recall: 0.9975 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0090 - val_accuracy: 0.9968 - val_precision: 0.9384 - val_recall: 0.7274 - val_auc_pr: 0.9150 - val_auc_roc: 0.9947\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0048 - accuracy: 0.9984 - precision: 0.9991 - recall: 0.9977 - auc_pr: 1.0000 - auc_roc: 0.9999 - val_loss: 0.0091 - val_accuracy: 0.9969 - val_precision: 0.9466 - val_recall: 0.7312 - val_auc_pr: 0.9148 - val_auc_roc: 0.9952\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0040 - accuracy: 0.9985 - precision: 0.9991 - recall: 0.9980 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9972 - val_precision: 0.8838 - val_recall: 0.8296 - val_auc_pr: 0.9237 - val_auc_roc: 0.9937\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0040 - accuracy: 0.9986 - precision: 0.9990 - recall: 0.9982 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0076 - val_accuracy: 0.9974 - val_precision: 0.9047 - val_recall: 0.8208 - val_auc_pr: 0.9313 - val_auc_roc: 0.9920\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0035 - accuracy: 0.9987 - precision: 0.9992 - recall: 0.9983 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9973 - val_precision: 0.8589 - val_recall: 0.8750 - val_auc_pr: 0.9278 - val_auc_roc: 0.9938\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0032 - accuracy: 0.9989 - precision: 0.9992 - recall: 0.9985 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0067 - val_accuracy: 0.9978 - val_precision: 0.9559 - val_recall: 0.8145 - val_auc_pr: 0.9496 - val_auc_roc: 0.9966\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0026 - accuracy: 0.9991 - precision: 0.9993 - recall: 0.9988 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0060 - val_accuracy: 0.9981 - val_precision: 0.9054 - val_recall: 0.8986 - val_auc_pr: 0.9573 - val_auc_roc: 0.9986\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0022 - accuracy: 0.9992 - precision: 0.9994 - recall: 0.9990 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0057 - val_accuracy: 0.9982 - val_precision: 0.9137 - val_recall: 0.9034 - val_auc_pr: 0.9591 - val_auc_roc: 0.9975\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0020 - accuracy: 0.9993 - precision: 0.9994 - recall: 0.9991 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0081 - val_accuracy: 0.9973 - val_precision: 0.8222 - val_recall: 0.9250 - val_auc_pr: 0.9491 - val_auc_roc: 0.9925\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.9995 - recall: 0.9991 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9981 - val_precision: 0.8953 - val_recall: 0.9122 - val_auc_pr: 0.9628 - val_auc_roc: 0.9964\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0022 - accuracy: 0.9993 - precision: 0.9995 - recall: 0.9992 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9978 - val_precision: 0.8724 - val_recall: 0.9062 - val_auc_pr: 0.9548 - val_auc_roc: 0.9975\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.9995 - recall: 0.9992 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0069 - val_accuracy: 0.9976 - val_precision: 0.9537 - val_recall: 0.7987 - val_auc_pr: 0.9470 - val_auc_roc: 0.9968\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0015 - accuracy: 0.9994 - precision: 0.9995 - recall: 0.9994 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0073 - val_accuracy: 0.9980 - val_precision: 0.9150 - val_recall: 0.8761 - val_auc_pr: 0.9396 - val_auc_roc: 0.9863\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0013 - accuracy: 0.9995 - precision: 0.9996 - recall: 0.9995 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0073 - val_accuracy: 0.9979 - val_precision: 0.9027 - val_recall: 0.8813 - val_auc_pr: 0.9421 - val_auc_roc: 0.9912\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0011 - accuracy: 0.9996 - precision: 0.9997 - recall: 0.9996 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9975 - val_precision: 0.8358 - val_recall: 0.9311 - val_auc_pr: 0.9425 - val_auc_roc: 0.9932\n"
     ]
    }
   ],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5\n",
    ")\n",
    "history_g2 = classifier_gs_G2.fit(train_dataset_gs_G2, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-powell",
   "metadata": {},
   "source": [
    "## Train Using MLE generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "purple-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G1_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G1/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G1_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G1/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "special-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G1_gs_path, 'rb') as f:\n",
    "    syn_pos_et_G1_gs = pickle.load(f)\n",
    "    syn_pos_et_G1_gs = tf.one_hot(syn_pos_et_G1_gs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G1_gs = tf.squeeze(syn_pos_et_G1_gs, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G1_gs_path, 'rb') as f:\n",
    "    syn_pos_ts_G1_gs = pickle.load(f)\n",
    "    syn_pos_ts_G1_gs = zcore_norm_2(syn_pos_ts_G1_gs, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "determined-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_gs_G1, aug_pos_ts_gs_G1 = sample_from_syn_data(syn_pos_et_G1_gs, syn_pos_ts_G1_gs, train_pos_et, train_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "legendary-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_gs_G1 = combine_seqs_to_dataset(aug_pos_et_gs_G1, aug_pos_ts_gs_G1, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "concrete-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets/1-to-100-gs-G1.tf.data\n"
     ]
    }
   ],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-100-gs-G1.tf.data')\n",
    "tf.data.experimental.save(train_dataset_gs_G1, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_gs_G1 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "norwegian-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs_G1 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "conceptual-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 124s 64ms/step - loss: 0.0503 - accuracy: 0.9822 - precision: 0.9825 - recall: 0.9832 - auc_pr: 0.9958 - auc_roc: 0.9956 - val_loss: 0.0193 - val_accuracy: 0.9935 - val_precision: 0.8031 - val_recall: 0.4543 - val_auc_pr: 0.6760 - val_auc_roc: 0.9849\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0093 - accuracy: 0.9969 - precision: 0.9988 - recall: 0.9951 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0155 - val_accuracy: 0.9945 - val_precision: 0.6912 - val_recall: 0.7985 - val_auc_pr: 0.8316 - val_auc_roc: 0.9914\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0068 - accuracy: 0.9976 - precision: 0.9986 - recall: 0.9966 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0130 - val_accuracy: 0.9952 - val_precision: 0.9770 - val_recall: 0.5300 - val_auc_pr: 0.8752 - val_auc_roc: 0.9938\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0060 - accuracy: 0.9977 - precision: 0.9987 - recall: 0.9968 - auc_pr: 1.0000 - auc_roc: 0.9999 - val_loss: 0.0110 - val_accuracy: 0.9965 - val_precision: 0.9265 - val_recall: 0.6972 - val_auc_pr: 0.8822 - val_auc_roc: 0.9935\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0052 - accuracy: 0.9981 - precision: 0.9989 - recall: 0.9973 - auc_pr: 1.0000 - auc_roc: 0.9999 - val_loss: 0.0092 - val_accuracy: 0.9968 - val_precision: 0.9426 - val_recall: 0.7206 - val_auc_pr: 0.9124 - val_auc_roc: 0.9933\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0048 - accuracy: 0.9983 - precision: 0.9990 - recall: 0.9977 - auc_pr: 1.0000 - auc_roc: 0.9999 - val_loss: 0.0103 - val_accuracy: 0.9964 - val_precision: 0.7907 - val_recall: 0.8698 - val_auc_pr: 0.9177 - val_auc_roc: 0.9945\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0044 - accuracy: 0.9985 - precision: 0.9990 - recall: 0.9981 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0082 - val_accuracy: 0.9974 - val_precision: 0.8923 - val_recall: 0.8398 - val_auc_pr: 0.9372 - val_auc_roc: 0.9975\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0037 - accuracy: 0.9987 - precision: 0.9992 - recall: 0.9983 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0079 - val_accuracy: 0.9973 - val_precision: 0.8486 - val_recall: 0.8858 - val_auc_pr: 0.9388 - val_auc_roc: 0.9976\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0035 - accuracy: 0.9988 - precision: 0.9993 - recall: 0.9982 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9977 - val_precision: 0.9014 - val_recall: 0.8596 - val_auc_pr: 0.9436 - val_auc_roc: 0.9954\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0030 - accuracy: 0.9989 - precision: 0.9992 - recall: 0.9986 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9973 - val_precision: 0.9604 - val_recall: 0.7594 - val_auc_pr: 0.9400 - val_auc_roc: 0.9917\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0025 - accuracy: 0.9991 - precision: 0.9993 - recall: 0.9989 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0083 - val_accuracy: 0.9975 - val_precision: 0.9477 - val_recall: 0.7925 - val_auc_pr: 0.9398 - val_auc_roc: 0.9888\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0022 - accuracy: 0.9992 - precision: 0.9994 - recall: 0.9990 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0061 - val_accuracy: 0.9980 - val_precision: 0.9151 - val_recall: 0.8773 - val_auc_pr: 0.9521 - val_auc_roc: 0.9956\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0021 - accuracy: 0.9993 - precision: 0.9994 - recall: 0.9991 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9980 - val_precision: 0.8922 - val_recall: 0.9111 - val_auc_pr: 0.9585 - val_auc_roc: 0.9969\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.9996 - recall: 0.9992 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0084 - val_accuracy: 0.9970 - val_precision: 0.7939 - val_recall: 0.9411 - val_auc_pr: 0.9466 - val_auc_roc: 0.9962\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.9995 - recall: 0.9993 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0072 - val_accuracy: 0.9980 - val_precision: 0.9470 - val_recall: 0.8498 - val_auc_pr: 0.9462 - val_auc_roc: 0.9870\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0014 - accuracy: 0.9995 - precision: 0.9996 - recall: 0.9994 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0071 - val_accuracy: 0.9980 - val_precision: 0.9295 - val_recall: 0.8596 - val_auc_pr: 0.9453 - val_auc_roc: 0.9894\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 120s 63ms/step - loss: 0.0013 - accuracy: 0.9995 - precision: 0.9996 - recall: 0.9994 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0067 - val_accuracy: 0.9977 - val_precision: 0.9446 - val_recall: 0.8120 - val_auc_pr: 0.9491 - val_auc_roc: 0.9937\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.0011 - accuracy: 0.9996 - precision: 0.9997 - recall: 0.9995 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9981 - val_precision: 0.9191 - val_recall: 0.8833 - val_auc_pr: 0.9518 - val_auc_roc: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5\n",
    ")\n",
    "history_g1 = classifier_gs_G1.fit(train_dataset_gs_G1, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-reunion",
   "metadata": {},
   "source": [
    "## Train Using Random G0 generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "christian-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pos_et_G0_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G0/syn_type_sequences.pickle'\n",
    "syn_pos_ts_G0_gs_path = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/syn_data/gumbel_softmax_2021-03-30-09-39-43/G0/syn_time_sequences.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "empty-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syn_pos_et_G0_gs_path, 'rb') as f:\n",
    "    syn_pos_et_G0_gs = pickle.load(f)\n",
    "    syn_pos_et_G0_gs = tf.one_hot(syn_pos_et_G0_gs, depth=EVENT_VOCAB_DIM, axis=2, dtype=tf.float64)\n",
    "    syn_pos_et_G0_gs = tf.squeeze(syn_pos_et_G0_gs, axis=3)\n",
    "    \n",
    "with open(syn_pos_ts_G0_gs_path, 'rb') as f:\n",
    "    syn_pos_ts_G0_gs = pickle.load(f)\n",
    "    syn_pos_ts_G0_gs = zcore_norm_2(syn_pos_ts_G0_gs, GLOBAL_MEAN_POS, GLOBAL_STD_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "amazing-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pos_et_gs_G0, aug_pos_ts_gs_G0 = sample_from_syn_data(syn_pos_et_G0_gs, syn_pos_ts_G0_gs, train_pos_et, train_pos_ts)\n",
    "train_dataset_gs_G0 = combine_seqs_to_dataset(aug_pos_et_gs_G0, aug_pos_ts_gs_G0, train_neg_et, train_neg_ts, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "gentle-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets/1-to-100-gs-G0.tf.data\n"
     ]
    }
   ],
   "source": [
    "path_prefix = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/classifier_tf_datasets'\n",
    "dataset_save_path = os.path.join(path_prefix, '1-to-100-gs-G0.tf.data')\n",
    "tf.data.experimental.save(train_dataset_gs_G0, dataset_save_path)\n",
    "print(dataset_save_path)\n",
    "train_dataset_gs_G0 = tf.data.experimental.load(dataset_save_path, element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "varied-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs_G0 = build_classifier(batch_size=BATCH_SIZE,\n",
    "                              T=T,\n",
    "                              event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                              emb_dim=EMB_DIM,\n",
    "                              hidden_dim=HIDDEN_DIM,    \n",
    "                              dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "legitimate-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 125s 65ms/step - loss: 0.0567 - accuracy: 0.9784 - precision: 0.9811 - recall: 0.9764 - auc_pr: 0.9942 - auc_roc: 0.9936 - val_loss: 0.0287 - val_accuracy: 0.9918 - val_precision: 0.8857 - val_recall: 0.1942 - val_auc_pr: 0.5114 - val_auc_roc: 0.9630\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 121s 64ms/step - loss: 0.0119 - accuracy: 0.9962 - precision: 0.9987 - recall: 0.9936 - auc_pr: 0.9998 - auc_roc: 0.9998 - val_loss: 0.0163 - val_accuracy: 0.9945 - val_precision: 0.7644 - val_recall: 0.6450 - val_auc_pr: 0.7689 - val_auc_roc: 0.9842\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 121s 64ms/step - loss: 0.0088 - accuracy: 0.9969 - precision: 0.9985 - recall: 0.9954 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0156 - val_accuracy: 0.9950 - val_precision: 0.8441 - val_recall: 0.6025 - val_auc_pr: 0.7911 - val_auc_roc: 0.9799\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 121s 64ms/step - loss: 0.0069 - accuracy: 0.9977 - precision: 0.9988 - recall: 0.9965 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0220 - val_accuracy: 0.9936 - val_precision: 0.9610 - val_recall: 0.3714 - val_auc_pr: 0.8207 - val_auc_roc: 0.9675\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0057 - accuracy: 0.9980 - precision: 0.9988 - recall: 0.9972 - auc_pr: 1.0000 - auc_roc: 0.9999 - val_loss: 0.0122 - val_accuracy: 0.9960 - val_precision: 0.9170 - val_recall: 0.6504 - val_auc_pr: 0.8489 - val_auc_roc: 0.9923\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0055 - accuracy: 0.9981 - precision: 0.9989 - recall: 0.9974 - auc_pr: 0.9999 - auc_roc: 0.9999 - val_loss: 0.0095 - val_accuracy: 0.9969 - val_precision: 0.9247 - val_recall: 0.7534 - val_auc_pr: 0.9030 - val_auc_roc: 0.9923\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 121s 64ms/step - loss: 0.0044 - accuracy: 0.9985 - precision: 0.9991 - recall: 0.9978 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0102 - val_accuracy: 0.9967 - val_precision: 0.9446 - val_recall: 0.7055 - val_auc_pr: 0.9069 - val_auc_roc: 0.9915\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0042 - accuracy: 0.9986 - precision: 0.9991 - recall: 0.9982 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0097 - val_accuracy: 0.9973 - val_precision: 0.9573 - val_recall: 0.7572 - val_auc_pr: 0.9080 - val_auc_roc: 0.9875\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0034 - accuracy: 0.9989 - precision: 0.9993 - recall: 0.9984 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9964 - val_precision: 0.9618 - val_recall: 0.6621 - val_auc_pr: 0.9058 - val_auc_roc: 0.9826\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0033 - accuracy: 0.9989 - precision: 0.9993 - recall: 0.9985 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0084 - val_accuracy: 0.9973 - val_precision: 0.9587 - val_recall: 0.7575 - val_auc_pr: 0.9334 - val_auc_roc: 0.9903\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0030 - accuracy: 0.9990 - precision: 0.9994 - recall: 0.9985 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0062 - val_accuracy: 0.9979 - val_precision: 0.9213 - val_recall: 0.8659 - val_auc_pr: 0.9533 - val_auc_roc: 0.9984\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0025 - accuracy: 0.9991 - precision: 0.9994 - recall: 0.9989 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9959 - val_precision: 0.7349 - val_recall: 0.9173 - val_auc_pr: 0.9175 - val_auc_roc: 0.9969\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0023 - accuracy: 0.9992 - precision: 0.9994 - recall: 0.9990 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0076 - val_accuracy: 0.9975 - val_precision: 0.8835 - val_recall: 0.8625 - val_auc_pr: 0.9324 - val_auc_roc: 0.9975\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0021 - accuracy: 0.9992 - precision: 0.9994 - recall: 0.9990 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0078 - val_accuracy: 0.9977 - val_precision: 0.9109 - val_recall: 0.8562 - val_auc_pr: 0.9360 - val_auc_roc: 0.9861\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.9995 - recall: 0.9991 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0071 - val_accuracy: 0.9979 - val_precision: 0.9756 - val_recall: 0.8033 - val_auc_pr: 0.9499 - val_auc_roc: 0.9882\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.9995 - recall: 0.9992 - auc_pr: 1.0000 - auc_roc: 1.0000 - val_loss: 0.0076 - val_accuracy: 0.9979 - val_precision: 0.9198 - val_recall: 0.8611 - val_auc_pr: 0.9376 - val_auc_roc: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# _TOTAL_STEPS = int(epochs * (train_pos_ts.shape[0] + train_neg_et.shape[0]) / BATCH_SIZE)\n",
    "epochs = 20\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5\n",
    ")\n",
    "history_g0 = classifier_gs_G0.fit(train_dataset_gs_G0,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-japan",
   "metadata": {},
   "source": [
    "## debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "royal-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = history_up.history\n",
    "best_ind = np.argmin(run['val_loss'])\n",
    "run_best = {k : v_list[best_ind] for k, v_list in run.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "caroline-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bests = [run_best, run_best]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(run_bests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "lightweight-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/experiments/test.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
