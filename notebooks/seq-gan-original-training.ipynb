{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/\"\n",
    "data_path = \"/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/data/long_seqs_v10/\"\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-object",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = os.path.join(data_path, 'positive_long_sequences.pickle')\n",
    "neg_data_path = os.path.join(data_path, 'negative_long_sequences.pickle')\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, time_delta)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, time_delta)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "#     print(seqs.shape)\n",
    "    \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    return et_seqs, ts_seqs\n",
    "    \n",
    "pos_event_type_seqs, pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path)\n",
    "neg_event_type_seqs, neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(features: np.array, labels: np.array, batch_size=2, epochs=10, buffer_size=10000):\n",
    "    \"\"\"\n",
    "    Create dataset from numpy arrays\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "T = 20 + 1\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-organic",
   "metadata": {},
   "source": [
    "## Create original SeqGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense, Dropout, Conv1D, Conv2D, MaxPooling1D, Activation, Multiply, Add, Lambda\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "def build_G(batch_size, event_vocab_dim, emb_dim, hidden_dim=11):\n",
    "    # normal LSTM\n",
    "    i_et = Input(batch_shape=(batch_size, None, 1), name='event_type')  # input of discrete feature event type\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(None, 1))\n",
    "    masked_et = mask_layer(i_et)\n",
    "    \n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, mask_zero=True)(masked_et)\n",
    "    # turn [batch_size, None, 1, emb_dim] -> [batch_size, None, emb_dim]\n",
    "    embed0 = tf.squeeze(embed0, axis=2)\n",
    "\n",
    "    hm = LSTM(hidden_dim, \n",
    "              name='lstm',\n",
    "              stateful=True,\n",
    "              return_sequences=False,\n",
    "              kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "              recurrent_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "              bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))(embed0)\n",
    "\n",
    "    logits = Dense(event_vocab_dim,\n",
    "                   activation='linear',\n",
    "                   name='dense_1',\n",
    "                   kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "                   bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))(hm)\n",
    "    \n",
    "    generator = Model(\n",
    "        inputs=i_et,\n",
    "        outputs=logits)\n",
    "        \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from: https://gist.github.com/iskandr/a874e4cf358697037d14a17020304535\n",
    "def highway_layers(value, n_layers, activation=\"tanh\", gate_bias=-3):\n",
    "    dim = K.int_shape(value)[-1]\n",
    "    gate_bias_initializer = tf.keras.initializers.Constant(gate_bias)\n",
    "    for i in range(n_layers):     \n",
    "        gate = Dense(units=dim, bias_initializer=gate_bias_initializer, activation='sigmoid')(value)\n",
    "        negated_gate = Lambda(\n",
    "            lambda x: 1.0 - x,\n",
    "            output_shape=(dim,))(gate)\n",
    "        transformed = Dense(units=dim, activation=activation)(value)\n",
    "        transformed_gated = Multiply()([gate, transformed])\n",
    "        identity_gated = Multiply()([negated_gate, value])\n",
    "        value = Add()([transformed_gated, identity_gated])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_D(T, event_vocab_dim, emb_dim, num_filters=100, kernel_size=4, dropout_rate=0.25):\n",
    "    # Highway network\n",
    "    i_et = Input(shape=(T, 1), name='event_type')  # input of discrete feature event type\n",
    "\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(T, 1))\n",
    "    masked_et = mask_layer(i_et)\n",
    "    \n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, mask_zero=True)(masked_et)\n",
    "    embed0 = tf.squeeze(embed0, axis=2)\n",
    "\n",
    "    conv1 = Conv1D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation='relu',\n",
    "        use_bias=True,\n",
    "        kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "        bias_initializer=tf.keras.initializers.Constant(value=0.1))(embed0)\n",
    "    \n",
    "    pooled = MaxPooling1D(pool_size=T-kernel_size+1, strides=1)(conv1)\n",
    "    \n",
    "    highwayed = highway_layers(pooled, n_layers=1, activation=\"relu\", gate_bias=0)\n",
    "\n",
    "    dropped = Dropout(rate=dropout_rate)(highwayed)\n",
    "    \n",
    "    prob = Dense(1, \n",
    "                 activation='sigmoid',\n",
    "                 name='final',\n",
    "                 kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                 bias_initializer=tf.keras.initializers.Constant(value=0.1))(dropped)\n",
    "\n",
    "    discriminator = Model(\n",
    "        inputs=i_et,\n",
    "        outputs=prob)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-jurisdiction",
   "metadata": {},
   "source": [
    "## Test initial G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_G(batch_size=BATCH_SIZE, event_vocab_dim=11, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_batch_1 = tf.cast(tf.random.uniform([BATCH_SIZE, T-1, 1], maxval=10, dtype=tf.int32)+1, tf.float32)\n",
    "# seq_batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "G(seq_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = build_D(T=20, event_vocab_dim=11, emb_dim=32, num_filters=100, kernel_size=4, dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "D(seq_batch_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-apartment",
   "metadata": {},
   "source": [
    "## Pre-Training of original SeqGAn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_discriminator(event_type_batch, label_batch, discriminator, verbose=False, optimizer=Adam(lr=0.001)):\n",
    "    # train the discriminator\n",
    "    with tf.GradientTape() as tape:\n",
    "        # train discriminator\n",
    "        true_prob = discriminator(event_type_batch)\n",
    "\n",
    "        # cross-entropy loss\n",
    "        discriminator_loss = ce_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(label_batch, true_prob, from_logits=False)\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print('total discriminator loss:{}'.format(discriminator_loss))\n",
    "\n",
    "    grads = tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "    return discriminator_loss\n",
    "\n",
    "\n",
    "def pretrain_generator(event_type_batch, generator, verbose=False, optimizer=Adam(lr=0.001)):\n",
    "    _, T, _ = event_type_batch.shape\n",
    "\n",
    "    # train the generator\n",
    "    with tf.GradientTape() as tape:\n",
    "        generator.reset_states()\n",
    "        ce_loss_list = []\n",
    "\n",
    "        for i in range(0, T - 1):\n",
    "            curr_state_et = event_type_batch[:, i:i + 1, :]\n",
    "            target_et = event_type_batch[:, i + 1, :]\n",
    "\n",
    "            token_logits = generator(curr_state_et)\n",
    "\n",
    "            ce_losses = tf.keras.losses.sparse_categorical_crossentropy(target_et, token_logits, from_logits=True)\n",
    "            ce_loss = tf.reduce_mean(ce_losses)\n",
    "            ce_loss_list.append(ce_loss)\n",
    "\n",
    "        generator_loss = tf.reduce_mean(ce_loss_list)\n",
    "\n",
    "    if verbose:\n",
    "        print('train loss:{}'.format(generator_loss))\n",
    "\n",
    "    # apply gradient decent per batch\n",
    "    grads = tape.gradient(generator_loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "    return generator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-worker",
   "metadata": {},
   "source": [
    "### pre-train G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total_G = pos_event_type_seqs.shape[0]\n",
    "\n",
    "EPOCHS = 1\n",
    "_TOTAL_STEPS = int(EPOCHS * N_total_G / BATCH_SIZE)\n",
    "\n",
    "pretrain_G_dataset = create_dataset(pos_event_type_seqs,\n",
    "                                  np.ones((N_total_G, 1)),\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  buffer_size=N_total_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_gen_ce_loss_history = []\n",
    "pretrain_gen_metrics_history = []\n",
    "\n",
    "pretrained_generator = build_G(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "OPTIMIZER = Adam(lr=1e-3)\n",
    "\n",
    "for event_type_batch, _ in tqdm(pretrain_G_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "        \n",
    "    gen_ce_loss =  pretrain_generator(event_type_batch, pretrained_generator, verbose=True, optimizer=OPTIMIZER)                    \n",
    "    pretrain_gen_ce_loss_history.append(gen_ce_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/models/seqgan_original'\n",
    "if not os.path.exists(model_save_dir + '/pretrained_gen_weights'):\n",
    "    os.makedirs(model_save_dir + '/pretrained_gen_weights')\n",
    "\n",
    "G_save_path = model_save_dir + '/pretrained_gen_weights/model.tf'\n",
    "pretrained_generator.save_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_gen = build_G(\n",
    "    batch_size = BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "reload_pretrained_gen.build(input_shape=((BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_gen.load_weights(G_save_path)\n",
    "reload_pretrained_gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_from_prefix(\n",
    "        G, batch_size, prefix, L=T, end_token=0, init_token=1):\n",
    "    # Begin from dummy init state (init_token=1, init_timestamp=0.0)\n",
    "    all_state_et = prefix\n",
    "    curr_state_et = prefix[:, -1:, :]\n",
    "    l_prefix = prefix.shape[1]\n",
    "    \n",
    "    G.reset_states()\n",
    "\n",
    "    for _ in range(L-l_prefix):  # sequence length\n",
    "        token_logits = G(curr_state_et)\n",
    "        \n",
    "        sampled_et = tf.random.categorical(token_logits, num_samples=1, dtype=tf.int32)\n",
    "        sampled_et = tf.reshape(sampled_et, [batch_size, 1, 1])\n",
    "\n",
    "        # cast sampled_et into float\n",
    "        sampled_et = tf.cast(sampled_et, dtype=tf.float64)\n",
    "\n",
    "        # stop genererating once hit end_token\n",
    "        cond_end_token = tf.equal(curr_state_et, end_token)\n",
    "        curr_state_et = tf.where(cond_end_token, curr_state_et, sampled_et)\n",
    "        all_state_et = tf.concat([all_state_et, curr_state_et], axis=1)\n",
    "\n",
    "    return all_state_et\n",
    "\n",
    "\n",
    "\n",
    "def generate_sequences(N_gen, generator, batch_size, T):\n",
    "    \"\"\"\n",
    "        Generate sequences batch per batch\n",
    "    :param N_gen: total number of seqs to be generated\n",
    "    :param generator:\n",
    "    :param batch_size:\n",
    "    :param T:\n",
    "    :return: a python list of shape [N_gen, T, 1]\n",
    "    \"\"\"\n",
    "    N = 0\n",
    "    all_type_seq = None\n",
    "    init_prefix = tf.ones([batch_size, 1, 1], dtype=tf.float64)\n",
    "    \n",
    "    while N < N_gen:\n",
    "        batch_state_et = rollout_from_prefix(generator, batch_size, init_prefix, T)\n",
    "        batch_type_seq = batch_state_et.numpy()\n",
    "\n",
    "        if all_type_seq is None:\n",
    "            all_type_seq = batch_type_seq\n",
    "        else:\n",
    "            all_type_seq = np.concatenate([all_type_seq, batch_type_seq], axis=0)\n",
    "\n",
    "        N += batch_size\n",
    "\n",
    "    # convert to python list\n",
    "    type_seq_list = all_type_seq[:N_gen].tolist()\n",
    "\n",
    "    return type_seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = generate_sequences(100, pretrained_generator, BATCH_SIZE, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(aa).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-container",
   "metadata": {},
   "source": [
    "### pre-train D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_fake_D = N_real_D = N_total_G // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake data by G to pre-train D\n",
    "fake_pos_event_type_seqs = np.array(generate_sequences(N_fake_D, pretrained_generator, BATCH_SIZE, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake data by G to pre-train D\n",
    "real_pos_event_type_seqs = pos_event_type_seqs[0:N_real_D, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total_D = N_fake_D + N_real_D\n",
    "\n",
    "pretrain_D_et = np.concatenate([fake_pos_event_type_seqs, real_pos_event_type_seqs], axis=0)\n",
    "pretrain_D_labels = np.concatenate([np.zeros((N_fake_D, 1)), np.ones((N_real_D, 1))], axis=0)\n",
    "\n",
    "EPOCHS = 10\n",
    "_TOTAL_STEPS = int(EPOCHS * N_total_D / BATCH_SIZE)\n",
    "\n",
    "pretrain_D_dataset = create_dataset(pretrain_D_et,\n",
    "                                    pretrain_D_labels,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    epochs=EPOCHS,\n",
    "                                    buffer_size=N_total_D) # shuffle the entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_disc_token_loss_history = []\n",
    "\n",
    "pretrained_discriminator = build_D(T=T,\n",
    "                                   event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                                   emb_dim=EMB_DIM,\n",
    "                                   num_filters=100,\n",
    "                                   kernel_size=4,\n",
    "                                   dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "OPTIMIZER = SGD(lr=1e-3)\n",
    "for event_type_batch, labels in tqdm(pretrain_D_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "\n",
    "    disc_token_loss = pretrain_discriminator(event_type_batch, labels, pretrained_discriminator, verbose=True, optimizer=OPTIMIZER)\n",
    "    pretrain_disc_token_loss_history.append(disc_token_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/models/seqgan_original'\n",
    "if not os.path.exists(model_save_dir + '/pretrained_disc_weights'):\n",
    "    os.makedirs(model_save_dir + '/pretrained_disc_weights')\n",
    "\n",
    "D_save_path = model_save_dir + '/pretrained_disc_weights/model.tf'\n",
    "pretrained_discriminator.save_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_pretrained_disc = build_D(T=T,\n",
    "                                   event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "                                   emb_dim=EMB_DIM,\n",
    "                                   num_filters=100,\n",
    "                                   kernel_size=4,\n",
    "                                   dropout_rate=0.25)\n",
    "\n",
    "reload_pretrained_disc.build(input_shape=((BATCH_SIZE, T, 1)))\n",
    "reload_pretrained_disc.load_weights(D_save_path)\n",
    "reload_pretrained_disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-bacon",
   "metadata": {},
   "source": [
    "## Seq Gan original Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, discriminator, batch_size, T, mc_size=10, verbose=False,                   \n",
    "                    optimizer=Adam(lr=0.001), end_token=0, init_token=1):\n",
    "    \n",
    "    prefix = tf.ones([batch_size, 1, 1], dtype=tf.float64)\n",
    "    curr_state_et = prefix[:, -1:, :]\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:                        \n",
    "\n",
    "        gen_step_loss = []\n",
    "\n",
    "        for _ in tqdm(range(1, T)):\n",
    "\n",
    "            # MC rollout to the end to get final return\n",
    "            rollout_returns = []\n",
    "            for _ in range(mc_size):\n",
    "                batch_rollout_et = rollout_from_prefix(generator, batch_size, prefix, T)\n",
    "                rollout_returns.append(discriminator(batch_rollout_et))\n",
    "            \n",
    "            Q_avg = tf.reduce_mean(rollout_returns, axis=0)\n",
    "            \n",
    "            # one-step rollout to get next prefix\n",
    "            token_logits = generator(prefix)        \n",
    "            sampled_et = tf.random.categorical(token_logits, num_samples=1, dtype=tf.int32)\n",
    "            sampled_et = tf.reshape(sampled_et, [batch_size, 1, 1])\n",
    "            sampled_et = tf.cast(sampled_et, dtype=tf.float64)\n",
    "\n",
    "            # stop genererating once hit end_token\n",
    "            cond_end_token = tf.equal(curr_state_et, end_token)\n",
    "            curr_state_et = tf.where(cond_end_token, curr_state_et, sampled_et)\n",
    "            prefix = tf.concat([prefix, curr_state_et], axis=1)\n",
    "            \n",
    "            # averge loss over batch at each rollout step: -E[log_prob * Q]\n",
    "            policy_gradient_loss = -tf.reduce_mean(tf.math.log(tf.nn.softmax(token_logits)) * Q_avg)\n",
    "            gen_step_loss.append(policy_gradient_loss)\n",
    "        \n",
    "        avg_gen_loss = tf.reduce_mean(gen_step_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print('generator loss:{}'.format(avg_gen_loss))\n",
    "            print('-----------------------')\n",
    "\n",
    "    # update generator\n",
    "    generator_grads = tape.gradient(avg_gen_loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "\n",
    "    # explicitly drop tape because persistent=True\n",
    "    del tape\n",
    "\n",
    "    return avg_gen_loss\n",
    "\n",
    "\n",
    "def train_discriminator(real_data_batch, generator, discriminator, batch_size, T, verbose=False,\n",
    "                        optimizer=Adam(lr=0.001)):\n",
    "\n",
    "    # data prep\n",
    "    real_labels = tf.ones((batch_size, 1))        \n",
    "\n",
    "    fake_labels = tf.zeros((batch_size, 1))    \n",
    "    fake_data_batch = np.array(generate_sequences(1, generator, batch_size, T))\n",
    "    \n",
    "    total_data = tf.concat([fake_data_batch, real_data_batch], axis=0)\n",
    "    total_labels = tf.concat([fake_labels, real_labels], axis=0)\n",
    "\n",
    "\n",
    "    # train the discriminator\n",
    "    with tf.GradientTape() as tape:                                                           \n",
    "        # train discriminator\n",
    "        pred_prob = discriminator(total_data)\n",
    "\n",
    "        # cross-entropy loss\n",
    "        discriminator_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(total_labels, pred_prob, from_logits=False))        \n",
    "\n",
    "        if verbose:\n",
    "            print('total discriminator loss:{}'.format(discriminator_loss))\n",
    "            print('-----------------------')\n",
    "\n",
    "    grads = tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-latest",
   "metadata": {},
   "source": [
    "## Start training from pretrained G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = '/home/lun/project-basileus/multitype-sequence-generation-by-tlstm-gan/models/seqgan_original'\n",
    "G_save_path = model_save_dir + '/pretrained_gen_weights/model.tf'\n",
    "D_save_path = model_save_dir + '/pretrained_disc_weights/model.tf'\n",
    "\n",
    "G = build_G(\n",
    "    batch_size = BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "G.build(input_shape=((BATCH_SIZE, T, 1)))\n",
    "G.load_weights(G_save_path)\n",
    "G.summary()\n",
    "\n",
    "D = build_D(T=T,\n",
    "            event_vocab_dim=EVENT_VOCAB_DIM,\n",
    "            emb_dim=EMB_DIM,\n",
    "            num_filters=100,\n",
    "            kernel_size=4,\n",
    "            dropout_rate=0.25)\n",
    "\n",
    "D.build(input_shape=((BATCH_SIZE, T, 1)))\n",
    "D.load_weights(D_save_path)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total_G = pos_event_type_seqs.shape[0]\n",
    "\n",
    "EPOCHS = 1\n",
    "_TOTAL_STEPS = int(EPOCHS * N_total_G / BATCH_SIZE)\n",
    "\n",
    "train_dataset = create_dataset(pos_event_type_seqs,\n",
    "                             np.ones((N_total_G, 1)),\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             epochs=EPOCHS,\n",
    "                             buffer_size=N_total_G)\n",
    "\n",
    "gen_loss_history = []\n",
    "disc_loss_history = []\n",
    "\n",
    "step = 0\n",
    "\n",
    "G_optimizer = SGD(learning_rate=1e-3)\n",
    "D_optimizer = SGD(learning_rate=1e-3)\n",
    "\n",
    "_G_STEPS = 1\n",
    "_D_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_type_batch, _ in tqdm(train_dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "    # train the generator\n",
    "    for _ in range(_G_STEPS):\n",
    "        gen_loss = train_generator(G, D, BATCH_SIZE, T, mc_size=100, verbose=True, optimizer=Adam(lr=0.001))        \n",
    "        gen_loss_history.append(gen_loss.numpy())\n",
    "    \n",
    "    # train the discriminator\n",
    "    for _ in range(_D_STEPS):\n",
    "        disc_loss = train_discriminator(event_type_batch, G, D, BATCH_SIZE, T, verbose=True, optimizer=Adam(lr=0.001))\n",
    "        disc_loss_history.append(disc_loss.numpy())\n",
    "\n",
    "#     # save weights every 200 steps\n",
    "#     if step % 100 == 0:\n",
    "#         print('Saving weights...')\n",
    "#         save_path_prefix = f'/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v10/oracle_train_{step}'\n",
    "#         save_model_weights(save_path_prefix, generator, discriminator, critic)\n",
    "#         print('All Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-annual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-illustration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
