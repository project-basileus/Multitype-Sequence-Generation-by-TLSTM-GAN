{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = '/home/lun/project-basileus/seq-gan/sgtlstm'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sgtlstm' in sys.modules:\n",
    "    importlib.reload(sys.modules['sgtlstm'])\n",
    "\n",
    "from sgtlstm.utils import load_fixed_length_sequence_from_pickle, create_dataset\n",
    "from sgtlstm.SeqGan import build_G, build_D\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_path = '/home/lun/project-basileus/seq-gan/data/fixed_length_with_init_token/valid_sequences.pickle'\n",
    "invalid_data_path = '/home/lun/project-basileus/seq-gan/data/fixed_length_with_init_token/invalid_sequences.pickle'\n",
    "\n",
    "valid_event_type_seqs, valid_timestamp_seqs = load_fixed_length_sequence_from_pickle(valid_data_path, to_timedelta=True, end_token=0)\n",
    "invalid_event_type_seqs, invalid_timestamp_seqs = load_fixed_length_sequence_from_pickle(invalid_data_path, to_timedelta=True, end_token=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "T = 10 + 1\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 5\n",
    "HIDDEN_DIM = 64\n",
    "K_MIST = 7\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_D(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    "    k_mixt = K_MIST\n",
    ")\n",
    "discriminator.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "D_save_path = './gan_model_weights/pretrained_1000_disc.h5'\n",
    "discriminator.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_G(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    "    k_mixt = K_MIST,\n",
    "    return_sequence=False,\n",
    ")\n",
    "generator.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "G_save_path = './gan_model_weights/pretrained_1000_gen.h5'\n",
    "generator.load_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_et = valid_event_type_seqs\n",
    "train_ts = valid_timestamp_seqs\n",
    "\n",
    "train_labels = np.ones((valid_event_type_seqs.shape[0], 1))\n",
    "\n",
    "train_features = (train_et, train_ts)\n",
    "N_train = train_et.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "# optimizer = SGD(learning_rate=1e-4)\n",
    "\n",
    "dataset = create_dataset(train_features,\n",
    "                         train_labels,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=3,\n",
    "                         buffer_size=N_train)\n",
    "\n",
    "gen_token_loss_history = []\n",
    "gen_gaussian_loss_history = []\n",
    "\n",
    "disc_token_loss_history = []\n",
    "disc_gaussian_loss_history = []\n",
    "\n",
    "WEIGHT_GAUSSIAN_LOSS = 1e-2\n",
    "_G_STEPS = 2\n",
    "_D_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS = 240\n",
    "\n",
    "step = 0\n",
    "\n",
    "for features_batch, _ in tqdm(dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "\n",
    "    # train the generator\n",
    "    for _ in range(_G_STEPS):\n",
    "        gen_token_loss, gen_gaussian_loss = train_generator(generator, discriminator, T, event_vocab_dim=EVENT_VOCAB_DIM,  verbose=False, weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS)\n",
    "        gen_token_loss_history.append(gen_token_loss.numpy())\n",
    "        gen_gaussian_loss_history.append(gen_gaussian_loss.numpy())        \n",
    "    \n",
    "    # train the discriminator\n",
    "    for _ in range(_D_STEPS):\n",
    "        disc_token_loss, disc_gaussian_loss = train_discriminator(features_batch, generator, discriminator, T, event_vocab_dim=EVENT_VOCAB_DIM, verbose=False)              \n",
    "        disc_token_loss_history.append(disc_token_loss.numpy())\n",
    "        disc_gaussian_loss_history.append(disc_gaussian_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(gen_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, gen_token_loss_history)\n",
    "plt.title('Generator Toke Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "x = range(len(gen_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, gen_gaussian_loss_history)\n",
    "plt.title('Generator Gaussian Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(disc_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, disc_token_loss_history)\n",
    "plt.title('Discriminator Toke Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "x = range(len(disc_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, disc_gaussian_loss_history)\n",
    "plt.title('Discriminator Gaussian Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_et, states_ts, episode_token_probs, gaussian_log = generate_one_seq(generator)\n",
    "# states_et.squeeze()\n",
    "\n",
    "def recover_timedelta_to_timestamp(time_seq):\n",
    "    csum = []\n",
    "    curr = 0\n",
    "    \n",
    "    for dt in time_seq:\n",
    "        if dt != 0:\n",
    "            curr += dt\n",
    "            csum.append(curr)\n",
    "        else:\n",
    "            csum.append(0)\n",
    "    \n",
    "    return csum\n",
    "\n",
    "# recover_timedelta_to_timestamp(time_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_gen = 1000 # \n",
    "generated_seqs = []\n",
    "\n",
    "for i in range(N_gen):\n",
    "    states_et, states_ts, episode_token_probs, gaussian_log = generate_one_sequence_by_rollout(generator,\n",
    "                                                                                               T, EVENT_VOCAB_DIM,\n",
    "                                                                                               verbose=False)\n",
    "    type_seq = states_et[-1,:,:].squeeze().tolist()\n",
    "    time_seq = states_ts[-1,:,:].squeeze().tolist()\n",
    "    recovered_time_seq = recover_timedelta_to_timestamp(time_seq)\n",
    "    generated_seqs.append(list(zip(type_seq, recovered_time_seq)))\n",
    "    if i % 50 == 0: \n",
    "        print(i)\n",
    "        print(list(zip(type_seq, recovered_time_seq)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
