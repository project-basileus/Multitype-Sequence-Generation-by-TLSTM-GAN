{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = '/home/lun/project-basileus/seq-gan/'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sgtlstm' in sys.modules:\n",
    "    importlib.reload(sys.modules['sgtlstm'])\n",
    "\n",
    "from sgtlstm.utils import create_dataset, recover_timedelta_to_timestamp\n",
    "from sgtlstm.SeqGan import build_G, build_D, build_critic\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "# from sgtlstm.train import generate_batch_sequence_by_rollout, train_discriminator, train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v6/positive_long_sequences.pickle'\n",
    "neg_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v6/negative_long_sequences.pickle'\n",
    "all_data_path = '/home/lun/project-basileus/seq-gan/data/long_seqs_v6/all_long_sequences.pickle'\n",
    "\n",
    "def load_sequence_from_pickle_to_numpy(pickle_file_path):\n",
    "    \"\"\"\n",
    "        A list of sequence in format of (event_type, delta_time)\n",
    "    :param pickle_file_path: e.g. /.../project-basileus/seq-gan/data/fixed_length/valid_sequences.pickle\n",
    "    :return: (event_type_seqs, delta_time)\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        raw_seqs = pickle.load(f)\n",
    "\n",
    "    if not raw_seqs or not raw_seqs[0]:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    N = len(raw_seqs)\n",
    "    T = len(raw_seqs[0])\n",
    "    \n",
    "    seqs = np.array(raw_seqs)\n",
    "#     print(seqs.shape)\n",
    "    \n",
    "    et_seqs = seqs[:, :, 0].astype(np.float64).reshape((N, T, 1))\n",
    "    ts_seqs = seqs[:, :, 1].astype(np.float64).reshape((N, T, 1))\n",
    "    return et_seqs, ts_seqs\n",
    "    \n",
    "pos_event_type_seqs, pos_timestamp_seqs = load_sequence_from_pickle_to_numpy(pos_data_path)\n",
    "neg_event_type_seqs, neg_timestamp_seqs = load_sequence_from_pickle_to_numpy(neg_data_path)\n",
    "all_event_type_seqs, all_timestamp_seqs = load_sequence_from_pickle_to_numpy(all_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "T = 20 + 1\n",
    "VOCAB = ['END/PADDING', 'INIT', 'start', 'view', 'click', 'install']\n",
    "EVENT_VOCAB_DIM = len(VOCAB)\n",
    "EMB_DIM = 5\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "END_TOKEN = 0\n",
    "MAX_TIME = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "def build_D_2(T, event_vocab_dim, emb_dim, hidden_dim=11):\n",
    "    \"\"\"\n",
    "        Build a discriminator for event type sequence of shape (batch_size, T, input_dim)\n",
    "        and input event type sequence of shape (batch_size, T, 1)\n",
    "    :param T: length of the sequence\n",
    "    :param event_vocab_dim: size of event vocabulary ['na', 'init', 'start', 'view', 'click', 'install']\n",
    "    :param emb_dim: dimension of the embedding layer output for event type\n",
    "    :param hidden_dim: dimension hidden of the time lstm cell\n",
    "    :return: discriminator D\n",
    "    \"\"\"\n",
    "    # Time-LSTM:\n",
    "    i_et = Input(shape=(T, 1), name='event_type')  # input of discrete feature event type\n",
    "    i_ts = Input(shape=(T, 1), name='time_delta')  # input of continuous feature timestamp\n",
    "    mask_layer = tf.keras.layers.Masking(mask_value=0., input_shape=(T, 1))\n",
    "    masked_ts = mask_layer(i_ts)\n",
    "    masked_et = mask_layer(i_et)\n",
    "\n",
    "    embed0 = Embedding(input_dim=event_vocab_dim, output_dim=emb_dim, input_length=T, mask_zero=True)(masked_et)\n",
    "    embed0 = Reshape((T, emb_dim))(embed0)  # shape=[Batch_size, T, emb_dim]\n",
    "    merged0 = tf.keras.layers.concatenate([embed0, masked_ts], axis=2)  # # shape=[Batch_size, T, emb_dim + time_dim]\n",
    "\n",
    "    hm, tm = TimeLSTM1(hidden_dim, activation='selu', name='time_lstm', return_sequences=False)(merged0)\n",
    "\n",
    "    time_comb = tf.keras.layers.concatenate([hm, tm], axis=1)\n",
    "\n",
    "    # predicted real prob\n",
    "    real_prob = Dense(1, activation='sigmoid', name='fraud_prob', kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3))(\n",
    "        time_comb)\n",
    "\n",
    "    discriminator = Model(\n",
    "        inputs=[i_et, i_ts],\n",
    "        outputs=[real_prob])\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_D_2(\n",
    "    T = T,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM,\n",
    ")\n",
    "\n",
    "discriminator.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "D_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_disc_weights/model_2.tf'\n",
    "discriminator.load_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_G(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)\n",
    "\n",
    "generator.build(input_shape=((BATCH_SIZE, T, 1), (BATCH_SIZE, T, 1)))\n",
    "\n",
    "G_save_path = '/home/lun/project-basileus/seq-gan/experiment_results/long_seqs_v6/init_pretrained/pretrained_gen_weights/model.tf'\n",
    "generator.load_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = build_critic(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_vocab_dim = EVENT_VOCAB_DIM,\n",
    "    emb_dim = EMB_DIM,\n",
    "    hidden_dim= HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Reshape, Dense\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "from sgtlstm.TimeLSTM import TimeLSTM0, TimeLSTM1, TimeLSTM2, TimeLSTM3\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sequence_by_rollout(\n",
    "        G, batch_size, T, end_token=0, init_token=1.0, max_time=1024, verbose=False):\n",
    "    # Begin from dummy init state (init_token=1, init_timestamp=0.0)\n",
    "    curr_state_et = tf.ones([batch_size, 1, 1], dtype=tf.float64)\n",
    "    curr_state_ts = tf.zeros([batch_size, 1, 1], dtype=tf.float64)\n",
    "\n",
    "    all_state_et = curr_state_et\n",
    "    all_state_ts = curr_state_ts\n",
    "\n",
    "    episode_token_probs = tf.constant(1., dtype=tf.float64, shape=(batch_size, 1))\n",
    "    gaussian_log = tf.constant(0., dtype=tf.float64, shape=(batch_size, 1))\n",
    "\n",
    "    G.reset_states()\n",
    "\n",
    "    for step in range(1, T):  # sequence length\n",
    "        token_prob, time_out = G([curr_state_et, curr_state_ts])\n",
    "\n",
    "        sampled_et = tf.random.categorical(tf.math.log(token_prob), num_samples=1, dtype=tf.int32)\n",
    "        sampled_et = tf.reshape(sampled_et, [batch_size, 1, 1])\n",
    "\n",
    "        # get the chosen token probability per batch for each step\n",
    "        batch_sample_et = tf.reshape(sampled_et, (batch_size, 1))\n",
    "        batch_ind = tf.reshape(tf.range(0, batch_size), (batch_size, 1))\n",
    "        batch_sample_et_2d = tf.concat([batch_ind, batch_sample_et], axis=1)\n",
    "\n",
    "        sampled_token_prob = tf.reshape(tf.gather_nd(token_prob, batch_sample_et_2d), (batch_size, 1))\n",
    "        episode_token_probs = tf.concat([episode_token_probs, sampled_token_prob], axis=1)\n",
    "\n",
    "        # cast sampled_et into float\n",
    "        sampled_et = tf.cast(sampled_et, dtype=tf.float64)\n",
    "\n",
    "        # stop genererating once hit end_token\n",
    "        cond_end_token = tf.equal(curr_state_et, end_token)\n",
    "        curr_state_et = tf.where(cond_end_token, curr_state_et, sampled_et)\n",
    "        all_state_et = tf.concat([all_state_et, curr_state_et], axis=1)\n",
    "\n",
    "        # generate one timstamp using time_out\n",
    "        sampled_ts_raw = time_out.sample()\n",
    "        sampled_ts = tf.clip_by_value(tf.reshape(sampled_ts_raw, (batch_size, 1, 1))\n",
    "                                      , clip_value_min=1, clip_value_max=max_time)\n",
    "\n",
    "        # get the gaussian log likelihood for the sampled timestamps\n",
    "        sampled_gaussian_log = time_out.log_prob(sampled_ts_raw)\n",
    "        gaussian_log = tf.concat([gaussian_log, sampled_gaussian_log], axis=1)\n",
    "\n",
    "        # stop generating once hit end_token\n",
    "        curr_state_ts = tf.where(cond_end_token, curr_state_et, sampled_ts)\n",
    "        all_state_ts = tf.concat([all_state_ts, curr_state_ts], axis=1)\n",
    "\n",
    "    return all_state_et, all_state_ts, episode_token_probs, gaussian_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(N_gen, generator, batch_size, T, recover_to_timestamp=True):\n",
    "    \"\"\"\n",
    "        Generate sequences batch per batch\n",
    "    :param N_gen: total number of seqs to be generated\n",
    "    :param generator:\n",
    "    :param batch_size:\n",
    "    :param T:\n",
    "    :param recover_to_timestamp: whether to recover time deltas to absolute timestamps\n",
    "    :return: a python list of shape [N_gen, T, 2]\n",
    "    \"\"\"\n",
    "    all_type_seq = None\n",
    "    all_time_seq = None\n",
    "    N = 0\n",
    "\n",
    "    while N < N_gen:\n",
    "        batch_state_et, batch_state_ts, _, _ = generate_batch_sequence_by_rollout(generator, batch_size, T,\n",
    "                                                                                  end_token=0, init_token=1.0,\n",
    "                                                                                  max_time=1024, verbose=False)\n",
    "\n",
    "        batch_type_seq = batch_state_et.numpy()\n",
    "        batch_time_seq = batch_state_ts.numpy()\n",
    "\n",
    "        # recover time delta to time stamps\n",
    "        if recover_to_timestamp:\n",
    "            batch_time_seq = np.cumsum(batch_time_seq, axis=1)\n",
    "\n",
    "        if all_type_seq is None:\n",
    "            all_type_seq = batch_type_seq\n",
    "        else:\n",
    "            all_type_seq = np.concatenate([all_type_seq, batch_type_seq], axis=0)\n",
    "\n",
    "        if all_time_seq is None:\n",
    "            all_time_seq = batch_time_seq\n",
    "        else:\n",
    "            all_time_seq = np.concatenate([all_time_seq, batch_time_seq], axis=0)\n",
    "\n",
    "        N += batch_size\n",
    "\n",
    "    # concat type and time in depth\n",
    "    concated_seq_list = np.concatenate([all_type_seq, all_time_seq], axis=2).tolist()\n",
    "\n",
    "    return concated_seq_list[:N_gen]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, discriminator, critic_network, batch_size, T, verbose=False,\n",
    "                    weight_gaussian_loss=1,\n",
    "                    optimizer=Adam(lr=0.001)):\n",
    "    # reset hidden states for critic network per batch\n",
    "    critic_network.reset_states()\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        states_et, states_ts, episode_token_probs, gaussian_log = generate_batch_sequence_by_rollout(generator,\n",
    "                                                                                                     batch_size, T,\n",
    "                                                                                                     end_token=0,\n",
    "                                                                                                     init_token=1.0,\n",
    "                                                                                                     max_time=1024,\n",
    "                                                                                                     verbose=False)\n",
    "        ce_loss_list = []\n",
    "        gaussian_list = []\n",
    "        critic_loss_list = []\n",
    "\n",
    "        # run disc on whole sequence\n",
    "        # true_return is the total reward for generating this seq\n",
    "        true_return = discriminator((states_et, states_ts))\n",
    "        \n",
    "        ZERO_PENALTY = 10\n",
    "\n",
    "        for i in range(0, T):\n",
    "            # TODO: should we include the init token in loss?\n",
    "            curr_state_et = states_et[:, i:i + 1, :]\n",
    "            curr_state_ts = states_ts[:, i:i + 1, :]\n",
    "\n",
    "            curr_token_prob = episode_token_probs[:, i:i + 1]\n",
    "            curr_gaussian_log = gaussian_log[:, i:i + 1]\n",
    "\n",
    "            q_value = critic_network([curr_state_et, curr_state_ts])\n",
    "            advantage = true_return - q_value\n",
    "\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `q_value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `true_return`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.            \n",
    "            \n",
    "            mask = tf.squeeze(curr_state_et)\n",
    "            curr_state_et = tf.boolean_mask(curr_state_et, mask)\n",
    "            curr_state_ts = tf.boolean_mask(curr_state_ts, mask)\n",
    "            curr_token_prob = tf.boolean_mask(curr_token_prob, mask)\n",
    "            curr_gaussian_log = tf.boolean_mask(curr_gaussian_log, mask)\n",
    "            \n",
    "            if curr_state_et.shape[0] == 0:\n",
    "                ce_loss_list.append(ZERO_PENALTY)\n",
    "                continue\n",
    "\n",
    "            masked_q_value = tf.boolean_mask(q_value, mask)\n",
    "            masked_advantage = tf.boolean_mask(advantage, mask)\n",
    "            masked_true_return = tf.boolean_mask(true_return, mask)            \n",
    "            \n",
    "            ce_loss_list.append(-tf.reduce_mean(tf.math.log(curr_token_prob) * masked_advantage))\n",
    "            gaussian_list.append(-tf.reduce_mean(curr_gaussian_log * masked_advantage))\n",
    "\n",
    "            ce_loss_list.append(-tf.reduce_mean(tf.math.log(curr_token_prob)))\n",
    "            gaussian_list.append(-tf.reduce_mean(curr_gaussian_log))\n",
    "\n",
    "            critic_loss_list.append(tf.reduce_mean(tf.keras.losses.MSE(masked_true_return, masked_q_value)))\n",
    "\n",
    "        total_ce_loss = tf.reduce_sum(ce_loss_list)\n",
    "        total_gaussian_loss = tf.reduce_sum(gaussian_list)\n",
    "        total_critic_loss = tf.reduce_sum(critic_loss_list)        \n",
    "        total_generator_loss = total_ce_loss + weight_gaussian_loss * total_gaussian_loss\n",
    "\n",
    "        average_true_return = tf.reduce_mean(true_return)\n",
    "\n",
    "        if verbose:\n",
    "            print('generator token loss:{}'.format(total_ce_loss))\n",
    "            print('generator gaussian loss:{}'.format(total_gaussian_loss))\n",
    "            print('generator total loss:{}'.format(total_generator_loss))\n",
    "            print('generator critic loss:{}'.format(total_critic_loss))\n",
    "            print('average true_return: {}'.format(average_true_return))\n",
    "            print('-----------------------')\n",
    "\n",
    "    # update generator\n",
    "    generator_grads = tape.gradient(total_generator_loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "\n",
    "    # update critic network\n",
    "    critic_grads = tape.gradient(total_critic_loss, critic_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(critic_grads, critic_network.trainable_variables))\n",
    "\n",
    "    # explicitly drop tape because persistent=True\n",
    "    del tape\n",
    "\n",
    "    return total_ce_loss, total_gaussian_loss, total_critic_loss, average_true_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(features_batch, generator, discriminator, batch_size, T, verbose=False,\n",
    "                        optimizer=Adam(lr=0.001)):\n",
    "    # train the discriminator\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_et, real_ts = features_batch\n",
    "        # (batch_size, 1)\n",
    "        real_labels = tf.ones((batch_size, 1)) + tfd.Normal(loc=0, scale=0.1, name='normal_disturbance_true').sample(sample_shape=(batch_size, 1))                \n",
    "#         real_labels = tf.clip_by_value(real_labels, clip_value_min=0.0, clip_value_max=1.0)\n",
    "        \n",
    "\n",
    "        generated_et, generated_ts, episode_token_probs, gaussian_log = generate_batch_sequence_by_rollout(generator,\n",
    "                                                                                                           batch_size,\n",
    "                                                                                                           T,\n",
    "                                                                                                           end_token=0,\n",
    "                                                                                                           init_token=1.0,\n",
    "                                                                                                           max_time=1024,\n",
    "                                                                                                           verbose=False)\n",
    "\n",
    "        generated_labels = tf.zeros((batch_size, 1)) + tfd.Normal(loc=0, scale=0.1, name='normal_disturbance_fake').sample(sample_shape=(batch_size, 1))\n",
    "#         generated_labels = tf.clip_by_value(generated_labels, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "        total_et = tf.concat([generated_et, real_et], axis=0)\n",
    "        total_ts = tf.concat([generated_ts, real_ts], axis=0)\n",
    "        total_labels = tf.concat([generated_labels, real_labels], axis=0)\n",
    "\n",
    "        # train discriminator\n",
    "        pred_prob = discriminator((total_et, total_ts))\n",
    "\n",
    "        # cross-entropy loss\n",
    "        ce_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(total_labels, pred_prob, from_logits=False))\n",
    "        discriminator_loss = ce_loss\n",
    "\n",
    "        if verbose:\n",
    "            print('total discriminator loss:{}'.format(discriminator_loss))\n",
    "            print('-----------------------')\n",
    "\n",
    "    grads = tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_et = pos_event_type_seqs\n",
    "train_ts = pos_timestamp_seqs\n",
    "\n",
    "train_labels = np.ones((pos_event_type_seqs.shape[0], 1))\n",
    "\n",
    "train_features = (train_et, train_ts)\n",
    "N_train = train_et.shape[0]\n",
    "N_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "# optimizer = SGD(learning_rate=1e-4)\n",
    "EPOCHS = 2\n",
    "_TOTAL_STEPS = int(EPOCHS * N_train / BATCH_SIZE)\n",
    "# _TOTAL_STEPS = 1000\n",
    "\n",
    "dataset = create_dataset(train_features,\n",
    "                         train_labels,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=EPOCHS,\n",
    "                         buffer_size=N_train)\n",
    "\n",
    "gen_token_loss_history = []\n",
    "gen_gaussian_loss_history = []\n",
    "critic_network_loss_history = []\n",
    "average_true_return_history = []\n",
    "\n",
    "disc_ce_loss_history = []\n",
    "\n",
    "WEIGHT_GAUSSIAN_LOSS = 1\n",
    "_G_STEPS = 2\n",
    "_D_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "for features_batch, _ in tqdm(dataset.take(_TOTAL_STEPS)):\n",
    "    step += 1\n",
    "    print('Training Step:', step)\n",
    "    # train the generator\n",
    "    for _ in range(_G_STEPS):\n",
    "        gen_token_loss, gen_gaussian_loss, critic_network_loss, average_true_return = train_generator(generator, discriminator, critic, \n",
    "                                                                                 batch_size=BATCH_SIZE, T=T, verbose=True, \n",
    "                                                                                 weight_gaussian_loss=WEIGHT_GAUSSIAN_LOSS,\n",
    "                                                                                 optimizer=optimizer\n",
    "                                                                                )\n",
    "        gen_token_loss_history.append(gen_token_loss.numpy())\n",
    "        gen_gaussian_loss_history.append(gen_gaussian_loss.numpy())    \n",
    "        critic_network_loss_history.append(critic_network_loss.numpy())\n",
    "        average_true_return_history.append(average_true_return.numpy())\n",
    "    \n",
    "    # train the discriminator\n",
    "    for _ in range(_D_STEPS):\n",
    "        disc_ce_loss = train_discriminator(features_batch, generator, discriminator, \n",
    "                                           batch_size=BATCH_SIZE, T=T, verbose=True, \n",
    "                                           optimizer=optimizer)\n",
    "        disc_ce_loss_history.append(disc_ce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 steps: Loss over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(gen_token_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, gen_token_loss_history)\n",
    "plt.title('Generator Toke Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "x = range(len(gen_gaussian_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, gen_gaussian_loss_history)\n",
    "plt.title('Generator Gaussian Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "x = range(len(disc_ce_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, disc_ce_loss_history)\n",
    "plt.title('Discriminator CE Loss History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(critic_network_loss_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, critic_network_loss_history)\n",
    "plt.title('Critic Loss History')\n",
    "plt.xlabel('training steps')\n",
    "\n",
    "\n",
    "x = range(len(average_true_return_history))\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, average_true_return_history)\n",
    "plt.title('Average True Return History')\n",
    "plt.xlabel('training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/v6/after_1_epoch/loss'\n",
    "if not os.path.exists(loss_save_dir):\n",
    "    os.makedirs(loss_save_dir)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'gen_token_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(gen_token_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'gen_gaussian_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(gen_gaussian_loss_history, f)\n",
    "    \n",
    "with open(os.path.join(loss_save_dir, 'critic_network_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(critic_network_loss_history, f)\n",
    "\n",
    "with open(os.path.join(loss_save_dir, 'disc_ce_loss_history.pickle'), 'wb') as f:\n",
    "    pickle.dump(disc_ce_loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 steps: Save G and D models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/v6/after_1_epoch/gen_weights'\n",
    "if not os.path.exists(G_save_dir):\n",
    "    os.makedirs(G_save_dir)\n",
    "    \n",
    "G_save_path = os.path.join(G_save_dir, 'gen_model.tf')\n",
    "generator.save_weights(G_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_save_dir = '/home/lun/project-basileus/seq-gan/experiment_results/v6/after_1_epoch/disc_weights'\n",
    "if not os.path.exists(D_save_dir):\n",
    "    os.makedirs(D_save_dir)\n",
    "    \n",
    "D_save_path = os.path.join(D_save_dir, 'disc_model.tf')\n",
    "discriminator.save_weights(D_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 steps: Generate sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_gen = 100\n",
    "generated_seqs = generate_sequences(N_gen, generator, batch_size=BATCH_SIZE, T=T, recover_to_timestamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_seqs_save_dir = './experiment_results/after_1000_steps/generated_seqs'\n",
    "# if not os.path.exists(generated_seqs_save_dir):\n",
    "#     os.makedirs(generated_seqs_save_dir)\n",
    "\n",
    "# with open(os.path.join(generated_seqs_save_dir, 'generated_seqs.pickle'), 'wb') as f:\n",
    "#     pickle.dump(generated_seqs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "init_token = 1\n",
    "end_token = 0\n",
    "max_time = 1024\n",
    "G = generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin from dummy init state (init_token=1, init_timestamp=0.0)\n",
    "curr_state_et = tf.ones([batch_size, 1, 1], dtype=tf.float64)\n",
    "curr_state_ts = tf.zeros([batch_size, 1, 1], dtype=tf.float64)\n",
    "\n",
    "all_state_et = curr_state_et\n",
    "all_state_ts = curr_state_ts\n",
    "\n",
    "episode_token_probs = tf.constant(1., dtype=tf.float64, shape=(batch_size, 1))\n",
    "gaussian_log = tf.constant(0., dtype=tf.float64, shape=(batch_size, 1))\n",
    "\n",
    "G.reset_states()\n",
    "\n",
    "for step in range(1, T):  # sequence length\n",
    "    token_prob, time_out = G([curr_state_et, curr_state_ts])\n",
    "\n",
    "    sampled_et = tf.random.categorical(tf.math.log(token_prob), num_samples=1, dtype=tf.int32)\n",
    "    sampled_et = tf.reshape(sampled_et, [batch_size, 1, 1])\n",
    "\n",
    "    # get the chosen token probability per batch for each step\n",
    "    batch_sample_et = tf.reshape(sampled_et, (batch_size, 1))\n",
    "    batch_ind = tf.reshape(tf.range(0, batch_size), (batch_size, 1))\n",
    "    batch_sample_et_2d = tf.concat([batch_ind, batch_sample_et], axis=1)\n",
    "\n",
    "    sampled_token_prob = tf.reshape(tf.gather_nd(token_prob, batch_sample_et_2d), (batch_size, 1))\n",
    "    episode_token_probs = tf.concat([episode_token_probs, sampled_token_prob], axis=1)\n",
    "    \n",
    "    # cast sampled_et into float\n",
    "    sampled_et = tf.cast(sampled_et, dtype=tf.float64)\n",
    "    \n",
    "    # stop genererating once hit end_token\n",
    "    cond_end_token = tf.equal(curr_state_et, end_token)\n",
    "    curr_state_et = tf.where(cond_end_token, curr_state_et, sampled_et)\n",
    "    all_state_et = tf.concat([all_state_et, curr_state_et], axis=1)\n",
    "\n",
    "    # generate one timstamp using time_out\n",
    "    sampled_ts_raw = time_out.sample()\n",
    "    sampled_ts = tf.clip_by_value(tf.reshape(sampled_ts_raw, (batch_size, 1, 1))\n",
    "                                  , clip_value_min=1, clip_value_max=max_time)\n",
    "\n",
    "    # get the gaussian log likelihood for the sampled timestamps\n",
    "    sampled_gaussian_log = time_out.log_prob(sampled_ts_raw)\n",
    "    gaussian_log = tf.concat([gaussian_log, sampled_gaussian_log], axis=1)\n",
    "\n",
    "    # stop generating once hit end_token\n",
    "    curr_state_ts = tf.where(cond_end_token, curr_state_ts, sampled_ts)\n",
    "    all_state_ts = tf.concat([all_state_ts, curr_state_ts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_state_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_state_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather_nd(token_prob, tf.reshape(sampled_et, (batch_size, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_et_2 = tf.reshape(sampled_et, (batch_size, 1))\n",
    "batch_ind = tf.reshape(tf.range(0, batch_size), (batch_size, 1))\n",
    "sample_ed_3 = tf.concat([batch_ind, sample_et_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather_nd(token_prob, sample_ed_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
